import csv
import os
import re

def extract_domains_from_csv(csv_path):
    domains = set()
    with open(csv_path, newline='', encoding='utf-8') as csvfile:
        reader = csv.reader(csvfile)
        next(reader)  # Skip header
        for row in reader:
            if len(row) > 1:
                domains.add(row[1].strip())  # Extract domain from second column
    return domains

def search_domains_in_texts(pdf_text_folder, domains, output_txt_path):
    domain_pattern = re.compile(r'\b(' + '|'.join(re.escape(domain) for domain in domains) + r')\b')
    url_pattern = re.compile(r'https?://\S+')  # Regex to extract URLs
    matches = {}
    
    # Créer le dossier result s'il n'existe pas
    os.makedirs(os.path.dirname(output_txt_path), exist_ok=True)
    
    for filename in os.listdir(pdf_text_folder):
        if filename.endswith(".txt"):
            file_path = os.path.join(pdf_text_folder, filename)
            with open(file_path, "r", encoding="utf-8") as txt_file:
                text = txt_file.read().splitlines()
                
                for line in text:
                    found = domain_pattern.findall(line)
                    if found:
                        urls = url_pattern.findall(line)  
                        if filename not in matches:
                            matches[filename] = []
                        for url in urls:
                            matches[filename].append((found[0], url))  
    
    with open(output_txt_path, "w", encoding="utf-8") as output_file:
        for file, found_domains in matches.items():
            for domain, url in found_domains:
                output_file.write(f"{file}: {domain} (match: {url})\n")
    
    print(f"Correspondances enregistrées dans {output_txt_path}")

# Chemins d'accès mis à jour
csv_path = os.path.join("data", "SH", "SH_forge.csv")
txt_folder = os.path.join("data", "txt")
output_path = os.path.join("data", "result", "txt_result_daniel.txt")

# Exécution
domains = extract_domains_from_csv(csv_path)
search_domains_in_texts(txt_folder, domains, output_path)


import csv
import os
import re

def extract_domains_from_csv(csv_path):
    domains = set()
    with open(csv_path, newline='', encoding='utf-8') as csvfile:
        reader = csv.reader(csvfile)
        next(reader)  # Skip header
        for row in reader:
            if len(row) > 1:
                domains.add(row[1].strip())  # Extract domain from second column
    return domains

def search_domains_in_texts(pdf_text_folder, domains, output_csv_path):
    domain_pattern = re.compile(r'\b(' + '|'.join(re.escape(domain) for domain in domains) + r')\b')
    url_pattern = re.compile(r'https?://\S+')  # Regex to extract URLs
    matches = []  # Changed to list to store rows for CSV
    
    # Créer le dossier result s'il n'existe pas
    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)
    
    for filename in os.listdir(pdf_text_folder):
        if filename.endswith(".txt"):
            file_path = os.path.join(pdf_text_folder, filename)
            with open(file_path, "r", encoding="utf-8") as txt_file:
                text = txt_file.read().splitlines()
                
                for line in text:
                    found = domain_pattern.findall(line)
                    if found:
                        urls = url_pattern.findall(line)
                        for url in urls:
                            # Ajouter chaque correspondance comme une ligne pour le CSV
                            matches.append([filename, found[0], url])
    
    # Écrire dans un fichier CSV
    with open(output_csv_path, "w", encoding="utf-8", newline='') as output_file:
        writer = csv.writer(output_file)
        # Écrire l'en-tête
        writer.writerow(["Fichier", "Domaine", "URL"])
        # Écrire les données
        writer.writerows(matches)
    
    print(f"Correspondances enregistrées dans {output_csv_path}")

# Chemins d'accès avec remontée d'un niveau
csv_path = os.path.join("..", "data", "SH", "SH_forge.csv")
txt_folder = os.path.join("..", "data", "txt")
output_path = os.path.join("..", "result", "txt_result_daniel.csv")  # Changed extension to .csv

# Exécution
domains = extract_domains_from_csv(csv_path)
search_domains_in_texts(txt_folder, domains, output_path)



