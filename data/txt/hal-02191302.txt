Sliced-Wasserstein Flows: Nonparametric Generative
Modeling via Optimal Transport and Diffusions
Antoine Liutkus, Umut ≈û Im≈üekli, Szymon Majewski, Alain Durmus,

Fabian-Robert St√∂ter

To cite this version:

Antoine Liutkus, Umut ≈û Im≈üekli, Szymon Majewski, Alain Durmus, Fabian-Robert St√∂ter. Sliced-
Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions. ICML
2019 - 36th International Conference on Machine Learning, Jun 2019, Long Beach, United States.
pp.4104-4113. Ôøøhal-02191302Ôøø

HAL Id: hal-02191302

https://inria.hal.science/hal-02191302

Submitted on 23 Jul 2019

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L‚Äôarchive ouverte pluridisciplinaire HAL, est
destin√©e au d√©p√¥t et √† la diffusion de documents
scientifiques de niveau recherche, publi√©s ou non,
√©manant des √©tablissements d‚Äôenseignement et de
recherche fran√ßais ou √©trangers, des laboratoires
publics ou priv√©s.

Distributed under a Creative Commons Attribution 4.0 International License

Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal
Transport and Diffusions

Antoine Liutkus 1 Umut S¬∏ ims¬∏ekli 2 Szymon Majewski 3 Alain Durmus 4 Fabian-Robert St¬®oter 1

Abstract

By building upon the recent theory that estab-
lished the connection between implicit generative
modeling (IGM) and optimal transport, in this
study, we propose a novel parameter-free algo-
rithm for learning the underlying distributions of
complicated datasets and sampling from them.
The proposed algorithm is based on a functional
optimization problem, which aims at Ô¨Ånding a
measure that is close to the data distribution as
much as possible and also expressive enough for
generative modeling purposes. We formulate the
problem as a gradient Ô¨Çow in the space of proba-
bility measures. The connections between gradi-
ent Ô¨Çows and stochastic differential equations let
us develop a computationally efÔ¨Åcient algorithm
for solving the optimization problem. We provide
formal theoretical analysis where we prove Ô¨Ånite-
time error guarantees for the proposed algorithm.
To the best of our knowledge, the proposed algo-
rithm is the Ô¨Årst nonparametric IGM algorithm
with explicit theoretical guarantees. Our experi-
mental results support our theory and show that
our algorithm is able to successfully capture the
structure of different types of data distributions.

1. Introduction

Implicit generative modeling (IGM) (Diggle & Gratton,
1984; Mohamed & Lakshminarayanan, 2016) has become
very popular recently and has proven successful in various
Ô¨Åelds; variational auto-encoders (VAE) (Kingma & Welling,

1Inria and LIRMM, Univ.

of Montpellier, France
2LTCI, T¬¥el¬¥ecom Paristech, Universit¬¥e Paris-Saclay, Paris,
3Institute of Mathematics, Polish Academy of Sci-
France
ences, Warsaw, Poland 4CNRS, ENS Paris-Saclay,Universit
Paris-Saclay, Cachan, France.
Correspondence to: An-
toine Liutkus <antoine.liutkus@inria.fr>, Umut S¬∏ ims¬∏ekli
<umut.simsekli@telecom-paristech.fr>.

Proceedings of the 36 th International Conference on Machine
Learning, Long Beach, California, PMLR 97, 2019. Copyright
2019 by the author(s).

2013) and generative adversarial networks (GAN) (Goodfel-
low et al., 2014) being its two well-known examples. The
goal in IGM can be brieÔ¨Çy described as learning the un-
derlying probability measure of a given dataset, denoted as
ŒΩ ‚àà P(‚Ñ¶), where P is the space of probability measures on
the measurable space (‚Ñ¶, A), ‚Ñ¶ ‚äÇ Rd is a domain and A is
the associated Borel œÉ-Ô¨Åeld.

Given a set of data points {y1, . . . , yP } that are assumed to
be independent and identically distributed (i.i.d.) samples
drawn from ŒΩ, the implicit generative framework models
them as the output of a measurable map, i.e. y = T (x), with
T : ‚Ñ¶¬µ (cid:55)‚Üí ‚Ñ¶. Here, the inputs x are generated from a known
and easy to sample source measure ¬µ on ‚Ñ¶¬µ (e.g. Gaussian
or uniform measures), and the outputs T (x) should match
the unknown target measure ŒΩ on ‚Ñ¶.

Learning generative networks have witnessed several
groundbreaking contributions in recent years. Motivated
by this fact, there has been an interest in illuminating the
theoretical foundations of VAEs and GANs (Bousquet et al.,
2017; Liu et al., 2017). It has been shown that these implicit
models have close connections with the theory of Optimal
Transport (OT) (Villani, 2008). As it turns out, OT brings
new light on the generative modeling problem: there have
been several extensions of VAEs (Tolstikhin et al., 2017;
Kolouri et al., 2018) and GANs (Arjovsky et al., 2017; Gul-
rajani et al., 2017; Guo et al., 2017; Lei et al., 2017), which
exploit the links between OT and IGM.

OT studies whether it is possible to transform samples from
a source distribution ¬µ to a target distribution ŒΩ. From this
perspective, an ideal generative model is simply a transport
map from ¬µ to ŒΩ. This can be written by using some ‚Äòpush-
forward operators‚Äô: we seek a mapping T that ‚Äòpushes ¬µ
onto ŒΩ‚Äô, and is formally deÔ¨Åned as ŒΩ(A) = ¬µ(T ‚àí1(A)) for
all Borel sets A ‚äÇ A. If this relation holds, we denote the
push-forward operator T#, such that T#¬µ = ŒΩ. Provided
mild conditions on these distributions hold (notably ¬µ is non-
atomic (Villani, 2008)), existence of such a transport map is
guaranteed; however, it remains a challenge to construct it
in practice.

One common point between VAE and GAN is to adopt
an approximate strategy and consider transport maps that

Sliced-Wasserstein Flows

belong to a parametric family TœÜ with œÜ ‚àà Œ¶. Then,
they aim at Ô¨Ånding the best parameter œÜ(cid:63) that would give
TœÜ(cid:63)#¬µ ‚âà ŒΩ. This is typically achieved by attempting
to minimize the following optimization problem: œÜ(cid:63) =
arg minœÜ‚ààŒ¶ W2(TœÜ#¬µ, ŒΩ), where W2 denotes the Wasser-
stein distance that will be properly deÔ¨Åned in Section 2. It
has been shown that (Genevay et al., 2017) OT-based GANs
(Arjovsky et al., 2017) and VAEs (Tolstikhin et al., 2017)
both use this formulation with different parameterizations
and different equivalent deÔ¨Ånitions of W2. However, their
resulting algorithms still lack theoretical understanding.

In this study, we follow a completely different approach
for IGM, where we aim at developing an algorithm with
explicit theoretical guarantees for estimating a transport
map between source ¬µ and target ŒΩ. The generated transport
map will be nonparametric (in the sense that it does not
belong to some family of functions, like a neural network),
and it will be iteratively augmented: always increasing the
quality of the Ô¨Åt along iterations. Formally, we take Tt as
the constructed transport map at time t ‚àà [0, ‚àû), and deÔ¨Åne
¬µt = Tt#¬µ as the corresponding output distribution. Our
objective is to build the maps so that ¬µt will converge to
the solution of a functional optimization problem, deÔ¨Åned
through a gradient Ô¨Çow in the Wasserstein space. Informally,
we will consider a gradient Ô¨Çow that has the following form:

‚àÇt¬µt = ‚àí‚àáW2

(cid:110)

Cost(¬µt, ŒΩ) + Reg(¬µt)

(cid:111)

, ¬µ0 = ¬µ, (1)

where the functional Cost computes a discrepancy between
¬µt and ŒΩ, Reg denotes a regularization functional, and ‚àáW2
denotes a notion of gradient with respect to a probability
measure in the W2 metric for probability measures1. If this
Ô¨Çow can be simulated, one would hope for ¬µt = (Tt)#¬µ
to converge to the minimum of the functional optimization
problem: min¬µ(Cost(¬µ, ŒΩ) + Reg(¬µ)) (Ambrosio et al.,
2008; Santambrogio, 2017).

We construct a gradient Ô¨Çow where we choose the Cost
functional as the sliced Wasserstein distance (SW2) (Rabin
et al., 2012; Bonneel et al., 2015) and the Reg functional as
the negative entropy. The SW2 distance is equivalent to the
W2 distance (Bonnotte, 2013) and has important computa-
tional implications since it can be expressed as an average
of (one-dimensional) projected optimal transportation costs
whose analytical expressions are available.

We Ô¨Årst show that, with the choice of SW2 and the negative-
entropy functionals as the overall objective, we obtain a
valid gradient Ô¨Çow that has a solution path (¬µt)t, and the
probability density functions of this path solve a particular

1This gradient Ô¨Çow is similar to the usual Euclidean gradient
Ô¨Çows, i.e. ‚àÇtxt = ‚àí‚àá(f (xt) + r(xt)), where f is typically the
data-dependent cost function and r is a regularization term. The
(explicit) Euler discretization of this Ô¨Çow results in the well-known
gradient descent algorithm for solving minx(f (x) + r(x)).

partial differential equation, which has close connections
with stochastic differential equations. Even though gradient
Ô¨Çows in Wasserstein spaces cannot be solved in general,
by exploiting this connection, we are able to develop a
practical algorithm that provides approximate solutions to
the gradient Ô¨Çow and is algorithmically similar to stochastic
gradient Markov Chain Monte Carlo (MCMC) methods2
(Welling & Teh, 2011; Ma et al., 2015; Durmus et al., 2016;
S¬∏ ims¬∏ekli, 2017; S¬∏ ims¬∏ekli et al., 2018). We provide Ô¨Ånite-
time error guarantees for the proposed algorithm and show
explicit dependence of the error to the algorithm parameters.

To the best of our knowledge, the proposed algorithm is the
Ô¨Årst nonparametric IGM algorithm that has explicit theoreti-
cal guarantees. In addition to its nice theoretical properties,
the proposed algorithm has also signiÔ¨Åcant practical impor-
tance: it has low computational requirements and can be
easily run on an everyday laptop CPU.Our experiments on
both synthetic and real datasets support our theory and illus-
trate the advantages of the algorithm in several scenarios.

2. Technical Background

2.1. Wasserstein distance, optimal transport maps and

Kantorovich potentials

For two probability measures ¬µ, ŒΩ ‚àà P2(‚Ñ¶), P2(‚Ñ¶) =
{¬µ ‚àà P(‚Ñ¶) : (cid:82)
‚Ñ¶ (cid:107)x(cid:107)2 ¬µ(dx) < +‚àû}, the 2-Wasserstein
distance is deÔ¨Åned as follows:

W2(¬µ, ŒΩ) (cid:44)

(cid:110)

inf
Œ≥‚ààC(¬µ,ŒΩ)

(cid:90)

‚Ñ¶√ó‚Ñ¶

(cid:107)x ‚àí y(cid:107)2Œ≥(dx, dy)

(cid:111)1/2

,

(2)

where C(¬µ, ŒΩ) is called the set of transportation plans and
deÔ¨Åned as the set of probability measures Œ≥ on ‚Ñ¶√ó‚Ñ¶ satisfy-
ing for all A ‚àà A, Œ≥(A√ó‚Ñ¶) = ¬µ(A) and Œ≥(‚Ñ¶√óA) = ŒΩ(A),
i.e. the marginals of Œ≥ coincide with ¬µ and ŒΩ. From now on,
we will assume that ‚Ñ¶ is a compact subset of Rd.

In the case where ‚Ñ¶ is Ô¨Ånite, computing the Wasserstein
distance between two probability measures turns out to be a
linear program with linear constraints, and has therefore a
dual formulation. Since ‚Ñ¶ is a Polish space (i.e. a complete
and separable metric space), this dual formulation can be
generalized as follows (Villani, 2008)[Theorem 5.10]:

W2(¬µ, ŒΩ) = sup

œà‚ààL1(¬µ)

(cid:110)(cid:90)

‚Ñ¶

œà(x)¬µ(dx) +

œàc(x)ŒΩ(dx)

(cid:111)1/2

(cid:90)

‚Ñ¶

(3)

where L1(¬µ) denotes the class of functions that are abso-
lutely integrable under ¬µ and œàc denotes the c-conjugate
of œà and is deÔ¨Åned as follows: œàc(y) (cid:44) {inf x‚àà‚Ñ¶ (cid:107)x ‚àí

2We note that, despite the algorithmic similarities, the proposed

algorithm is not a Bayesian posterior sampling algorithm.

Sliced-Wasserstein Flows

y(cid:107)2 ‚àí œà(x)}. The functions œà that realize the supremum
in (3) are called the Kantorovich potentials between ¬µ and
ŒΩ. Provided that ¬µ satisÔ¨Åes a mild condition, we have the
following uniqueness result.

Theorem 1 ((Santambrogio, 2014)[Theorem 1.4]). Assume
that ¬µ ‚àà P2(‚Ñ¶) is absolutely continuous with respect to
the Lebesgue measure. Then, there exists a unique optimal
transport plan Œ≥(cid:63) that realizes the inÔ¨Åmum in (2) and it
is of the form (Id √ó T )#¬µ, for a measurable function T :
‚Ñ¶ ‚Üí ‚Ñ¶. Furthermore, there exists at least a Kantorovich
potential œà whose gradient ‚àáœà is uniquely determined ¬µ-
almost everywhere. The function T and the potential œà are
linked by T (x) = x ‚àí ‚àáœà(x).

The measurable function T : ‚Ñ¶ ‚Üí ‚Ñ¶ is referred to as the
optimal transport map from ¬µ to ŒΩ. This result implies that
there exists a solution for transporting samples from ¬µ to
samples from ŒΩ and this solution is optimal in the sense that
it minimizes the (cid:96)2 displacement. However, identifying this
solution is highly non-trivial. In the discrete case, effective
solutions have been proposed (Cuturi, 2013). However,
for continuous and high-dimensional probability measures,
constructing an actual transport plan remains a challenge.
Even if recent contributions (Genevay et al., 2016) have
made it possible to rapidly compute W2, they do so without
constructing the optimal map T , which is our objective here.

2.2. Wasserstein spaces and gradient Ô¨Çows

By (Ambrosio et al., 2008)[Proposition 7.1.5], W2 is a dis-
tance over P(‚Ñ¶). In addition, if ‚Ñ¶ ‚äÇ Rd is compact, the
topology associated with W2 is equivalent to the weak con-
vergence of probability measures and (P(‚Ñ¶), W2)3 is com-
pact. The metric space (P2(‚Ñ¶), W2) is called the Wasser-
stein space.

In this study, we are interested in functional optimiza-
tion problems in (P2(‚Ñ¶), W2), such as min¬µ‚ààP2(‚Ñ¶) F(¬µ),
where F is the functional that we would like to minimize.
Similar to Euclidean spaces, one way to formulate this opti-
mization problem is to construct a gradient Ô¨Çow of the form
‚àÇt¬µt = ‚àí‚àáW2 F(¬µt) (Benamou & Brenier, 2000; Lavenant
et al., 2018), where ‚àáW2 denotes a notion of gradient in
(P2(‚Ñ¶), W2). If such a Ô¨Çow can be constructed, one can uti-
lize it both for practical algorithms and theoretical analysis.

Gradient Ô¨Çows ‚àÇt¬µt = ‚àáW2 F(¬µt) with respect to a func-
tional F in (P2(‚Ñ¶), W2) have strong connections with par-
tial differential equations (PDE) that are of the form of a con-
tinuity equation (Santambrogio, 2017). Indeed, it is shown
than under appropriate conditions on F (see e.g.(Ambrosio
et al., 2008)), (¬µt)t is a solution of the gradient Ô¨Çow if and
only if it admits a density œÅt with respect to the Lebesgue
measure for all t ‚â• 0, and solves the continuity equation

3Note that in that case, P2(‚Ñ¶) = P(‚Ñ¶)

given by: ‚àÇtœÅt + div(vœÅt) = 0, where v denotes a vector
Ô¨Åeld and div denotes the divergence operator. Then, for a
given gradient Ô¨Çow in (P2(‚Ñ¶), W2), we are interested in the
evolution of the densities œÅt, i.e. the PDEs which they solve.
Such PDEs are of our particular interest since they have a
key role for building practical algorithms.

2.3. Sliced-Wasserstein distance

ŒΩ

ŒΩ

¬µ , F ‚àí1

¬µ (œÑ ) ‚àí F ‚àí1

i.e. ¬µ, ŒΩ ‚àà P2(R), W2
In the one-dimensional case,
has an analytical form, given as follows: W2(¬µ, ŒΩ) =
(cid:82) 1
0 |F ‚àí1
(œÑ )|2 dœÑ , where F¬µ and FŒΩ denote the
cumulative distribution functions (CDF) of ¬µ and ŒΩ, respec-
tively, and F ‚àí1
denote the inverse CDFs, also called
quantile functions (QF). In this case, the optimal transport
map from ¬µ to ŒΩ has a closed-form formula as well, given
as follows: T (x) = (F ‚àí1
‚ó¶ F¬µ)(x) (Villani, 2008). The
optimal map T is also known as the increasing arrangement,
which maps each quantile of ¬µ to the same quantile of ŒΩ,
e.g. minimum to minimum, median to median, maximum to
maximum (Villani, 2008). Due to Theorem 1, the derivative
of the corresponding Kantorovich potential is given as:

ŒΩ

œà(cid:48)(x) (cid:44) ‚àÇxœà(x) = x ‚àí (F ‚àí1

ŒΩ

‚ó¶ F¬µ)(x).

In the multidimensional case d > 1, building a transport
map is much more difÔ¨Åcult. The nice properties of the
one-dimensional Wasserstein distance motivate the usage
of sliced-Wasserstein distance (SW2) for practical appli-
cations. Before formally deÔ¨Åning SW2, let us Ô¨Årst deÔ¨Åne
the orthogonal projection Œ∏‚àó(x) (cid:44) (cid:104)Œ∏, x(cid:105) for any direction
Œ∏ ‚àà Sd‚àí1 and x ‚àà Rd, where (cid:104)¬∑, ¬∑(cid:105) denotes the Euclidean
inner-product and Sd‚àí1 ‚äÇ Rd denotes the d-dimensional
unit sphere. Then, the SW2 distance is formally deÔ¨Åned as
follows:

SW2(¬µ, ŒΩ) (cid:44)

(cid:90)

Sd‚àí1

W2(Œ∏‚àó

#¬µ, Œ∏‚àó

#ŒΩ) dŒ∏,

(4)

where dŒ∏ represents the uniform probability measure on
Sd‚àí1. As shown in (Bonnotte, 2013), SW2 is indeed a
distance metric and induces the same topology as W2 for
compact domains.

#¬µ, Œ∏‚àó

#¬µ and Œ∏‚àó

The SW2 distance has important practical implications:
provided that the projected distributions Œ∏‚àó
#ŒΩ
can be computed, then for any Œ∏ ‚àà Sd‚àí1, the distance
W2(Œ∏‚àó
#ŒΩ), as well as its optimal transport map and
the corresponding Kantorovich potential can be analyt-
ically computed (since the projected measures are one-
dimensional). Therefore, one can easily approximate (4)
by using a simple Monte Carlo scheme that draws uniform
random samples from Sd‚àí1 and replaces the integral in (4)
with a Ô¨Ånite-sample average. Thanks to its computational
beneÔ¨Åts, SW2 was very recently considered for OT-based
VAEs and GANs (Deshpande et al., 2018; Wu et al., 2018;

Sliced-Wasserstein Flows

Kolouri et al., 2018), appearing as a stable alternative to the
adversarial methods.

3. Regularized Sliced-Wasserstein Flows for

Generative Modeling

3.1. Construction of the gradient Ô¨Çow

In this paper, we propose the following functional minimiza-
tion problem on P2(‚Ñ¶) for implicit generative modeling:

(cid:110)

Œª (¬µ) (cid:44) 1
F ŒΩ
2

min
¬µ

SW 2

2 (¬µ, ŒΩ) + ŒªH(¬µ)

(cid:111)
,

(5)

where Œª > 0 is a regularization parameter and H denotes the
negative entropy deÔ¨Åned by H(¬µ) (cid:44) (cid:82)
‚Ñ¶ œÅ(x) log œÅ(x)dx if
¬µ has density œÅ with respect to the Lebesgue measure and
H(¬µ) = +‚àû otherwise. Note that the case Œª = 0 has
been already proposed and studied in (Bonnotte, 2013) in
a more general OT context. Here, in order to introduce the
necessary noise inherent to generative model, we suggest
to penalize the slice-Wasserstein distance using H. In other
words, the main idea is to Ô¨Ånd a measure ¬µ(cid:63) that is close
to ŒΩ as much as possible and also has a certain amount of
entropy to make sure that it is sufÔ¨Åciently expressive for gen-
erative modeling purposes. The importance of the entropy
regularization becomes prominent in practical applications
where we have Ô¨Ånitely many data samples that are assumed
to be drawn from ŒΩ. In such a circumstance, the regular-
ization would prevent ¬µ(cid:63) to collapse on the data points and
therefore avoid ‚Äòover-Ô¨Åtting‚Äô to the data distribution. Note
that this regularization is fundamentally different from the
one used in Sinkhorn distances (Genevay et al., 2018).

In our Ô¨Årst result, we show that there exists a Ô¨Çow (¬µt)t‚â•0 in
(P(B(0, r)), W2) which decreases along F ŒΩ
Œª , where B(0, a)
denotes the closed unit ball centered at 0 and radius a. This
Ô¨Çow will be referred to as a generalized minimizing move-
ment scheme (see DeÔ¨Ånition 1 in the supplementary docu-
ment). In addition, the Ô¨Çow (¬µt)t‚â•0 admits a density œÅt with
respect to the Lebesgue measure for all t > 0 and (œÅt)t‚â•0
is solution of a non-linear PDE (in the weak sense).

‚àö

Theorem 2. Let ŒΩ be a probability measure on B(0, 1) with
a strictly positive smooth density. Choose a regularization
d, where d is the data
constant Œª > 0 and radius r >
dimension. Assume that ¬µ0 ‚àà P(B(0, r)) is absolutely con-
tinuous with respect to the Lebesgue measure with density
œÅ0 ‚àà L‚àû(B(0, r)). There exists a generalized minimizing
movement scheme (¬µt)t‚â•0 associated to (5) and if œÅt stands
for the density of ¬µt for all t ‚â• 0, then (œÅt)t satisÔ¨Åes the
following continuity equation:

in a weak sense. Here, ‚àÜ denotes the Laplacian opera-
tor, div the divergence operator, and œàt,Œ∏ denotes the Kan-
torovich potential between Œ∏‚àó

#¬µt and Œ∏‚àó

#ŒΩ.

The precise statement of this Theorem, related results and
its proof are postponed to the supplementary document.
For its proof, we use the technique introduced in (Jordan
et al., 1998): we Ô¨Årst prove the existence of a generalized
minimizing movement scheme by showing that the solution
curve (¬µt)t is a limit of the solution of a time-discretized
problem. Then we prove that the curve (œÅt)t solves the PDE
given in (6).

3.2. Connection with stochastic differential equations

As a consequence of the entropy regularization, we obtain
the Laplacian operator ‚àÜ in the PDE given in (6). We there-
fore observe that the overall PDE is a Fokker-Planck-type
equation (Bogachev et al., 2015) that has a well-known prob-
abilistic counterpart, which can be expressed as a stochastic
differential equation (SDE). More precisely, let us consider
a stochastic process (Xt)t, that is the solution of the follow-
ing SDE starting at X0 ‚àº ¬µ0:

dXt = v(Xt, ¬µt)dt +

‚àö

2ŒªdWt,

(8)

where (Wt)t denotes a standard Brownian motion. Then,
the probability distribution of Xt at time t solves the PDE
given in (6) (Bogachev et al., 2015). This informally means
that, if we could simulate (8), then the distribution of Xt
would converge to the solution of (5), therefore, we could
use the sample paths (Xt)t as samples drawn from (¬µt)t.
However, in practice this is not possible due to two reasons:
(i) the drift vt cannot be computed analytically since it
depends on the probability distribution of Xt, (ii) the SDE
(8) is a continuous-time process, it needs to be discretized.

We now focus on the Ô¨Årst issue. We observe that the SDE
(8) is similar to McKean-Vlasov SDEs (Veretennikov, 2006;
Mishura & Veretennikov, 2016), a family of SDEs whose
drift depends on the distribution of Xt. By using this connec-
tion, we can borrow tools from the relevant SDE literature
(Malrieu, 2003; Cattiaux et al., 2008) for developing an
approximate simulation method for (8).

Our approach is based on deÔ¨Åning a particle system that
serves as an approximation to the original SDE (8). The
particle system can be written as a collection of SDEs, given
as follows (Bossy & Talay, 1997):

dX i

t = v(X i

t , ¬µN

t )dt +

‚àö

2ŒªdW i
t ,

i = 1, . . . , N,

(9)

‚àÇœÅt
‚àÇt

= ‚àí div(vtœÅt) + Œª‚àÜœÅt,

(cid:90)

Sd‚àí1

œà(cid:48)

t,Œ∏((cid:104)x, Œ∏(cid:105))Œ∏dŒ∏

vt(x) (cid:44) v(x, ¬µt) = ‚àí

(6)

(7)

where i denotes the particle index, N ‚àà N+ denotes the
total number of particles, and ¬µN
j=1 Œ¥X j
de-
notes the empirical distribution of the particles {X j
t }N
j=1.
This particle system is particularly interesting, since (i) one

t = (1/N ) (cid:80)N

t

Sliced-Wasserstein Flows

‚àö

typically has limN‚Üí‚àû ¬µN
t = ¬µt with a rate of convergence
N ) for all t (Malrieu, 2003; Cattiaux et al.,
of order O(1/
2008), and (ii) each of the particle systems in (9) can be sim-
ulated by using an Euler-Maruyama discretization scheme.
We note that the existing theoretical results in (Veretennikov,
2006; Mishura & Veretennikov, 2016) do not directly ap-
ply to our case due to the non-standard form of our drift.
However, we conjecture that a similar result holds for our
problem as well. Such a result would be proven by using
the techniques given in (Zhang et al., 2018); however, it is
out of the scope of this study.

3.3. Approximate Euler-Maruyama discretization

In order to be able to simulate the particle SDEs (9) in
practice, we propose an approximate Euler-Maruyama dis-
cretization for each particle SDE. The algorithm iteratively
applies the following update equation: (‚àÄi ‚àà {1, . . . , N })

¬ØX i
0

i.i.d.‚àº ¬µ0, ¬ØX i

k+1 = ¬ØX i

k + hÀÜvk( ¬ØX i

k) +

‚àö

2ŒªhZ i

k+1, (10)

where k ‚àà N+ denotes the iteration number, Z i
k is a stan-
dard Gaussian random vector in Rd, h denotes the step-
size, and ÀÜvk is a short-hand notation for a computation-
ally tractable estimator of the original drift v(¬∑, ¬Ø¬µN
kh), with
kh = (1/N ) (cid:80)N
¬Ø¬µN
being the empirical distribution of
{ ¬ØX j
k}N
is how to compute this function ÀÜv.

j=1. A question of fundamental practical importance

j=1 Œ¥ ¬ØX j

k

We propose to approximate the integral in (7) via a simple
Monte Carlo estimate. This is done by Ô¨Årst drawing NŒ∏ uni-
form i.i.d. samples from the sphere Sd‚àí1, {Œ∏n}NŒ∏
n=1. Then,
at each iteration k, we compute:

ÀÜvk(x) (cid:44) ‚àí(1/NŒ∏)

(cid:88)NŒ∏

n=1

œà(cid:48)

k,Œ∏n

((cid:104)Œ∏n, x(cid:105))Œ∏n,

(11)

where for any Œ∏, œà(cid:48)
k,Œ∏ is the derivative of the Kantorovich
potential (cf. Section 2) that is applied to the OT problem
from Œ∏‚àó

# ¬Ø¬µN

kh to Œ∏‚àó
#ŒΩ: i.e.
k,Œ∏(z) = (cid:2)z ‚àí (F ‚àí1
œà(cid:48)

#ŒΩ ‚ó¶ FŒ∏‚àó
Œ∏‚àó

# ¬Ø¬µN
kh

)(z)(cid:3).

(12)

For any particular Œ∏ ‚àà Sd‚àí1, the QF, F ‚àí1
#ŒΩ for the projection
Œ∏‚àó
of the target distribution ŒΩ on Œ∏ can be easily computed from
the data. This is done by Ô¨Årst computing the projections
(cid:104)Œ∏, yi(cid:105) for all data points yi, and then computing the empir-
ical quantile function for this set of P scalars. Similarly,
, the CDF of the particles at iteration k, is easy to
FŒ∏‚àó
compute: we Ô¨Årst project all particles ¬ØX i
k to get (cid:104)Œ∏, ¬ØX i
k(cid:105),
and then compute the empirical CDF of this set of N scalar
values.

# ¬Ø¬µN
kh

Algorithm 1: Sliced-Wasserstein Flow (SWF)

i.i.d.‚àº ¬µ0,

i=1, ¬µ0, N , NŒ∏, h, Œª

:D ‚â° {yi}P
K}N
i=1

input
output :{ ¬ØX i
// Initialize the particles
¬ØX i
0
// Generate random directions
Œ∏n ‚àº Uniform(Sd‚àí1),
// Quantiles of projected target
for Œ∏ ‚àà {Œ∏n}NŒ∏

n=1 do

i = 1, . . . , N

n = 1, . . . , NŒ∏

F ‚àí1
Œ∏‚àó

#ŒΩ = QF{(cid:104)Œ∏, yi(cid:105)}P

i=1

// Iterations
for k = 0, . . . K ‚àí 1 do

for Œ∏ ‚àà {Œ∏n}NŒ∏

n=1 do

# ¬Ø¬µN
kh

// CDF of projected particles
FŒ∏‚àó

= CDF{(cid:104)Œ∏, ¬ØX i

k(cid:105)}N
i=1
// Update the particles
‚àö
¬ØX i

k ‚àí hÀÜvk( ¬ØX i

k+1 = ¬ØX i

k) +

2ŒªhZ i

k+1
i = 1, . . . , N

computed Q ‚àà N+ empirical quantiles. Another source of
approximation here comes from the fact that the target ŒΩ
will in practice be a collection of Dirac measures on the
observations yi. Since it is currently common to have a very
large dataset, we believe this approximation to be accurate
in practice for the target. Finally, yet another source of ap-
proximation comes from the error induced by using a Ô¨Ånite
number of Œ∏n instead of a sum over Sd‚àí1 in (12).

Even though the error induced by these approximation
schemes can be incorporated into our current analysis frame-
work, we choose to neglect it for now, because (i) all of these
one-dimensional computations can be done very accurately
and (ii) the quantization of the empirical CDF and QF can
be modeled as additive Gaussian noise that enters our dis-
cretization scheme (10) (Van der Vaart, 1998). Therefore,
we will assume that ÀÜvk is an unbiased estimator of v, i.e.
E[ÀÜv(x, ¬µ)] = v(x, ¬µ), for any x and ¬µ, where the expecta-
tion is taken over Œ∏n.

The overall algorithm is illustrated in Algorithm 1. It is re-
markable that the updates of the particles only involves the
learning data {yi} through the CDFs of its projections on the
many Œ∏n ‚àà Sd‚àí1. This has a fundamental consequence of
high practical interest: these CDF may be computed before-
hand in a massively distributed manner that is independent
of the sliced Wasserstein Ô¨Çow. This aspect is reminiscent
of the compressive learning methodology (Gribonval et al.,
2017), except we exploit quantiles of random projections
here, instead of random generalized moments as done there.

In both cases, the true CDF and quantile functions are ap-
proximated as a linear interpolation between a set of the

Besides, we can obtain further reductions in the computing
time if the CDF, FŒ∏‚àó
#ŒΩ for the target is computed on random

Sliced-Wasserstein Flows

mini-batches of the data, instead of the whole dataset of size
P . This simpliÔ¨Åed procedure might also have some interest-
ing consequences in privacy-preserving settings: since we
can vary the number of projection directions NŒ∏ for each
data point yi, we may guarantee that yi cannot be recovered
via these projections, by picking fewer than necessary for
reconstruction using, e.g. compressed sensing (Donoho &
Tanner, 2009).

3.4. Finite-time analysis for the inÔ¨Ånite particle regime

In this section we will analyze the behavior of the proposed
algorithm in the asymptotic regime where the number of
particles N ‚Üí ‚àû. Within this regime, we will assume that
the original SDE (8) can be directly simulated by using an
approximate Euler-Maruyama scheme, deÔ¨Åned starting at
¬ØX0

i.i.d.‚àº ¬µ0 as follows:
¬ØXk+1 = ¬ØXk + hÀÜv( ¬ØX i

‚àö

2ŒªhZk+1,

k, ¬Ø¬µkh) +
(13)
where ¬Ø¬µkh denotes the law of ¬ØXk with step size h and {Zk}k
denotes a collection of standard Gaussian random variables.
Apart from its theoretical signiÔ¨Åcance, this scheme is also
practically relevant, since one would expect that it captures
the behavior of the particle method (10) with large number
of particles.

In practice, we would like to approximate the measure se-
quence (¬µt)t as accurate as possible, where ¬µt denotes the
law of Xt. Therefore, we are interested in analyzing the
distance (cid:107)¬Ø¬µKh ‚àí ¬µT (cid:107)TV, where K denotes the total number
of iterations, T = Kh is called the horizon, and (cid:107)¬µ ‚àí ŒΩ(cid:107)TV
denotes the total variation distance between two probability
measures ¬µ and ŒΩ: (cid:107)¬µ ‚àí ŒΩ(cid:107)TV (cid:44) supA‚ààB(‚Ñ¶) |¬µ(A) ‚àí ŒΩ(A)|.
In order to analyze this distance, we exploit the algorith-
mic similarities between (13) and the stochastic gradient
Langevin dynamics (SGLD) algorithm (Welling & Teh,
2011), which is a Bayesian posterior sampling method hav-
ing a completely different goal, and is obtained as a dis-
cretization of an SDE whose drift has a much simpler form.
We then bound the distance by extending the recent results
on SGLD (Raginsky et al., 2017) to time- and measure-
dependent drifts, that are of our interest in the paper.

We now present our second main theoretical result. We
present all our assumptions and the explicit forms of the
constants in the supplementary document.
Theorem 3. Assume that the conditions given in the supple-
mentary document hold. Then, the following bound holds
for T = Kh:

(cid:107)¬Ø¬µKh ‚àí ¬µT (cid:107)2

TV ‚â§ Œ¥Œª

(cid:40)

L2K
2Œª

(cid:16) C1h3
3

+ 3Œªdh2(cid:17)

+

C2Œ¥Kh
4Œª

(cid:41)

,

(14)

for some C1, C2, L > 0, Œ¥ ‚àà (0, 1), and Œ¥Œª > 1.

Here, the constants C1, C2, L are related to the regularity
and smoothness of the functions v and ÀÜv; Œ¥ is directly propor-
tional to the variance of ÀÜv, and Œ¥Œª is inversely proportional
to Œª. The theorem shows that if we choose h small enough,
we can have a non-asymptotic error guarantee, which is
formally shown in the following corollary.

Corollary 1. Assume that the conditions of Theorem 3 hold.
Then for all Œµ > 0, K ‚àà N+, setting

h = (3/C1) ‚àß

(cid:18) 2Œµ2Œª
Œ¥ŒªL2T

(cid:19)1/2

(1 + 3Œªd)‚àí1

,

(15)

we have

(cid:107)¬Ø¬µKh ‚àí ¬µT (cid:107)TV ‚â§ Œµ +

(cid:19)1/2

(cid:18) C2Œ¥ŒªŒ¥T
4Œª

(16)

for T = Kh.

This corollary shows that for a large horizon T , the approxi-
mate drift ÀÜv should have a small variance in order to obtain
accurate estimations. This result is similar to (Raginsky
et al., 2017) and (Nguyen et al., 2019): for small Œµ the vari-
ance of the approximate drift should be small as well. On
the other hand, we observe that the error decreases as Œª
increases. This behavior is expected since for large Œª, the
Brownian term in (8) dominates the drift, which makes the
simulation easier.

We note that these results establish the explicit dependency
of the error with respect to the algorithm parameters (e.g.
step-size, gradient noise) for a Ô¨Åxed number of iterations,
rather than explaining the asymptotic behavior of the algo-
rithm when K goes to inÔ¨Ånity.

4. Experiments

In this section, we evaluate the SWF algorithm on a syn-
thetic and a real data setting. Our primary goal is to validate
our theory and illustrate the behavior of our non-standard
approach, rather than to obtain the state-of-the-art results in
IGM. In all our experiments, the initial distribution ¬µ0 is se-
lected as the standard Gaussian distribution on Rd, we take
Q = 100 quantiles and N = 5000 particles, which proved
sufÔ¨Åcient to approximate the quantile functions accurately.

4.1. Gaussian Mixture Model

We perform the Ô¨Årst set of experiments on synthetic data
where we consider a standard Gaussian mixture model
(GMM) with 10 components and random parameters. Cen-
troids are taken as sufÔ¨Åciently distant from each other to
make the problem more challenging. We generate P =
50000 data samples in each experiment.

Sliced-Wasserstein Flows

Target

k = 2

k = 3

k = 5

k = 10

Œª = 0.1

Œª = 0.2

k = 20

k = 50

Œª = 0.5

Œª = 1

Figure 1. SWF on toy 2D data. Left: Target distribution (shaded contour plot) and distribution of particles (lines) during SWF. (bottom)
SW cost over iterations during training (left) and test (right) stages. Right: InÔ¨Çuence of the regularization parameter Œª.

Figure 2. First, we learn an autoencoder (AE). Then, we use SWF
to transport random vectors to the distribution of the bottleneck
features of the training set. The trained decoder is used for visual-
ization.

In our Ô¨Årst experiment, we set d = 2 for visualization pur-
poses and illustrate the general behavior of the algorithm.
Figure 1 shows the evolution of the particles through the
iterations. Here, we set NŒ∏ = 30, h = 1 and Œª = 10‚àí4.
We Ô¨Årst observe that the SW cost between the empirical
distributions of training data and particles is steadily de-
creasing along the SW Ô¨Çow. Furthermore, we see that the
QFs, F ‚àí1
that are computed with the initial set of par-
# ¬Ø¬µN
Œ∏‚àó
kh
ticles (the training stage) can be perfectly re-used for new
unseen particles in a subsequent test stage, yielding similar
‚Äî yet slightly higher ‚Äî SW cost.

In our second experiment on Figure 1, we investigate the
effect of the level of the regularization Œª. The distribution of
the particles becomes more spread with increasing Œª. This
is due to the increment of the entropy, as expected.

4.2. Experiments on real data

In the second set of experiments, we test the SWF algorithm
on two real datasets. (i) The traditional MNIST dataset
that contains 70K binary images corresponding to different
digits. (ii) The popular CelebA dataset (Liu et al., 2015), that

Figure 3. Samples generated after 200 iterations of SWF to match
the distribution of bottleneck features for the training dataset. Vi-
sualization is done with the pre-trained decoder.

contains 202K color-scale images. This dataset is advocated
as more challenging than MNIST. Images were interpolated
as 32 √ó 32 for MNIST, and 64 √ó 64 for CelebA.

In experiments reported in the supplementary document,
we found out that directly applying SWF to such high-
dimensional data yielded noisy results, possibly due to the
insufÔ¨Åcient sampling of Sd‚àí1. To reduce the dimensional-
ity, we trained a standard convolutional autoencoder (AE)
on the training set of both datasets (see Figure 2 and the
supplementary document), and the target distribution ŒΩ con-
sidered becomes the distribution of the resulting bottleneck
features, with dimension d. Particles can be visualized with
the pre-trained decoder. Our goal is to show that SWF per-
mits to directly sample from the distribution of bottleneck
features, as an alternative to enforcing this distribution to

0123456789101520253035404550iteration‚àí60‚àí40‚àí20020SW loss (dB)tasktraintestENCODERtarget distribution ùúàoriginal datasetbottleneck featuresreconstructed datasetsource distribution ŒºSWFfrom Œº to ùúà generated samplesgenerated featuresAE TRAININGSYNTHESISDECODERDECODERSliced-Wasserstein Flows

Figure 6. Applying a pre-trained SWF on new samples located
in-between the ones used for training. Visualization is done with
the pre-trained decoder.

dataset, namely GAN (Goodfellow et al., 2014), Wasser-
stein GAN (W-GAN) (Arjovsky et al., 2017) and the Sliced-
Wasserstein Generator (SWG) (Deshpande et al., 2018).
The visual comparison suggests that the samples generated
by SWF are of slightly better quality than those, although
research must still be undertaken to scale up to high dimen-
sions without an AE.

We also provide the outcome of the pre-trained SWF with
samples that are regularly spaced in between those used
for training. The result is shown in Figure 4.2. This plot
suggests that SWF is a way to interpolate non-parametrically
in between latent spaces of regular AE.

5. Conclusion and Future Directions

In this study, we proposed SWF, an efÔ¨Åcient, nonparamet-
ric IGM algorithm. SWF is based on formulating IGM as
a functional optimization problem in Wasserstein spaces,
where the aim is to Ô¨Ånd a probability measure that is close
to the data distribution as much as possible while maintain-
ing the expressiveness at a certain level. SWF lies in the
intersection of OT, gradient Ô¨Çows, and SDEs, which allowed
us to convert the IGM problem to an SDE simulation prob-
lem. We provided Ô¨Ånite-time bounds for the inÔ¨Ånite-particle
regime and established explicit links between the algorithm
parameters and the overall error. We conducted several ex-
periments, where we showed that the results support our
theory: SWF is able to generate samples from non-trivial
distributions with low computational requirements.

The SWF algorithm opens up interesting future directions:
(i) extension to differentially private settings (Dwork &
Roth, 2014) by exploiting the fact that it only requires ran-
dom projections of the data, (ii) showing the convergence
scheme of the particle system (9) to the original SDE (8),
(iii) providing bounds directly for the particle scheme (10).

3

5

8

10

15

20

30

50

100

200

Figure 4. Initial random particles (left), particles through iterations
(middle, from 1 to 200 iterations) and closest sample from the
training dataset (right), for both MNIST and CelebA.

match some prior, as in VAE. In the following, we set Œª = 0,
NŒ∏ = 40000, d = 32 for MNIST and d = 64 for CelebA.

Assessing the validity of IGM algorithms is generally done
by visualizing the generated samples. Figure 3 shows some
particles after 500 iterations of SWF. We can observe they
are considerably accurate. Interestingly, the generated sam-
ples gradually take the form of either digits or faces along
the iterations, as seen on Figure 4. In this Ô¨Ågure, we also dis-
play the closest sample from the original database to check
we are not just reproducing training data.

For a visual comparison, we provide the results presented in
(Deshpande et al., 2018) in Figure 5. These results are ob-
tained by running different IGM approaches on the MNIST

Figure 5. Performance of GAN (left), W-GAN (middle), SWG
(right) on MNIST. (The Ô¨Ågure is directly taken from (Deshpande
et al., 2018).)

77.588.599.510 8.36 9.39 10.22 11.2log(n)log(E[ÀúW22(ÀÜPd,ÀÜP d)])MNISTTFDCelebALSUNFigure3.LimitedsampleestimateoftheslicedWassersteindistanceasafunctionofthesamplesize.051015200.40.81.612.94 10 3Traininginthousandsofiterations Trainingloss=E[ÀúW22(ÀÜPd,ÀÜPf)]Samplesize12825651210241282565121024Figure4.TrainingwithdifferentsamplesizesonMNIST.ThedashedlinesdenoteE[ÀúW22(ÀÜPd,ÀÜP d)].andshowinFig.3,howE[ÀúW22(ÀÜPd,ÀÜP d)]decreaseswiththenumberofsamplesusedforestimation.Toobtainthisquan-titywetaketwosetsofnsamples,eachfromthedatadistri-butionPd.WethencomputetheslicedWassersteindistancebetweenthosesetsinthemannerdescribedinAlg.1.WeobservethatE[ÀúW22(ÀÜPd,ÀÜP d)]decreasesroughlyviaO(n 1).UsingCorollary1,thisimpliesthatÀúW22(Pd,P f)decreasesinO(n 1)fortheoptimalsolutionP f.Totestthequalityofthislossestimate,wetrainafullyconnecteddeepnetbasedgeneratorontheslicedWassersteindistancewithdifferentsamplesizesfortheMNISTdataset.EachconÔ¨Ågurationwastrained5timeswithrandomlysetseeds,andtheaverageswitherrorbarsarepresentedinFig.4.Duringtraining,ateveryiteration,gradientsarecomputedusing10,000randomprojections.WeemphasizethesmallGANWGANSWGConvConv+BNFCFC+BNFigure5.MNISTsamplesafter40ktrainingiterationsfordiffer-entgeneratorconÔ¨Ågurations.Batchsize=250,Learningrate=0.0005,Adamoptimizererrorbarswhichhighlightthestabilityoftheproposedap-proach.Thegeneratorisabletoproducegoodimagesinallfourcases.Thisshowsthat,inpractice,asetofasfewas128sam-plesisgoodenoughforsimpledistributions.ThegeneratorisabletobeatE[ÀúW22(ÀÜPd,ÀÜP d)](dashedblackline)ontheloss,indicatingthatithasprobablyconvergedinallcases.Asthenumberofsamplesincreases,weseethisboundgettingtighter.4.2.StabilityofTrainingTodemonstratethestabilityoftheproposedapproach,fourdifferentgeneratorarchitecturesaretrainedwithourmethodaswellasthetwoaforementionedbaselinesusingexactlythesamesetofhyperparameters.Onegeneratoriscomposedoffullyconnectedlayerswhiletheotheriscom-posedofconvolutionalanddeconvolutionallayers.Foreachgeneratorweassessitsperformancewhenusingandwhennotusingbatchnormalization[12].ThearchitecturesaredescribedinmoredetailinAppendixD.Forthisexperiment,onlytheGANandWassersteinGANuseadiscriminator,whileourapproachreliesonrandomprojectionsinstead.Furthernotethatthesearchitecturesarearbitrarilychosen,andthiscomparisonisonlyintendedtoshowhowthetrain-ingstabilitycomparesacrossdifferentmethods,aswellashowtheslicedWassersteinlosscorrelateswiththegeneratedsamples.Thisisnottocomparethebestpossiblesamplesfromdifferenttrainingmethods.Samplesobtainedfromtheresultinggeneratorarevisu-Sliced-Wasserstein Flows

Acknowledgments

This work is partly supported by the French National
Research Agency (ANR) as a part of the FBIMATRIX
(ANR-16-CE23-0014) and KAMoulox (ANR-15-CE38-
0003-01) projects. Szymon Majewski is partially sup-
ported by Polish National Science Center grant number
2016/23/B/ST1/00454.

References

Ambrosio, L., Gigli, N., and Savar¬¥e, G. Gradient Ô¨Çows: in
metric spaces and in the space of probability measures.
Springer Science & Business Media, 2008.

Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein gen-
erative adversarial networks. In International Conference
on Machine Learning, pp. 214‚Äì223, 2017.

Benamou, J.-D. and Brenier, Y. A computational Ô¨Çuid me-
chanics solution to the Monge-Kantorovich mass transfer
problem. Numerische Mathematik, 84(3):375‚Äì393, 2000.

Bogachev, V. I., Krylov, N. V., R¬®ockner, M., and Shaposh-
nikov, S. V. Fokker-Planck-Kolmogorov Equations, vol-
ume 207. American Mathematical Soc., 2015.

Bonneel, N., Rabin, J., Peyr¬¥e, G., and PÔ¨Åster, H. Sliced and
Radon Wasserstein barycenters of measures. Journal of
Mathematical Imaging and Vision, 51(1):22‚Äì45, 2015.

Bonnotte, N. Unidimensional and evolution methods for
optimal transportation. PhD thesis, Paris 11, 2013.

Bossy, M. and Talay, D. A stochastic particle method for the
McKean-Vlasov and the Burgers equation. Mathematics
of Computation of the American Mathematical Society,
66(217):157‚Äì192, 1997.

Dalalyan, A. S. Theoretical guarantees for approximate
sampling from smooth and log-concave densities. Jour-
nal of the Royal Statistical Society: Series B (Statistical
Methodology), 79(3):651‚Äì676, 2017.

Deshpande, I., Zhang, Z., and Schwing, A. Generative
modeling using the sliced wasserstein distance. arXiv
preprint arXiv:1803.11188, 2018.

Diggle, P. J. and Gratton, R. J. Monte carlo methods of
inference for implicit statistical models. Journal of the
Royal Statistical Society. Series B (Methodological), pp.
193‚Äì227, 1984.

Donoho, D. and Tanner, J. Observed universality of phase
transitions in high-dimensional geometry, with implica-
tions for modern data analysis and signal processing.
Philosophical Transactions of the Royal Society of Lon-
don A: Mathematical, Physical and Engineering Sciences,
367(1906):4273‚Äì4293, 2009.

Durmus, A., S¬∏ ims¬∏ekli, U., Moulines, E., Badeau, R., and
Richard, G. Stochastic gradient Richardson-Romberg
Markov Chain Monte Carlo. In NIPS, 2016.

Dwork, C. and Roth, A. The algorithmic foundations of
differential privacy. Foundations and Trends R(cid:13) in Theo-
retical Computer Science, 9(3‚Äì4):211‚Äì407, 2014.

Genevay, A., Cuturi, M., Peyr¬¥e, G., and Bach, F. Stochastic
In Ad-
optimization for large-scale optimal transport.
vances in Neural Information Processing Systems, pp.
3440‚Äì3448, 2016.

Genevay, A., Peyr¬¥e, G., and Cuturi, M. Gan and vae
from an optimal transport point of view. arXiv preprint
arXiv:1706.01807, 2017.

Bousquet, O., Gelly, S., Tolstikhin, I., Simon-Gabriel, C.-J.,
and Schoelkopf, B. From optimal transport to gener-
the vegan cookbook. arXiv preprint
ative modeling:
arXiv:1705.07642, 2017.

Genevay, A., Peyr¬¥e, G., and Cuturi, M. Learning genera-
tive models with Sinkhorn divergences. In International
Conference on ArtiÔ¨Åcial Intelligence and Statistics, pp.
1608‚Äì1617, 2018.

Cattiaux, P., Guillin, A., and Malrieu, F. Probabilistic ap-
proach for granular media equations in the non uniformly
convex case. Prob. Theor. Rel. Fields, 140(1-2):19‚Äì40,
2008.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A., and Bengio,
Y. Generative adversarial nets. In Advances in neural
information processing systems, pp. 2672‚Äì2680, 2014.

S¬∏ ims¬∏ekli, U., Yildiz, C., Nguyen, T. H., Cemgil, A. T.,
and Richard, G. Asynchronous stochastic quasi-Newton
MCMC for non-convex optimization. In ICML, pp. 4674‚Äì
4683, 2018.

Cuturi, M. Sinkhorn distances: Lightspeed computation
of optimal transport. In Advances in neural information
processing systems, pp. 2292‚Äì2300, 2013.

Gribonval, R., Blanchard, G., Keriven, N., and Traonmilin,
Y. Compressive statistical learning with random feature
moments. arXiv preprint arXiv:1706.07180, 2017.

Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and
Courville, A. C. Improved training of Wasserstein GANs.
In Advances in Neural Information Processing Systems,
pp. 5769‚Äì5779, 2017.

Sliced-Wasserstein Flows

Guo, X., Hong, J., Lin, T., and Yang, N. Relaxed
Wasserstein with applications to GANs. arXiv preprint
arXiv:1705.07164, 2017.

Jordan, R., Kinderlehrer, D., and Otto, F. The variational
formulation of the Fokker‚ÄìPlanck equation. SIAM journal
on mathematical analysis, 29(1):1‚Äì17, 1998.

Kingma, D. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.

Kingma, D. P. and Welling, M. Auto-encoding variational

bayes. arXiv preprint arXiv:1312.6114, 2013.

Kolouri, S., Martin, C. E., and Rohde, G. K. Sliced-
wasserstein autoencoder: An embarrassingly simple gen-
erative model. arXiv preprint arXiv:1804.01947, 2018.

Lavenant, H., Claici, S., Chien, E., and Solomon, J. Dynam-
ical optimal transport on discrete surfaces. In SIGGRAPH
Asia 2018 Technical Papers, pp. 250. ACM, 2018.

Lei, N., Su, K., Cui, L., Yau, S.-T., and Gu, D. X. A
geometric view of optimal transportation and generative
model. arXiv preprint arXiv:1710.05488, 2017.

Liu, S., Bousquet, O., and Chaudhuri, K. Approxima-
tion and convergence properties of generative adversarial
learning. In Advances in Neural Information Processing
Systems, pp. 5551‚Äì5559, 2017.

Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face
attributes in the wild. In Proceedings of International
Conference on Computer Vision (ICCV), 2015.

Ma, Y. A., Chen, T., and Fox, E. A complete recipe for
stochastic gradient MCMC. In Advances in Neural Infor-
mation Processing Systems, pp. 2899‚Äì2907, 2015.

Malrieu, F. Convergence to equilibrium for granular media
equations and their Euler schemes. Ann. Appl. Probab.,
13(2):540‚Äì560, 2003.

Mishura, Y. S. and Veretennikov, A. Y. Existence and
uniqueness theorems for solutions of McKean‚ÄìVlasov
stochastic equations. arXiv preprint arXiv:1603.02212,
2016.

Mohamed, S. and Lakshminarayanan, B.

ing in implicit generative models.
arXiv:1610.03483, 2016.

Learn-
arXiv preprint

A. M., and Bronstein, M. M. (eds.), Scale Space and
Variational Methods in Computer Vision, pp. 435‚Äì446,
Berlin, Heidelberg, 2012. Springer Berlin Heidelberg.
ISBN 978-3-642-24785-9.

Raginsky, M., Rakhlin, A., and Telgarsky, M. Non-convex
learning via stochastic gradient Langevin dynamics: a
nonasymptotic analysis. In Proceedings of the 2017 Con-
ference on Learning Theory, volume 65, pp. 1674‚Äì1703,
2017.

Samangouei, P., Kabkab, M., and Chellappa, R. Defense-
GAN: Protecting classiÔ¨Åers against adversarial attacks
using generative models. In International Conference on
Learning Representations, 2018.

Santambrogio, F. Introduction to optimal transport theory.
In Pajot, H., Ollivier, Y., and Villani, C. (eds.), Opti-
mal Transportation: Theory and Applications, chapter 1.
Cambridge University Press, 2014.

Santambrogio, F. {Euclidean, metric, and Wasserstein}
gradient Ô¨Çows: an overview. Bulletin of Mathematical
Sciences, 7(1):87‚Äì154, 2017.

S¬∏ ims¬∏ekli, U. Fractional Langevin Monte Carlo: Explor-
ing L¬¥evy Driven Stochastic Differential Equations for
Markov Chain Monte Carlo. In International Conference
on Machine Learning, 2017.

Tolstikhin, I., Bousquet, O., Gelly, S., and Schoelkopf,
arXiv preprint

B. Wasserstein auto-encoders.
arXiv:1711.01558, 2017.

Van der Vaart, A. W. Asymptotic statistics, volume 3. Cam-

bridge university press, 1998.

Veretennikov, A. Y. On ergodic measures for McKean-
Vlasov stochastic equations. In Monte Carlo and Quasi-
Monte Carlo Methods 2004, pp. 471‚Äì486. Springer, 2006.

Villani, C. Optimal transport: old and new, volume 338.

Springer Science & Business Media, 2008.

Welling, M. and Teh, Y. W. Bayesian learning via stochastic
gradient Langevin dynamics. In International Conference
on Machine Learning, pp. 681‚Äì688, 2011.

Wu, J., Huang, Z., Li, W., and Gool, L. V. Sliced wasserstein
generative models. arXiv preprint arXiv:1706.02631,
abs/1706.02631, 2018.

Nguyen, T. H., S¬∏ ims¬∏ekli, U., , and Richard, G. Non-
asymptotic analysis of fractional Langevin Monte Carlo
for non-convex optimization. In ICML, 2019.

Zhang, J., Zhang, R., and Chen, C. Stochastic particle-
optimization sampling and the non-asymptotic conver-
gence theory. arXiv preprint arXiv:1809.01293, 2018.

Rabin, J., Peyr¬¥e, G., Delon, J., and Bernot, M. Wasser-
stein barycenter and its application to texture mixing. In
Bruckstein, A. M., ter Haar Romeny, B. M., Bronstein,

Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal
Transport and Diffusions
SUPPLEMENTARY DOCUMENT

Antoine Liutkus 1 Umut S¬∏ ims¬∏ekli 2 Szymon Majewski 3 Alain Durmus 4 Fabian-Robert St¬®oter 1

1. Proof of Theorem 2

We Ô¨Årst need to generalize (Bonnotte, 2013)[Lemma 5.4.3] to distribution œÅ ‚àà L‚àû(B(0, r)), r > 0.
Theorem S4. Let ŒΩ be a probability measure on B(0, 1) with a strictly positive smooth density. Fix a time step h > 0,
regularization constant Œª > 0 and a radius r >
d. For any probability measure ¬µ0 on B(0, r) with density œÅ0 ‚àà
L‚àû(B(0, r)), there is a probability measure ¬µ on B(0, r) minimizing:

‚àö

G(¬µ) = F ŒΩ

Œª (¬µ) +

1
2h

W 2

2 (¬µ, ¬µ0),

where F ŒΩ

Œª is given by (5). Moreover the optimal ¬µ has a density œÅ on B(0, r) and:

||œÅ||L‚àû ‚â§ (1 + h/

‚àö

d)d||œÅ0||L‚àû.

(S1)

Proof. The set of measures supported on B(0, r) is compact in the topology given by W2 metric. Furthermore by (Ambrosio
et al., 2008)[Lemma 9.4.3] H is lower semicontinuous on (P(B(0, r)), W2). Since by (Bonnotte, 2013)[Proposition 5.1.2,
Proposition 5.1.3], SW2 is a distance on P(B(0, r)), dominated by d‚àí1/2W2, we have:

|SW2(œÄ0, ŒΩ) ‚àí SW2(œÄ1, ŒΩ)| ‚â§ SW2(œÄ0, œÄ1) ‚â§

1
‚àö
d

W2(œÄ0, œÄ1).

The above means that SW2(¬∑, ŒΩ) is continuous with respect to topology given by W2, which implies that SW 2
2 (¬∑, ŒΩ) is
continuous in this topology as well. Therefore G : P(B(0, r)) ‚Üí (‚àí‚àû, +‚àû] is a lower semicontinuous function on the
compact set (P(B(0, r)), W2). Hence there exists a minimum ¬µ of G on P(B(0, r)). Furthermore, since H(œÄ) = +‚àû for
measures œÄ that do not admit a density with respect to Lebesgue measure, the measure ¬µ must admit a density œÅ.

If œÅ0 is smooth and positive on B(0, r), the inequality S1 is true by (Bonnotte, 2013)[Lemma 5.4.3.] When œÅ0 is just in
L‚àû(B(0, r)), we proceed by smoothing. For t ‚àà (0, 1], let œÅt be a function obtained by convolution of œÅ0 with a Gaussian
kernel (t, x, y) (cid:55)‚Üí (2œÄ)d/2 exp((cid:107)x ‚àí y(cid:107)2 /2), restricting the result to B(0, r) and normalizing to obtain a probability density.
Then (œÅt)t are smooth positive densities, and it is easy to see that limt‚Üí0 ||œÅt||L‚àû ‚â§ ||œÅ0||L‚àû . Furthermore, if we denote
by ¬µt the measure on B(0, r) with density œÅt, then ¬µt converge weakly to ¬µ0. For t ‚àà (0, 1] let ÀÜ¬µt be the minimum of
F ŒΩ

2 (¬∑, ¬µt), and let ÀÜœÅt be the density of ÀÜ¬µt. Using (Bonnotte, 2013)[Lemma 5.4.3.] we get

Œª (¬∑) + 1

2h W 2

||ÀÜœÅt||L‚àû ‚â§ (1 + h

‚àö

d)d||œÅt||L‚àû .

so ÀÜœÅt lies in a ball of Ô¨Ånite radius in L‚àû. Using compactness of P(B(0, r)) in weak topology and compactness of closed
ball in L‚àû(B(0, r)) in weak star topology, we can choose a subsequence ÀÜ¬µtk , ÀÜœÅtk , limk‚Üí+‚àû tk = 0, that converges along
that subsequence to limits ÀÜ¬µ, ÀÜœÅ. Obviously ÀÜœÅ is the density of ÀÜ¬µ, since for any continuous function f on B(0, r) we have:

(cid:90)

(cid:90)

ÀÜœÅf dx = lim
k‚Üí‚àû

œÅtk f dx = lim
k‚Üí‚àû

(cid:90)

f d¬µtk =

(cid:90)

f d¬µ.

Furthermore, since ÀÜœÅ is the weak star limit of a bounded subsequence, we have:

||ÀÜœÅ||L‚àû ‚â§ lim sup
k‚Üí‚àû

(1 + h

‚àö

d)d||œÅtk ||L‚àû ‚â§ (1 + h

‚àö

d)d||œÅ0||L‚àû .

Sliced-Wasserstein Flows

To Ô¨Ånish, we just need to prove that ÀÜ¬µ is a minimum of G. We remind our reader, that we already established existence of
some minimum ¬µ (that might be different from ÀÜ¬µ). Since ÀÜ¬µtk converges weakly to ÀÜ¬µ in P(B(0, r)), it implies convergence
in W2 as well since B(0, r) is compact. Similarly ¬µtk converges to ¬µ0 in W2. Using the lower semicontinuity of G we now
have:

F ŒΩ

Œª (ÀÜ¬µ) +

W 2

2 (ÀÜ¬µ, ¬µ0) ‚â§ lim inf
k‚Üí‚àû

1
2h

(cid:18)

F ŒΩ

Œª (ÀÜ¬µtk ) +

W 2

2 (ÀÜ¬µtk , ¬µ0)

(cid:19)

1
2h

1
2h
1
2h

F ŒΩ

Œª (¬µ) +

W 2

2 (¬µ, ¬µtk )

‚â§ lim inf
k‚Üí‚àû
1
2h
= F ŒΩ

W 2

+

2 (ÀÜ¬µtk , ¬µ0) ‚àí
1
2h

W 2

Œª (¬µ) +

2 (¬µ, ¬µ0),

W 2

2 (ÀÜ¬µtk , ¬µtk )

where the second inequality comes from the fact, that ÀÜ¬µtk minimizes F ŒΩ
previously established facts, it follows that ÀÜ¬µ is a minimum of G with density satisfying S1.

Œª (¬∑) + 1

2h W 2

2 (¬∑, ¬µtk ). From the above inequality and

DeÔ¨Ånition 1. Minimizing movement scheme Let r > 0 and F : R+ √ó P(B(0, r)) √ó P(B(0, r)) ‚Üí R be a functional. Let
¬µ0 ‚àà P(B(0, r)) be a starting point. For h > 0 a piecewise constant trajectory ¬µh : [0, ‚àû) ‚Üí P(B(0, r)) for F starting at
¬µ0 is a function such that:

‚Ä¢ ¬µh(0) = ¬µ0.

‚Ä¢ ¬µh is constant on each interval [nh, (n + 1)h), so ¬µh(t) = ¬µh(nh) with n = (cid:98)t/h(cid:99).

‚Ä¢ ¬µh((n + 1)h) minimizes the functional Œ∂ (cid:55)‚Üí F(h, Œ∂, ¬µh(nh)), for all n ‚àà N.

We say ÀÜ¬µ is a minimizing movement scheme for F starting at ¬µ0, if there exists a family of piecewise constant trajectory
(¬µh)h>0 for F such that ÀÜ¬µ is a pointwise limit of ¬µh as h goes to 0, i.e. for all t ‚àà R+, limh‚Üí0 ¬µh(t) = ¬µ(t) in P(B(0, r)).
We say that Àú¬µ is a generalized minimizing movement for F starting at ¬µ0, if there exists a family of piecewise constant
trajectory (¬µh)h>0 for F and a sequence (hn)n, limn‚Üí‚àû hn = 0, such that ¬µhn converges pointwise to Àú¬µ.
Theorem S5. Let ŒΩ be a probability measure on B(0, 1) with a strictly positive smooth density. Fix a regularization constant
d. Given an absolutely continuous measure ¬µ0 ‚àà P(B(0, r)) with density œÅ0 ‚àà L‚àû(B(0, r)), there
Œª > 0 and radius r >
is a generalized minimizing movement scheme (¬µt)t in P(B(0, r)) starting from ¬µ0 for the functional deÔ¨Åned by

‚àö

F ŒΩ(h, ¬µ+, ¬µ‚àí) = F ŒΩ

Œª (¬µ+) +

1
2h

W 2

2 (¬µ+, ¬µ‚àí).

(S2)

Moreover for any time t > 0, the probability measure ¬µt = ¬µ(t) has density œÅt with respect to the Lebesgue measure and:

||œÅt||L‚àû ‚â§ edt

‚àö

d||œÅ0||L‚àû .

(S3)

Proof. We start by noting, that by S4 for any h > 0 there exists a piecewise constant trajectory ¬µh for S2 starting at ¬µ0.
Furthermore for t ‚â• 0 measure ¬µh
t , and:
‚àö

t = ¬µh(t) has density œÅh

||œÅh

t ||L‚àû ‚â§ ed

d(t+h)||œÅ0||L‚àû .

(S4)

Let us choose T > 0. We denote œÅh(t, x) = œÅh
t (x). For h ‚â§ 1, the functions œÅh lie in a ball in L‚àû([0, T ] √ó B(0, r)), so
from Banach-Alaoglu theorem there is a sequence hn converging to 0, such that œÅhn converges in weak-star topology in
L‚àû([0, T ] √ó B(0, r)) to a certain limit œÅ. Since œÅ has to be nonnegative except for a set of measure zero, we assume œÅ is
nonnegative. We denote œÅt(x) = œÅ(t, x). We will prove that for almost all t, œÅt is a probability density and ¬µhn
converges
t
in W2 to a measure ¬µt with density œÅt.

First of all, for almost all t ‚àà [0, T ], œÅt is a probability density, since for any Borel set A ‚äÜ [0, T ] the indicator of set
A √ó B(0, r) is integrable, and hence by deÔ¨Ånition of the weak-star topology:

(cid:90)

(cid:90)

A

B(0,r)

œÅt(x)dxdt = lim
n‚Üí‚àû

(cid:90)

(cid:90)

A

B(0,r)

œÅhn
t (x)dxdt,

Sliced-Wasserstein Flows

and so we have to have (cid:82) œÅt(x)dx = 1 for almost all t ‚àà [0, T ]. Nonnegativity of œÅt follows from nonnegativity of œÅ.
We will now prove, that for almost all t ‚àà [0, T ] the measures ¬µhn
t
take Œ¥ < min(T ‚àí t, t) and Œ∂ ‚àà C1(B(0, r)). We have:

converge to a measure with density œÅt. Let t ‚àà (0, T ),

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

B(0,r)

Œ∂d¬µhn

t ‚àí

(cid:90)

B(0,r)

Œ∂d¬µhm
t

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

B(0,r)

Œ∂d¬µhn

t ‚àí

1
2Œ¥

‚â§

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:90) t+Œ¥

(cid:90)

t‚àíŒ¥

B(0,r)

(cid:12)
(cid:90)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:90) t+Œ¥

B(0,r)

(cid:90)

Œ∂d¬µhn

(cid:12)
(cid:12)
(cid:12)
s ds
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
2Œ¥

t‚àíŒ¥

B(0,r)

Œ∂d¬µhm

t ‚àí

1
2Œ¥

(cid:90) t+Œ¥

(cid:90)

t‚àíŒ¥

B(0,r)

Œ∂d¬µhm

(cid:12)
(cid:12)
(cid:12)
s ds
(cid:12)
(cid:12)

+

Œ∂d¬µhm

s ds ‚àí

1
2Œ¥

(cid:90) t+Œ¥

(cid:90)

t‚àíŒ¥

B(0,r)

Œ∂d¬µhn

(cid:12)
(cid:12)
(cid:12)
s ds
(cid:12)
(cid:12)

.

(S5)

t have densities œÅhn
t

Because ¬µhn
right hand side converges to zero, as n, m ‚Üí ‚àû. Next, we get a bound on the other two terms.
First, if we denote by Œ≥ the optimal transport plan between ¬µhn
t

s , we have:

and ¬µhn

and both œÅhn, œÅhm converge to œÅ in weak-star topology, the last element of the sum on the

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

B(0,r)

Œ∂d¬µhn

t ‚àí

(cid:90)

B(0,r)

Œ∂d¬µhn
s

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

‚â§

B(0,r)√óB(0,r)

|Œ∂(x) ‚àí Œ∂(y)|2 dŒ≥(x, y) ‚â§ ||‚àáŒ∂||2

‚àûW 2

2 (¬µhn
t

, ¬µhn

s ).

(S6)

In addition, for nt = (cid:98)t/hn(cid:99) and ns = (cid:98)s/hn(cid:99) we have ¬µhn

t = ¬µhn

and ¬µhn

s = ¬µhn

nshn

. For all k ‚â• 0 we have:

W 2

2 (¬µhn
khn

, ¬µhn

(k+1)hn

) ‚â§ 2hn(F ŒΩ

) ‚àí F ŒΩ

Œª (¬µhn

(k+1)hn

).

(S7)

nthn
Œª (¬µhn
khn

Using this result and (S6) and assuming without loss of generality nt ‚â§ ns, from the Cauchy-Schwartz inequality we get:

W 2

2 (¬µhn
t

, ¬µhn

s ) ‚â§

(cid:32)ns‚àí1
(cid:88)

k=nt

W2(¬µhn
khn

, ¬µhn

(k+1)hn

(cid:33)2
)

‚â§ |nt ‚àí ns|

ns1
(cid:88)

k=nt

W 2

2 (¬µhn
khn

, ¬µhn

(k+1)hn

)

‚â§ 2hn|nt ‚àí ns|(F ŒΩ

Œª (¬µhn

nthn

) ‚àí F ŒΩ

Œª (¬µhn

nshn

)) ‚â§ 2C(|t ‚àí s| + hn),

(S8)

where we used for the last inequality, denoting C = F ŒΩ
(S7) and minP(B(0,r)) F ŒΩ
and S6 we get:

Œª is Ô¨Ånite since F ŒΩ

))n is non-increasing by
Œª is lower semi-continuous. Finally, using Jensen‚Äôs inequality, the above bound

Œª (¬µ0) ‚àí minP(B(0,r)) F ŒΩ

Œª , that (F ŒΩ

Œª (¬µhn
khn

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

B(0,r)

Œ∂d¬µhn

t ‚àí

1
2Œ¥

(cid:90) t+Œ¥

(cid:90)

t‚àíŒ¥

B(0,r)

2

Œ∂d¬µhn

(cid:12)
(cid:12)
(cid:12)
s ds
(cid:12)
(cid:12)

‚â§

‚â§

(cid:90)

1
2Œ¥

(cid:90) t+Œ¥

t‚àíŒ¥

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

C||‚àáŒ∂||2
‚àû
Œ¥

B(0,r)
(cid:90) t+Œ¥

‚â§ 2C||‚àáŒ∂||2

t‚àíŒ¥
‚àû(hn + Œ¥).

Œ∂d¬µhn

t ‚àí

(cid:90)

B(0,r)

Œ∂d¬µhn
s

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ds

(|t ‚àí s| + hn)ds

B(0,r) Œ∂d¬µhn

Together with (S5), when taking Œ¥ = hn, this result means that (cid:82)
other hand, since œÅhn converges to œÅ in weak-star topology on L‚àû, the limit of (cid:82)
for almost all t ‚àà (0, T ). This means that for almost all t ‚àà [0, T ] sequence ¬µhn
t
Let S ‚àà [0, T ] be the set of times such that for t ‚àà S sequence ¬µhn
converges to ¬µt. As we established almost all points
t
from [0, T ] belong to S. Let t ‚àà [0, T ] \ S. Then, there exists a sequence of times tk ‚àà S converging to t, such that ¬µtk
converge to some limit ¬µt. We have:
W2(¬µhn
t

is a Cauchy sequence for all t ‚àà (0, T ). On the
B(0,r) Œ∂d¬µhn
B(0,r) Œ∂(x)œÅt(x)dx
converges to a measure ¬µt with density œÅt.

, ¬µtk ) + W2(¬µtk , ¬µt).

, ¬µt) ‚â§ W2(¬µhn
t

t has to be (cid:82)

) + W2(¬µhn
tk

, ¬µhn
tk

t

From which we have for all k ‚â• 1:

Sliced-Wasserstein Flows

lim sup
n‚Üí‚àû

W2(¬µhn
t

, ¬µt) ‚â§ W2(¬µtk , ¬µt) + lim sup
n‚Üí‚àû

W2(¬µhn
t

, ¬µhn
tk

),

and using (S8), we get ¬µhn
so we can choose a subsequence of œÅhn
t

t ‚Üí ¬µt. Furthermore, the measure ¬µt has to have density, since œÅhn
t

lie in a ball in L‚àû(B(0, r)),
converging in weak-star topology to a certain limit ÀÜœÅt, which is the density of ¬µt.

n be a sequence converging to 0, such that ¬µh1

We use now the diagonal argument to get convergence for all t > 0. Let (Tk)‚àû
inÔ¨Ånity. Let h1
above, we can choose a subsequence h2
construct subsequences hk
n, and in the end take hn = hn
t > 0, and ¬µt has a density satisfying the bound from the statement of the theorem.

k=1 be a sequence of times increasing to
converge to ¬µt for all t ‚àà [0, T1]. Using the same arguments as
converges to a limit ¬µt for all t ‚àà [0, T2]. Inductively, we
converges to ¬µt for all

n. For this subsequence we have that ¬µhn
t

n, such that ¬µh2

n of h1

n

n

t

t

Finally, note that (S5) follows from (S4).

Theorem S6. Let (¬µt)t‚â•0 be a generalized minimizing movement scheme given by Theorem S5 with initial distribution ¬µ0
with density œÅ0 ‚àà L(B(0, r)). We denote by œÅt the density of ¬µt for all t ‚â• 0. Then œÅt satisÔ¨Åes the continuity equation:

‚àÇœÅt
‚àÇt

+ div(vtœÅt) + Œª‚àÜœÅt = 0 ,

vt(x) = ‚àí

(cid:90)

Sd‚àí1

œà(cid:48)

t,Œ∏((cid:104)x, Œ∏(cid:105))Œ∏dŒ∏,

in a weak sense, that is for all Œæ ‚àà C‚àû

c ([0, ‚àû) √ó B(0, r)) we have:

(cid:90) ‚àû

(cid:90)

0

B(0,r)

(cid:20) ‚àÇŒæ
‚àÇt

(cid:21)
(t, x) ‚àí vt‚àáŒæ(t, x) ‚àí Œª‚àÜŒæ(t, x)

œÅt(x)dxdt = ‚àí

(cid:90)

B(0,r)

Œæ(0, x)œÅ0(x)dx.

Proof. Our proof is based on the proof of (Bonnotte, 2013)[Theorem 5.6.1]. We proceed in Ô¨Åve steps.

(1) Let hn ‚Üí 0 be a sequence given by Theorem S5, such that ¬µhn
t
¬µhn have densities œÅhn that converge to œÅ in Lr, for r ‚â• 1, and in weak-star topology in L‚àû. Let Œæ ‚àà C‚àû
We denote Œæn

k (x) = Œæ(khn, x). Using part 1 of the proof of (Bonnotte, 2013)[Theorem 5.6.1], we obtain:

converges to ¬µt pointwise. Furthermore we know that
c ([0, ‚àû) √ó B(0, r)).

(cid:90)

B(0,r)

Œæ(0, x)œÅ0(x)dx +

(cid:90) ‚àû

(cid:90)

0

B(0,r)

‚àÇŒæ
‚àÇt

(t, x)œÅt(x)dxdt

= lim
n‚Üí‚àû

‚àíhn

‚àû
(cid:88)

(cid:90)

k=1

B(0,r)

Œæn
k (x)

œÅhn
khn

(x) ‚àí œÅhn

(k‚àí1)hn

(x)

hn

dx.

(S9)

(2) Again, this part is the same as part 2 of the proof of (Bonnotte, 2013)[Theorem 5.6.1]. For any Œ∏ ‚àà Sd‚àí1 we denote by
#ŒΩ, and by œàhn
œàt,Œ∏ the unique Kantorovich potential from Œ∏‚àó
#¬µt to Œ∏‚àó
#ŒΩ.
Then, by the same reasoning as part 2 of the proof of (Bonnotte, 2013)[Theorem 5.6.1], we get:

t,Œ∏ the unique Kantorovich potential from Œ∏‚àó

#¬µhn

to Œ∏‚àó

t

(cid:90) ‚àû

(cid:90)

(cid:90)

0

B(0,r)

Sd‚àí1

(œàt,Œ∏)(cid:48)((cid:104)Œ∏, x(cid:105))(cid:104)Œ∏, ‚àáŒæ(x, t)(cid:105)dŒ∏d¬µt(x)dt

= lim
n‚Üí‚àû

hn

‚àû
(cid:88)

(cid:90)

(cid:90)

k=1

B(0,r)

Sd‚àí1

œàhn
khn,Œ∏(Œ∏‚àó)(cid:104)Œ∏, ‚àáŒæn

k (cid:105)dŒ∏d¬µhn
khn

.

(S10)

(3) Since Œæ is compactly supported and smooth, ‚àÜŒæ is Lipschitz, and so for any t ‚â• 0 if we take k = (cid:98)t/hn(cid:99) we get
|‚àÜŒæn

k (x) ‚àí ‚àÜŒæ(t, x)| ‚â§ Chn for some constant C. Let T > 0 be such that Œæ(t, x) = 0 for t > T . We have:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

‚àû
(cid:88)

k=1

(cid:90)

hn

B(0,r)

‚àÜŒæn

k (x)œÅhn
khn

(x)dx ‚àí

(cid:90) +‚àû

(cid:90)

0

B(0,r)

‚àÜŒæ(t, x)œÅhn

(cid:12)
(cid:12)
(cid:12)
t (x)dxdt
(cid:12)
(cid:12)

‚â§ CT hn.

Sliced-Wasserstein Flows

On the other hand, we know, that œÅhn converges to œÅ in weak star topology on L‚àû([0, T ] √ó B(0, r)), and ‚àÜŒæ is bounded, so:

lim
n‚Üí+‚àû

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90) +‚àû

(cid:90)

0

B(0,r)

‚àÜŒæ(t, x)œÅhn

t (x)dxdt ‚àí

(cid:90) +‚àû

(cid:90)

0

B(0,r)

‚àÜŒæ(t, x)œÅt(x)dxdt

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 0.

Combining those two results give:

lim
n‚Üí‚àû

hn

‚àû
(cid:88)

(cid:90)

k=1

B(0,r)

‚àÜŒæn

k (x)œÅhn
khn

(x)dx =

(cid:90) +‚àû

(cid:90)

0

B(0,r)

‚àÜŒæ(t, x)œÅt(x)dxdt.

(S11)

k denote the unique Kantorovich potential from ¬µhn
khn

(4) Let œÜhn
and 5.1.7], as well as (Jordan et al., 1998)[Equation (38)] with Œ® = 0, and optimality of ¬µhn
khn

to ¬µhn

(k‚àí1)hn

, we get:

. Using (Bonnotte, 2013)[Propositions 1.5.7

1
hn

(cid:90)

B(0,r)

(cid:104)‚àáœÜhn

k (x), ‚àáŒæn

k (x)(cid:105)d¬µhn
khn

(cid:90)

(cid:90)

(x) ‚àí

B(0,r)

Sd‚àí1

(œàhn
khn

)(cid:48)(Œ∏‚àó)(cid:104)Œ∏, ‚àáŒæn

k (x)(cid:105)dŒ∏d¬µhn
khn

(x)

(cid:90)

‚àí Œª

B(0,r)

‚àÜŒæn

k (x)d¬µhn
khn

(x),

(S12)

which is the derivative of F ŒΩ

Œª (¬∑) + 1
2hn

W 2

2 (¬∑, ¬µ(k‚àí1)hn ) in the direction given by vector Ô¨Åeld ‚àáŒæn

k is zero.

Let Œ≥ be the optimal transport between ¬µhn
khn

and ¬µhn

(k‚àí1)hn

. Then:

(cid:90)

B(0,r)

Œæn
k (x)

œÅhn
khn

(x) ‚àí œÅhn

(k‚àí1)hn

(x)

hn

dx =

1
hn

(cid:90)

B(0,r)

(cid:104)‚àáœÜhn

k (x), ‚àáŒæn

k (x)(cid:105)d¬µhn
khn

(x) =

1
hn

1
hn

(cid:90)

B(0,r)

(cid:90)

B(0,r)

(Œæn

k (y) ‚àí Œæn

k (x))dŒ≥(x, y).

(cid:104)‚àáŒæn

k (x), y ‚àí x(cid:105)dŒ≥(x, y).

(S13)

(S14)

Since Œæ is C‚àû
(cid:104)‚àáŒæ(x), y ‚àí x(cid:105)| ‚â§ C|x ‚àí y|2, and hence:

c , it has Lipschitz gradient. Let C be twice the Lipschitz constant of ‚àáŒæ. Then we have |Œæ(y) ‚àí Œæ(x) ‚àí

(cid:90)

B(0,r)

|Œæn

k (y) ‚àí Œæn

k (x) ‚àí (cid:104)‚àáŒæn

k (x), y ‚àí x(cid:105)|dŒ≥(x, y) ‚â§ CW 2

2 (¬µhn

(k‚àí1)hn

, ¬µhn
khn

).

(S15)

Combining (S13), (S14) and (S15), we get:

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

‚àû
(cid:88)

k=1

(cid:90)

hn

B(0,r)

Œæn
k (x)

œÅhn
khn

(k‚àí1)hn

‚àí œÅhn
hn

dx +

‚àû
(cid:88)

k=1

(cid:90)

hn

B(0,r)

As some F ŒΩ

Œª have a Ô¨Ånite minimum on P(B(0, r)), we have:

(cid:104)‚àáœÜhn

k , ‚àáŒæn

k (cid:105)d¬µhn
khn

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

‚â§ C

‚àû
(cid:88)

k=1

W 2

2 (¬µhn

(k‚àí1)hn

, ¬µhn
khn

).

(S16)

‚àû
(cid:88)

k=1

W 2

2 (¬µhn

(k‚àí1)hn

, ¬µhn
khn

) ‚â§ 2hn

‚â§ 2hn

‚àû
(cid:88)

k=1
(cid:32)

F ŒΩ

Œª (¬µhn

(k‚àí1)hn

) ‚àí F ŒΩ

Œª (¬µhn
khn

)

F ŒΩ

Œª (¬µ0) ‚àí min

P(B(0,r))

(cid:33)

F ŒΩ
Œª

.

(S17)

and so the sum on the right hand side of the equation goes to zero as n goes to inÔ¨Ånity.

Sliced-Wasserstein Flows

From (S16), (S17) and (S12) we conclude:

lim
n‚Üí‚àû

‚àíhn

‚àû
(cid:88)

k=1

Œæn
k (x)

œÅhn
khn

(k‚àí1)hn

‚àí œÅhn
hn

dx =

(cid:32)

lim
n‚Üí‚àû

hn

‚àû
(cid:88)

(cid:90)

(cid:90)

k=1

B(0,r)

Sd‚àí1

œàhn
khn,Œ∏(Œ∏‚àó)(cid:104)Œ∏, ‚àáŒæn

k (cid:105)dŒ∏d¬µhn
khn

+ hn

‚àû
(cid:88)

(cid:90)

k=1

B(0,r)

(cid:33)

‚àÜŒæn

k (x)œÅhn
khn

(x)dx

,

(S18)

where both limits exist, since the difference of left hand side and right hand side of the equation goes to zero, while the left
hand side converges to a Ô¨Ånite value by (S9).

(5) Combining (S9), (S10), (S11) and (S18) we get the result.

2. Proof of Theorem 3

Before proceeding to the proof, let us Ô¨Årst deÔ¨Åne the following Euler-Maruyama scheme which will be useful for our
analysis:

ÀÜXk+1 = ÀÜXk + hÀÜv( ÀÜXk, ¬µkh) +

‚àö

2ŒªhZn+1,

(S19)

where ¬µt denotes the probability distribution of Xt with (Xt)t being the solution of the original SDE (8). Now, consider the
probability distribution of ÀÜXk as ÀÜ¬µkh. Starting from the discrete-time process ( ÀÜXk)k‚ààN+, we Ô¨Årst deÔ¨Åne a continuous-time
process (Yt)t‚â•0 that linearly interpolates ( ÀÜXk)k‚ààN+, given as follows:

dYt = Àúvt(Y )dt +

‚àö

2ŒªdWt,

(S20)

where Àúvt(Y ) (cid:44) ‚àí (cid:80)‚àû
time process (Ut)t‚â•0 that linearly interpolates ( ¬ØXk)k‚ààN+, deÔ¨Åned by (13), given as follows:

k=0 ÀÜvkh(Ykh)1[kh,(k+1)h)(t) and 1 denotes the indicator function. Similarly, we deÔ¨Åne a continuous-

dUt = ¬Øvt(U )dt +

‚àö

2ŒªdWt,

(S21)

where ¬Øvt(U ) (cid:44) ‚àí (cid:80)‚àû
distributions of (Xt)t‚àà[0,T ], (Yt)t‚àà[0,T ] and (Ut)t‚àà[0,T ] as œÄT

k=0 ÀÜv(Ukh, ¬Ø¬µkh)1[kh,(k+1)h)(t) and ¬Ø¬µkh denotes the probability distribution of ¬ØXk. Let us denote the

X , œÄT

Y and œÄT

U respectively with T = Kh.

We consider the following assumptions:
HS1. For all Œª > 0, the SDE (8) has a unique strong solution denoted by (Xt)t‚â•0 for any starting point x ‚àà Rd.
HS2. There exits L < ‚àû such that

where vt(x) = v(x, ¬µt) and

(cid:107)vt(x) ‚àí vt(cid:48)(x(cid:48))(cid:107) ‚â§ L((cid:107)x ‚àí x(cid:48)(cid:107) + |t ‚àí t(cid:48)|),

(cid:107)ÀÜv(x, ¬µ) ‚àí ÀÜv(x(cid:48), ¬µ(cid:48))(cid:107) ‚â§ L((cid:107)x ‚àí x(cid:48)(cid:107) + (cid:107)¬µ ‚àí ¬µ(cid:48)(cid:107)TV).

HS3. For all t ‚â• 0, vt is dissipative, i.e. for all x ‚àà Rd,

(cid:104)x, vt(x)(cid:105) ‚â• m(cid:107)x(cid:107)2 ‚àí b,

(S22)

(S23)

(S24)

for some m, b > 0.
HS4. The estimator of the drift satisÔ¨Åes the following conditions: E[ÀÜvt] = vt for all t ‚â• 0, and for all t ‚â• 0, x ‚àà Rd,

E[(cid:107)ÀÜv(x, ¬µt) ‚àí v(x, ¬µt)(cid:107)2] ‚â§ 2Œ¥(L2(cid:107)x(cid:107)2 + B2),

(S25)

for some Œ¥ ‚àà (0, 1).

Sliced-Wasserstein Flows

HS5. For all t ‚â• 0: |Œ®t(0)| ‚â§ A and (cid:107)vt(0)(cid:107) ‚â§ B, for A, B ‚â• 0, where Œ®t = (cid:82)

Sd‚àí1 œàt((cid:104)Œ∏, ¬∑(cid:105))dŒ∏.

We start by upper-bounding (cid:107)ÀÜ¬µKh ‚àí ¬µT (cid:107)TV.
Lemma S1. Assume that the conditions HS2 to S5 hold. Then, the following bound holds:

(cid:107)ÀÜ¬µKh ‚àí ¬µT (cid:107)2

TV ‚â§ (cid:107)œÄT

Y ‚àí œÄT

X (cid:107)2

TV ‚â§

L2K
4Œª

(cid:16) C1h3
3

+ 3Œªdh2(cid:17)

+

C2Œ¥Kh
8Œª

,

(S26)

where C1 (cid:44) 12(L2C0 + B2) + 1, C2 (cid:44) 2(L2C0 + B2), C0 (cid:44) Ce + 2(1 ‚à® 1
of ¬µ0.

m )(b + 2B2 + dŒª), and Ce denotes the entropy

Proof. We use the proof technique presented in (Dalalyan, 2017; Raginsky et al., 2017). It is easy to verify that for all
k ‚àà N+, we have Ykh = ÀÜXk.

By Girsanov‚Äôs theorem to express the Kullback-Leibler (KL) divergence between these two distributions, given as follows:

KL(œÄT

X ||œÄT

Y ) =

=

=

1
4Œª

1
4Œª

1
4Œª

(cid:90) Kh

E[(cid:107)vt(Yt) + Àúvt(Y )(cid:107)2] dt

0
K‚àí1
(cid:88)

k=0

K‚àí1
(cid:88)

(cid:90) (k+1)h

kh

(cid:90) (k+1)h

k=0

kh

E[(cid:107)vt(Yt) + Àúvt(Y )(cid:107)2] dt

E[(cid:107)vt(Yt) ‚àí ÀÜvkh(Ykh)(cid:107)2] dt.

(S27)

(S28)

(S29)

By using vt(Yt) ‚àí ÀÜvkh(Ykh) = (vt(Yt) ‚àí vkh(Ykh)) + (vkh(Ykh) ‚àí ÀÜvkh(Ykh)), we obtain

KL(œÄT

X ||œÄT

Y ) ‚â§

1
2Œª

K‚àí1
(cid:88)

(cid:90) (k+1)h

k=0

kh

E[(cid:107)vt(Yt) ‚àí vkh(Ykh)(cid:107)2] dt

+

1
2Œª

K‚àí1
(cid:88)

(cid:90) (k+1)h

k=0

kh

E[(cid:107)vkh(Ykh) ‚àí ÀÜvkh(Ykh)(cid:107)2] dt

(S30)

‚â§

L2
Œª

K‚àí1
(cid:88)

(cid:90) (k+1)h

k=0

kh

(cid:0)E[(cid:107)Yt ‚àí Ykh(cid:107)2] + (t ‚àí kh)2(cid:1) dt

+

1
2Œª

K‚àí1
(cid:88)

(cid:90) (k+1)h

k=0

kh

E[(cid:107)vkh(Ykh) ‚àí ÀÜvkh(Ykh)(cid:107)2] dt.

(S31)

The last inequality is due to the Lipschitz condition HS2.
Now, let us focus on the term E[(cid:107)Yt ‚àí Ykh(cid:107)2]. By using (S20), we obtain:

Yt ‚àí Ykh = ‚àí(t ‚àí kh)ÀÜvkh(Ykh) + (cid:112)2Œª(t ‚àí kh)Z,
where Z denotes a standard normal random variable. By adding and subtracting the term ‚àí(t ‚àí kh)vkh(Ykh), we have:
Yt ‚àí Ykh = ‚àí(t ‚àí kh)vkh(Ykh) + (t ‚àí kh)(vkh(Ykh) ‚àí ÀÜvkh(Ykh)) + (cid:112)2Œª(t ‚àí kh)Z.

(S32)

(S33)

Taking the square and then the expectation of both sides yields:

E[(cid:107)Yt ‚àí Ykh(cid:107)2] ‚â§3(t ‚àí kh)2E[(cid:107)vkh(Ykh)(cid:107)2] + 3(t ‚àí kh)2E[(cid:107)vkh(Ykh) ‚àí ÀÜvkh(Ykh)(cid:107)2]

+ 6Œª(t ‚àí kh)d.

(S34)

As a consequence of HS2 and HS5, we have (cid:107)vt(x)(cid:107) ‚â§ L(cid:107)x(cid:107) + B for all t ‚â• 0, x ‚àà Rd. Combining this inequality with H
S4, we obtain:

E[(cid:107)Yt ‚àí Ykh(cid:107)2] ‚â§6(t ‚àí kh)2(L2E[(cid:107)Ykh(cid:107)2] + B2) + 6(t ‚àí kh)2(L2E[(cid:107)Ykh(cid:107)2] + B2)

+ 6Œª(t ‚àí kh)d

=12(t ‚àí kh)2(L2E[(cid:107)Ykh(cid:107)2] + B2) + 6Œª(t ‚àí kh)d.

(S35)

(S36)

Sliced-Wasserstein Flows

By Lemma 3.2 of (Raginsky et al., 2017)4, we have E[(cid:107)Ykh(cid:107)2] ‚â§ C0 (cid:44) Ce + 2(1 ‚à® 1
the entropy of ¬µ0. Using this result in the above equation yields:

m )(b + 2B2 + dŒª), where Ce denotes

E[(cid:107)Yt ‚àí Ykh(cid:107)2] ‚â§12(t ‚àí kh)2(L2C0 + B2) + 6Œª(t ‚àí kh)d.

(S37)

We now focus on the term E[(cid:107)vkh(Ykh) ‚àí ÀÜvkh(Ykh)(cid:107)2] in (S31). Similarly to the previous term, we can upper-bound this
term as follows:

E[(cid:107)vkh(Ykh) ‚àí ÀÜvkh(Ykh)(cid:107)2] ‚â§2Œ¥(L2E[(cid:107)Ykh(cid:107)2] + B2)

‚â§2Œ¥(L2C0 + B2).

By using (S37) and (S39) in (S31), we obtain:

K‚àí1
(cid:88)

(cid:90) (k+1)h

k=0

kh

(cid:0)12(t ‚àí kh)2(L2C0 + B2) + 6Œª(t ‚àí kh)d + (t ‚àí kh)2(cid:1)dt

KL(œÄT

X ||œÄT

Y ) ‚â§

L2
Œª

+

K‚àí1
(cid:88)

(cid:90) (k+1)h

2Œ¥(L2C0 + B2) dt

1
2Œª

k=0
(cid:16) C1h3
3

kh

+

(cid:17)

6Œªdh2
2

+

C2Œ¥Kh
2Œª

,

=

L2K
Œª

where C1 = 12(L2C0 + B2) + 1 and C2 = 2(L2C0 + B2).

Finally, by using the data processing and Pinsker inequalities, we obtain:

(cid:107)ÀÜ¬µKh ‚àí ¬µT (cid:107)2

TV ‚â§ (cid:107)œÄT

X ‚àí œÄT

Y (cid:107)2

TV ‚â§

=

1
4
L2K
4Œª

KL(œÄT

X ||œÄT
Y )
(cid:16) C1h3
+ 3Œªdh2(cid:17)
3

+

C2Œ¥Kh
8Œª

.

This concludes the proof.

Now, we bound the term (cid:107)¬Ø¬µKh ‚àí ÀÜ¬µKh(cid:107)TV.
Lemma S2. Assume that HS2 holds. Then the following bound holds:

(cid:107)œÄT

U ‚àí œÄT

Y (cid:107)2

TV ‚â§

L2Kh
16Œª

(cid:107)œÄT

X ‚àí œÄT

U (cid:107)2

TV.

Proof. We use that same approach than in Lemma S1. By Girsanov‚Äôs theorem once again, we have

(S38)

(S39)

(S40)

(S41)

(S42)

(S43)

(S44)

KL(œÄT

Y ||œÄT

U ) =

1
4Œª

K‚àí1
(cid:88)

(cid:90) (k+1)h

k=0

kh

E[(cid:107)ÀÜv(Ukh, ¬µkh) ‚àí ÀÜv(Ukh, ¬Ø¬µkh)(cid:107)2] dt,

(S45)

where œÄT

U denotes the distributions of (Ut)t‚àà[0,T ] with T = Kh. By using HS2, we have:

KL(œÄT

Y ||œÄT

U ) ‚â§

L2h
4Œª

K‚àí1
(cid:88)

k=0

(cid:107)¬µkh ‚àí ¬Ø¬µkh(cid:107)2
TV

‚â§

L2Kh
4Œª

(cid:107)œÄT

X ‚àí œÄT

U (cid:107)2

TV.

(S46)

(S47)

By applying the data processing and Pinsker inequalities, we obtain the desired result.

4Note that Lemma 3.2 of (Raginsky et al., 2017) considers the case where the drift is not time- or measure-dependent. However, with

HS3 it is easy to show that the same result holds for our case as well.

2.1. Proof of Theorem 3

Sliced-Wasserstein Flows

Here, we precise the statement of Theorem 3.
Theorem S7. Assume that the assumptions in Lemma S1 and Lemma S2 hold. Then for Œª > KL2h
holds:

8

, the following bound

(cid:107)¬Ø¬µKh ‚àí ¬µT (cid:107)2

TV ‚â§ Œ¥Œª

(cid:40)

L2K
2Œª

(cid:16) C1h3
3

+ 3Œªdh2(cid:17)

+

(cid:41)
,

C2Œ¥Kh
4Œª

where Œ¥Œª = (1 ‚àí KL2h

8Œª )‚àí1.

Proof. We have the following decomposition: (with T = Kh)

(cid:107)œÄT

X ‚àí œÄT

U (cid:107)2

TV ‚â§ 2(cid:107)œÄT
L2K
2Œª

‚â§

(cid:16)

‚â§

1 ‚àí

Y ‚àí œÄT

Y (cid:107)2
X ‚àí œÄT
(cid:16) C1h3
3
KL2h
8Œª

TV + 2(cid:107)œÄT
+ 3Œªdh2(cid:17)
(cid:17)‚àí1(cid:40)

L2K
2Œª

+

U (cid:107)2
TV
C2Œ¥Kh
4Œª
(cid:16) C1h3
3

+

L2Kh
8Œª
+ 3Œªdh2(cid:17)

+

(cid:107)œÄT

X ‚àí œÄT
U (cid:107)2
TV
(cid:41)
.

C2Œ¥Kh
4Œª

(S48)

(S49)

(S50)

(S51)

The second line follows from Lemma S1 and Lemma S2. Last line follows from the assumption that Œª is large enough. This
completes the proof.

3. Proof of Corollary 1

Proof. Considering the bound given in Theorem 3, the choice h implies that

Œ¥ŒªL2K
2Œª

(cid:16) C1h3
3

+ 3Œªdh2(cid:17)

‚â§ Œµ2.

(S52)

This Ô¨Ånalizes the proof.

4. Additional Experimental Results

4.1. The Sliced Wasserstein Flow

The whole code for the Sliced Wasserstein Flow was implemented in Python, for use with Pytorch5. The code was written
so as to run efÔ¨Åciently on GPU, and is available on the publicly available repository related to this paper6.

In practice, the SWF involves relatively simple operations, the most important being:

‚Ä¢ For each random Œ∏ ‚àà {Œ∏n}n=1...NŒ∏ , compute its inner product with all items from a dataset and obtain the empirical

quantiles for these projections.

‚Ä¢ At each step k of the SWF, for each projection z = (cid:10)Œ∏, ¬ØX i

(cid:11), apply two piece-wise linear functions, corresponding to

k

the scalar optimal transport œà(cid:48)

k,Œ∏(z).

Even if such steps are conceptually simple, the quantile and required linear interpolation functions were not available on
GPU for any framework we could Ô¨Ågure out at the time of writing this paper. Hence, we implemented them ourselves for
use with Pytorch, and the interested reader will Ô¨Ånd the details in the Github repository dedicated to this paper.

Given these operations, putting a SWF implementation together is straightforward. The code provided allows not only to
apply it on any dataset, but also provides routines to have the computation of these sketches running in the background in a
parallel manner.

5http://www.pytorch.org.
6https://github.com/aliutkus/swf.

Sliced-Wasserstein Flows

Figure S1. The evolution of SWF through 15000 iterations, when the original high-dimensional data is kept instead of working on reduced
bottleneck features as done in the main document. Showing results on the MNIST and FashionMNIST datasets. For a visual comparison
for FashionMNIST, we refer the reader to (Samangouei et al., 2018).

4.2. The need for dimension reduction through autoencoders

In this study, we used an autoencoder trained on the dataset as a dimension reduction technique, so that the SWF is applied
to transport particles in a latent space of dimension d ‚âà 50, instead of the original d > 1000 of image data.

The curious reader may wonder why SWF is not applied directly to this original space, and what performances should be
expected there. We have done this experiment, and we found out that SWF has much trouble rapidly converging to satisfying
samples. In Ô¨Ågure S1, we show the progressive evolution of particles undergoing SWF when the target is directly taken as
the uncompressed dataset.

In this experiment, the strategy was to change the projections Œ∏ at each iteration, so that we ended up with a set of projections
being {Œ∏n,k}k=1...K
instead of the Ô¨Åxed set of NŒ∏ we now consider in the main document (for this, we picked NŒ∏ = 200).
n=1...NŒ∏
This strategy is motivated by the complete failure we observed whenever we picked such Ô¨Åxed projections throughout
iterations, even for a relatively large number as NŒ∏ = 16000.

As may be seen on Figure S1, the particles deÔ¨Ånitely converge to samples from the desired datasets, and this is encouraging.
However, we feel that the extreme number of iterations required to achieve such convergence comes from the fact that theory
needs an integral over the d‚àídimensional sphere at each step of the SWF, which is clearly an issue whenever d gets too
large. Although our solution of picking new samples from the sphere at each iteration alleviated this issue to some extent,
the curse of dimensionality prevents us from doing much better with just thousands of random projections at a time.

Sliced-Wasserstein Flows

Figure S2. Approximately computed SW2 between the output ¬Ø¬µN
dimensions d for the bottleneck features (and the corresponding pre-trained AE).

k and data distribution ŒΩ in the MNIST experiment for different

This being said, we are conÔ¨Ådent that good performance would be obtained if millions of random projections could
be considered for transporting such high dimensional data because i/ theory suggests it and ii/ we observed excellent
performance on reduced dimensions.

However, we, unfortunately, did not have the computing power it takes for such large scale experiments and this is what
motivated us in the Ô¨Årst place to introduce some dimension-reduction technique through AE.

4.3. Structure of our autoencoders for reducing data dimension

As mentioned in the text, we used autoencoders to reduce the dimensionality of the transport problem. The structure of these
networks is the following:

‚Ä¢ Encoder Four 2d convolution layers with (num chan out, kernel size, stride, padding) being (3, 3, 1, 1), (32, 2, 2, 0),
(32, 3, 1, 1), (32, 3, 1, 1), each one followed by a ReLU activation. At the output, a linear layer gets the desired
bottleneck size.

‚Ä¢ Decoder A linear layer gets from the bottleneck features to a vector of dimension 8192, which is reshaped as
(32, 16, 16). Then, three convolution layers are applied, all with 32 output channels and (kernel size, stride, panning)
being respectively (3, 1, 1), (3, 1, 1), (2, 2, 0). A 2d convolution layer is then applied with an output number of channels
being that of the data (1 for black and white, 3 for color), and a (kernel size, stride, panning) as (3, 1, 1). In any case,
all layers are followed by a ReLU activation, and a sigmoid activation is applied a the very output.

Once these networks deÔ¨Åned, these autoencoders are trained in a very simple manner by minimizing the binary cross entropy
between input and output over the training set of the considered dataset (here MNIST, CelebA or FashionMNIST). This
training was achieved with the Adam algorithm (Kingma & Ba, 2014) with learning rate 1e ‚àí 3.

No additional training trick was involved as in Variational Autoencoder (Kingma & Welling, 2013) to make sure the
distribution of the bottleneck features matches some prior. The core advantage of the proposed method in this respect is
indeed to turn any previously learned AE as a generative model, by automatically and non-parametrically transporting
particles drawn from an arbitrary prior distribution ¬µ to the observed empirical distribution ŒΩ of the bottleneck features over
the training set.

4.4. Convergence plots of SWF

In the same experimental setting as in the main document, we also illustrate the behavior of the algorithm for varying
dimensionality d for the bottleneck-features. To monitor the convergence of SWF as predicted by theory, we display the

02004006008001000Number of Iterations (k)102101100101102Sliced-Wasserstein Lossd8032644816Sliced-Wasserstein Flows

Figure S3. The evolution of SWF through 200 iterations on the MNIST dataset. Plots are for 1, 11, 21, 31, 41, 51, 101 and 201 iterations

approximately computed SW2 distance between the distribution of the particles and the data distribution. Even though
minimizing this distance is not the real objective of our method, arguably, it is still a good proxy for understanding the
convergence behavior.

Figure S2 illustrates the results. We observe that, for all choices of d, we see a steady and smooth decrease in the cost for all
runs, which is in line with our theory. The absolute value of the cost for varying dimensions remains hard to interpret at this
stage of our investigations.

5. Additional samples

5.1. Evolution throughout iterations

In Figures S3 and S4 below, we provide the evolution of the SWF algorithm on the Fashion MNIST and the MNIST datasets
in higher resolution, for an AE with d = 48 bottleneck features.

5.2. Training samples, interpolation and extrapolation

In Figures S5 and S6 below, we provide other examples of outcome from SWF, both for the MNIST and the FashionMNIST
datasets, still with d = 48 bottleneck features.

The most noticeable fact we may see on these Ô¨Ågures is that while the actual particles which went through SWF, as well
as linear combinations of them, all yield very satisfying results, this is however not the case for particles that are drawn
randomly and then brought through a pre-learned SWF.

Once again, we interpret this fact through the curse of dimensionality: while we saw in our toy GMM example that using a
pre-trained SWF was totally working for small dimensions, it is already not so for d = 48 and only 3000 training samples.

Sliced-Wasserstein Flows

Figure S4. The evolution of SWF through 200 iterations on the FashionMNIST dataset. Plots are for 1, 11, 21, 31 (upper row) and 41, 51,
101, 201 (lower row) iterations

Sliced-Wasserstein Flows

(a) particles undergoing SWF

(b) After SWF is done: applying learned
map on linear combinations of train parti-
cles

(c) After SWF is done: applying learned
map on random inputs.

Figure S5. SWF on MNIST: training samples, interpolation in learned mapping, extrapolation.

This noticed, we highlight that this generalization weakness of SWF for high dimensions is not really an issue, since it is
always possible to i/ run SWF with more training samples if generalization is required ii/ re-run the algorithm for a set of
new particles. Remember indeed that this does not require passing through the data again, since the distribution of the data
projections needs to be done only once.

Sliced-Wasserstein Flows

(a) particles undergoing SWF

(b) After SWF is done: applying learned
map on linear combinations of train parti-
cles

(c) After SWF is done: applying learned
map on random inputs.

Figure S6. SWF on FashionMNIST: training samples, interpolation in learned mapping, extrapolation.

