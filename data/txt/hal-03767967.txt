Abstra: Toward Generic Abstractions for Data of Any
Model
Nelly Barret, Ioana Manolescu, Prajna Upadhyay

To cite this version:

Nelly Barret, Ioana Manolescu, Prajna Upadhyay. Abstra: Toward Generic Abstractions for Data
of Any Model. CIKM 2022 - 31st ACM International Conference on Information and Knowledge
Management, Oct 2022, Atlanta, Georgia / Hybrid, United States. ï¿¿hal-03767967ï¿¿

HAL Id: hal-03767967

https://inria.hal.science/hal-03767967

Submitted on 2 Sep 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

Lâ€™archive ouverte pluridisciplinaire HAL, est
destinÃ©e au dÃ©pÃ´t et Ã  la diffusion de documents
scientifiques de niveau recherche, publiÃ©s ou non,
Ã©manant des Ã©tablissements dâ€™enseignement et de
recherche franÃ§ais ou Ã©trangers, des laboratoires
publics ou privÃ©s.

Abstra: Toward Generic Abstractions for Data of Any Model

Nelly Barret, Ioana Manolescu, Prajna Upadhyay
Inria & Institut Polytechnique de Paris
nelly.barret@inria.fr,~ioana.manolescu@inria.fr,~prajna-devi.upadhyay@inria.fr

ABSTRACT

Digital data sharing leads to unprecedented opportunities to de-
velop data-driven systems for supporting economic activities, the
social and political life, and science. Many open-access datasets are
RDF (Linked Data) graphs, but others are JSON or XML documents,
CSV files, Neo4J property graphs, etc.

Potential users need to understand a dataset in order to decide if
it is useful for their goal. While some published datasets come with
a schema and/or documentation, this is not always the case.

We demonstrate Abstra, a dataset abstraction system, which
applies on a large variety of data models. Abstra computes a de-
scription meant for humans, and integrates Information Extraction
to classify dataset content among a set of categories of interest to the
user. Our abstractions are conceptually close to Entity-Relationship
diagrams, but our entities can have deeply nested structure.

1 INTRODUCTION
Open-access data being shared over the Internet enabled the de-
velopment of new businesses, economic opportunities and applica-
tions; it also leads to circulating knowledge on health, education,
environment, the arts, science, news, etc.

The World Wide Web Consortiumâ€™s recommended data shar-
ing format is as RDF (Linked data) graphs. However, in practice,
other formats are also widely used. For instance, bibliographic
notices on PubMed, a leading medical scientific site, are available
in XML; JSON is increasingly used, e.g., on social networks; CSV
files are shared on portals such as Kaggle. Relational databases are
sometimes shared as dumps, including schema constraints such as
primary and foreign keys, or as CSV files; property graphs [3] (PGs,
in short, such as pioneered by Neo4J) are used to share Offshore
leaks, a journalistic database of offshore companies, etc.

Users who must decide whether to use a dataset in an application
need a basic understanding of its content and the suitability to
their need. Towards this goal, schemas may be available to describe
the data structure, yet they have some limitations. (ğ‘–) Schemas are
often unavailable for semistructured datasets (XML, JSON, RDF,
PGs). Even when a schema is supplied with or extracted from the
data, e.g., [5, 10, 19, 23]. (ğ‘–ğ‘–) Schema syntactic details, such as regular
expressions, etc., are hard to interpret for non-expert users. (ğ‘–ğ‘–ğ‘–) A
schema focuses primarily on the dataset structure, not on its content.
It does not exploit the linguistic information encoded in node names,
in the string values the dataset may contain, etc. (ğ‘–ğ‘£) Schemas em-
ploy the data producerâ€™s terminology, not the concepts of interest to
users. (ğ‘£) Schemas do not quantitatively reflect the dataset, whereas
knowing â€œwhat is the main content of a datasetâ€ can be very helpful
for a first acquaintance with it. Data summaries can be built from
semistructured data, e.g., [8, 12], but they may still be quite large,
and they do not reflect user interest. An RDF dataset may come with
an ontology describing its semantics, which is a step toward lifting
limitation (ğ‘–ğ‘–ğ‘–); but, all the others still apply. Mining for patterns [15]

allows to find popular motifs, e.g., items often purchased together.
This avoids shortcomings (ğ‘–) and (ğ‘£), but not the others. Dataset
documentation, when well-written, is most helpful. However, it still
suffers from the issues (ğ‘–) and (ğ‘–ğ‘£) above: it is often lacking, and it
reflects the producerâ€™s view. In a data lake context, [14, 20] discov-
ers, extracts, and summarizes structural metadata, and annotates
data and metadata with semantic information. However, they focus
on rooted or hierarchical data, while we also handle graphs (RDF
or PGs) that may be cyclic, and our abstractions can also capture
such cyclic relationships when present in the data.

We propose to demonstrate Abstra, a all-in-one system for ab-
stracting any relational, CSV, XML, JSON, RDF or PG dataset.
Abstra is based on the idea that any dataset comprises some records,
typically grouped in collections (which we view as sets). Records
describe entities or relationships in the classical conceptual data-
base design sense [21]; Abstra entities can have deeply nested
structure. When several collections of entities co-exist in a dataset,
relationships typically connect them. Abstra proceeds as follows.
(1). Given any dataset, Abstra models it as a graph, and identifies
collections of equivalent nodes, leveraging graph structural
summarization, as we describe in Section 2.
(2). Among the collections, Abstra detects a few main collections,
which together, hold a large part of the dataset contents. Each
collection contains a set of similar, potentially deeply nested records.
The challenge here is to detect, in the data graph, the nodes and
edges that are â€œpart ofâ€ each main collection record, and to do so
efficiently even if the graph has complex, cyclic structure. This is
addressed by introducing a notion of data weight and exploiting it
as we describe in Section 3.
(3). Abstra attempts to classify each main collection into a
given semantic category, such as Person, Product, Geographi-
calPosition, etc., based on semantic resources resulting from prior
work [16]. The classification also leverages Information Extraction
to detect the presence of entities in the data values, as well as lan-
guage models to detect proximity between the dataset vocabulary,
and the target categories (Section 4).

Abstra outputs a Entity-Relationship style diagram of the
main, classified collections, together with the possible relationships
in which they participate; this description is free of any data model-
specific details. For instance, given an XMark [22] XML document
describing an online auctions site, containing 2.3M nodes with 80
different labels, Abstra returns: â€œA collection of Person entities,
a collection of Product, and a collection of categoryâ€ (the latter
are used to describe the items for sale). Users can explore entities,
and/or sample entity instances, through an interactive GUI (see
Section 5).

Below, we describe the abstraction steps and outline the demon-
stration scenarios before concluding. Abstra examples and a video
can be found at: https://team.inria.fr/cedar/projects/abstra/.

{authors: [{name: "Alice", cities: ["Paris"]}]}

ğœ–

ğœ–

authors

ğœ–

authors.

name

"Alice"

cities

cities.

ğœ–

"Paris"

Figure 1: JSON fragment (top) and its original graph repre-
sentation in Abstra (bottom).

Nelly Barret, Ioana Manolescu, Prajna Upadhyay

Figure 2: Sample normalized graph.

2 BUILDING A COLLECTION GRAPH

We first explain how any dataset is converted in a graph representa-
tion (Section 2.1), before partitioning it and constructing the central
tool of our method, the collection graph (Section 2.2).

2.1 Graph representation of any dataset

The graph representation we start from has been introduced in
ConnectionLens [2, 9], a graph-based heterogeneous data integra-
tion system, to which we bring some modifications. Any relational,
XML, JSON, RDF, or PG dataset is turned into a directed graph
ğº0 = (ğ‘0, ğ¸0, ğœ†0) where ğ¸0 âŠ† ğ‘0 Ã— ğ‘0 is a set of directed edges,
and ğœ†0 is a function labeling each node and edge with a string label,
that could in particular be ğœ– (the empty label).

XML trees and RDF graphs naturally map into this modeling.
JSON documents are modeled as trees. To the extent possible,
we attach meaningful, non-empty names to nodes, as illustrated in
Figure 1 on a sample JSON snippet. We move the labels of edges
which connect a map parent to its children, on the child nodes, and
we label the children of an array node with label of their parent, to
which we concatenate . (a dot).

Below, we focus only on the most irregular data formats, i.e.
XML, JSON and RDF. CSV files, relational databases and PGs are
easily converted into graphs [2] and can be similarly handled.

In ğº0, some edges have empty (ğœ–) labels, while other edges are
labeled. For uniformity, Abstra transforms ğº0 into a normalized
graph ğº, copying all the nodes of ğº0 and all its ğœ–-label edges,
ğ‘™
and replacing each ğº0 edge of the form ğ‘›1
âˆ’â†’ ğ‘›2 where ğ‘™ â‰  ğœ–
by two unlabeled edges ğ‘›1 â†’ ğ‘¥ğ‘™ , ğ‘¥ğ‘™ â†’ ğ‘›2 where ğ‘¥ğ‘™ is a new
intermediary node labeled ğ‘™. All subsequent Abstra steps apply on
the normalized graph ğº.

Figure 2 shows a sample bibliographic data graph ğº. It depicts
three papers (one partially shown), which are published in (pIn)
conferences. The papers are written by (wb) authors, described by
their name and email. Note the inverse â€œhas writtenâ€ (hW) edges
going from papers to their authors. Author 21 is invited (inv) by
the conference organizers. As Figure 2 shows, the graph may con-
tain: (ğ‘–) nodes such as papers, whose information content is deeply

Figure 3: Sample collection graph corresponding to Figure 2.
nested, and (ğ‘–ğ‘–) several cycles (in-cycle edges are shown in red).
Within each leaf (value) node, ConnectionLens extracts named
entities such as: persons (highlighted in yellow), dates (pink high-
light), emails (light blue), etc. Abstra leverages these in order to
classify the main entity collections (Section 4).

2.2 Partitioning nodes into collections
To leverage structural information present in ğº, we partition the
graph nodes ğ‘ into a set of pairwise disjoint node sets ğ¶ğ‘– .We say the
nodes from a given set ğ¶ğ‘– are equivalent, and call ğ¶ğ‘– an equivalence
class. Many node partitioning schemes, a.k.a. quotient summaries,
exist [8]. We need a method that is robust to heterogeneity, i.e., it
can recognize the various papers in Figure 2 even though they have
heterogeneous structure, and efficiently computed (ideally in linear
time in the size of ğ¸). For RDF graphs, we use Type Strong summa-
rization [11], which satisfies these requirements; it leverages RDF
types when available, but can also identify interesting equivalence
classes without them. We extend it also to PGs and graphs derived
from CSV files and relational databases. For graphs derived from
XML or JSON as discussed in Section 2.1, we simply partition the
nodes by their labels. On the graph in Figure 2, the equivalence
classes are: {1, 40}; {6, 8, 38}; {20, 21, 23}, etc.; there is one class for
each distinct label of non-leaf nodes, and one class for each set of
leaf nodes whose parents are equivalent.

We call collection graph the graph whose nodes are the collections
ğ¶ğ‘– , and having an edge ğ¶ğ‘– â†’ ğ¶ğ‘˜ if and only if for some nodes
ğ‘›ğ‘– âˆˆ ğ¶ğ‘–, ğ‘› ğ‘— âˆˆ ğ¶ ğ‘— , ğ‘›ğ‘– â†’ ğ‘› ğ‘— âˆˆ ğ¸. Figure 3 shows the collection graph
corresponding to the graph in Figure 2. Here, in each collection, all
nodes have the same label, shown in the collection; this does not
hold in general, e.g., in a collection of RDF nodes, each node has a
different label. The label year# is used to denote the collection of
text children of the nodes from the collection with the label year
and similarly for the others whose label end in #. The dw attributes
will be discussed in Section 3.

3 IDENTIFYING THE MAIN ENTITIES TO
REPORT AND THEIR RELATIONSHIPS
Among the collections C, some are more representative of the
dataset than others, e.g., in Figure 3, paper seems a better candidate
than its child collection year. However, we cannot select â€œthe parent
(or root) collection(s)â€, as the collection graph may lack a root node,
if it is cyclic as in Figure 3 (cycle edges are shown in red). Even if a

Abstra: Toward Generic Abstractions for Data of Any Model

root collection exists, it may not be the best choice. For instance,
consider XHTML search results grouped in pages, of the form âŸ¨topâŸ©
âŸ¨pageâŸ©âŸ¨resultâŸ©...âŸ¨/resultâŸ©âŸ¨resultâŸ©... âŸ¨/resultâŸ©âŸ¨/pageâŸ© âŸ¨pageâŸ©... âŸ¨/pageâŸ© ...
âŸ¨/topâŸ©. Here, the top collection is that of pages, but the actual data
is in the results, thus, "a collection of results" is a better abstraction.

3.1 Overview of the method

A high-level view of our method is the following (concrete details
will be provided below):

(1) Selecting the main entities (Section 3.2):

(a) We assign to each collection a weight, and to each edge

in the collection graph, a transfer factor.

(b) We propagate weights in the collection graph, based
on the weights and transfer factors, to assign to each
collection a score that reflects not only its own weight,
but also its position in the graph.

(c) In a greedy fashion, we select the main entities by

repeating:

(i) Select the collection node ğ¶ğ¸ currently having
the highest score, as a root of a main entity;
(ii) Determine the boundary of the entity ğ¶ğ¸ : this
is a connected subgraph of the collection graph,
containing ğ¶ğ¸ . We consider all this subgraph
as part of ğ¶ğ¸ , which will be reported to users
including all its boundary;

(iii) Update the collection graph to reflect the selection
of ğ¶ğ¸ and its boundaries, and recompute the
collection scores;

until a certain maximum number ğ¸ğ‘šğ‘ğ‘¥ of entities have
been selected, or these entities together cover a suffi-
cient fraction ğ‘ğ‘œğ‘£ğ‘šğ‘–ğ‘› of the data.

(2) Selecting relationships between the main entity col-
lections. These relationships will also be reported as part
of the abstraction (Section 3.3).

3.2 Main entity selection
We assign to each leaf node in ğº an own data weight (ğ‘œğ‘¤) equal to
the number of edges incoming that node. In tree data formats,
ğ‘œğ‘¤ is 1; in RDF, for instance, a literal that is the value of many
triples may have ğ‘œğ‘¤ > 1. We leverage this to define the ğ‘œğ‘¤ of a
leaf collection as the sum of the ğ‘œğ‘¤ of its nodes, e.g., in Figure 3,
ğ‘œğ‘¤ (title#) = 2, ğ‘œğ‘¤ (name#) = 5 etc.

For each edge ğ¶ğ‘– â†’ ğ¶ ğ‘— in the collection graph, we define the
edge transfer factor ğ‘“ğ‘—,ğ‘– as the fraction of nodes in ğ¶ ğ‘— having a
parent node in ğ¶ğ‘– ; 0 < ğ‘“ğ‘–,ğ‘— â‰¤ 1. Intuitively, ğ‘“ğ‘–,ğ‘— of ğ¶ ğ‘— â€™s weight can
also be seen as belonging to its parent ğ¶ğ‘– . For instance, there are 5
name nodes, but two belong to conferences, thus the transfer factor
from name to conf is ğ‘“ = 2/5.

We implemented two weight propagation methods.

â€¢ We run the PageRank [7] algorithm on the collection graph
with the edge direction inverted, so that each node transfers
ğ‘“ğ‘–,ğ‘— of its weight to its parent. Initially, only leaf collec-
tions have non-zero ğ‘œğ‘¤, but successive PageRank iterations
spread their weights across the graph. We call this method
PRğ‘œğ‘¤.

â€¢ Our second method, denoted propğ‘‘ğ‘¤, propagates weights
still backwards, but only outside of the collection graph cycles.

Specifically, we assign to each collection a data weight ğ‘‘ğ‘¤,
which on leaf collection is initialized to ğ‘œğ‘¤, and on others,
to 0. Then, for each non-leaf ğ¶ğ‘– , and non cyclic path from ğ¶ğ‘–
to a leaf collection ğ¶ğ‘˜ , we increase ğ‘‘ğ‘¤ (ğ¶ğ‘– ) by ğ‘“ğ‘˜,ğ‘– Â· ğ‘œğ‘¤ (ğ¶ğ‘˜ ).

For instance, using the second method, the collection author
in Figure 3 obtains ğ‘‘ğ‘¤ = 6, corresponding to 3 transferred from
mail, and 3 transferred from name. The intuition behind propğ‘‘ğ‘¤ is
that edges that are part of cycles may have a meaning closer to
â€œsymmetric relationships between entitiesâ€, than to â€œincluding a
collection in another collectionâ€™s boundaryâ€.

To determine entity boundaries, we proceed as follows:

â€¢ When using propğ‘‘ğ‘¤, we consider part of the boundary of
an entity ğ¶ğ‘– , any entity ğ¶ğ‘˜ that transferred some weight
to ğ¶ğ‘– , and all the edges along which such transfers took
place. In Figure 3, mail and name are within the boundary
of author.

â€¢ When using PRğ‘œğ‘¤, to determine the boundary of ğ¶ğ‘– , we
traverse the graph edges starting from ğ¶ğ‘– and include its
neighbor node ğ¶ ğ‘— if and only if (ğ‘–) the edge ğ¶ğ‘– â†’ ğ¶ ğ‘— has
a transfer factor of at least ğ‘“ğ‘šğ‘–ğ‘›, or (ğ‘–ğ‘–) each node from ğ¶ğ‘–
has at most one child in ğ¶ ğ‘— . The intuition for (ğ‘–ğ‘–) is that ğ¶ ğ‘—
â€œcan be assimilated to an attribute of ğ¶ğ‘– â€, rather than being
â€œindependent of itâ€.

Finally, to update the graph after selecting one main entity ğ¶ğ¸ ,
each leaf collection in the boundary of ğ¶ğ¸ substracts from its own
weight ğ‘œğ‘¤ the fraction (at most 1.0) that it propagated to ğ¶ğ¸ . For
instance, once author is selected with name in its boundary, the ğ‘œğ‘¤
of name decreases to 2. Then, the scores of all graph collections
(ğ‘‘ğ‘¤, respectively, PageRank score based on ğ‘œğ‘¤) are recomputed.

ğ¸, . . . , ğ¶ğ‘šğ‘ğ‘¥ğ¸

3.3 Relationship selection
Having selected the main entities {ğ¶1
} and their bound-
aries, every oriented path in the collection graph that goes from a
given ğ¶ğ‘–
ğ¸ is reported as a relationship. For instance, in
Figure 3, if the main entities are author (a), with mail and name in its
boundary, and paper (p) with year, title and abstract in its boundary,
hW
âˆ’âˆ’âˆ’â†’ p, and p

the relationships are: p

ğ¸ to another ğ¶ ğ‘—

pIn.conf.inv
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ a.

wB
âˆ’âˆ’âˆ’â†’ a, a

ğ¸

If the scores lead to reporting three main entities, the two above
and also conf (c) with name and inv in its boundary, the relationships

are: p

wB
âˆ’âˆ’âˆ’â†’ a, a

hW
âˆ’âˆ’âˆ’â†’ p, p

3.4 Discussion

pIn
âˆ’âˆ’âˆ’â†’ c, and c

inv
âˆ’âˆ’â†’ a.

Abstra may return different results on a given dataset, depending
on the scoring method used (propğ‘‘ğ‘¤ or PRğ‘œğ‘¤), as well as the pa-
rameters: ğ¸ğ‘šğ‘ğ‘¥ and ğ‘ğ‘œğ‘£ğ‘šğ‘–ğ‘› (Section 3.1), and ğ‘“ğ‘šğ‘–ğ‘› (Section 3.2). Em-
pirically, we have used ğ¸ğ‘šğ‘ğ‘¥ âˆˆ {3, 5}, ğ‘ğ‘œğ‘£ğ‘šğ‘–ğ‘› = 0.8 and ğ‘“ğ‘šğ‘–ğ‘› = 0.3.
More generally, classical Entity-Relationship (E-R) modeling is
known to include a subjective factor, and for a given database,
several E-R models may be correct. Our focus is on not missing any
essential component of the dataset, while allowing users to limit the
amount of information with ğ¸ğ‘šğ‘ğ‘¥ , and classifying the main entities
into categories, to make them as informative as possible (Section 4).

ğ¶

ğ¶ properties

ğ‘‘ğ‘ğ‘–

P

K

ğ‘
ğ‘‘ğ‘œğ‘šğ‘ğ‘–ğ‘›(ğ‘)
ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ (ğ‘)

l

compare

ğ‘˜

ğ¶ node
labels

profile
ğœğ¶,ğ‘‘ğ‘ğ‘–

vote to
classify ğ¶
as ğ‘˜

compare

Figure 4: Entity classification outline.

classify ğ¶
as winner
class ğ‘˜âˆ—

4 MAIN ENTITY CLASSIFICATION
To each main entity ğ¶ thus identified, we want to associate a cate-
gory from a predefined set K of semantic classes, and using also a
set P of semantic properties, which have known domain and range
constraints connecting them to the classes in K.

We build our K and P by leveraging GitTables [16], a reposi-
tory of 1.5M tables extracted from Github. For each attribute name
encountered in a table, it provides candidate properties from DBPe-
dia [4] and/or schema.org (https://schema.org/); it also provides the
domain and range triples corresponding to these properties. For
instance, GitTableâ€™s entry for gender is:

"id":"schema:gender", "label":"gender", "range": ["schema:GenderType"],
"domain":["schema:Person","schema:SportsTeam"],

GitTables have been populated using SHERLOCK [17], a state-of-
the-art deep learning semantic annotation technique. From GitTa-
bles, we derive 4.187 P properties; 3.687 among them have domain
information, and 3.898 have range statements.

The overall classification process is outlined in Figure 4; solid
arrows connect associated data items and trace the classification
process, while dotted arrows go from a set to one of its elements.

We exploit two kinds of information attached to ğ¶:
(1) We consider each data property ğ‘‘ğ‘ğ‘– that ğ¶ has, such as
mail for the author collection in Figure 3. Out of all the
values that ğ‘‘ğ‘ğ‘– takes on a node from ğ¶, we compute an
entity profile ğœğ¶,ğ‘‘ğ‘ğ‘– , reflecting the entities extracted from
these values. For instance, ğœauthor, mail states that each value
of the property mail contains an email (blue highlight in
Figure 2), and that overall, 100% of the length of these values
is part of an email entity. In general, a profile may reflect
the presence of entities of several types, which may span
over only a small part of the property values.
We compare ğ‘‘ğ‘ğ‘– and ğœğ¶,ğ‘‘ğ‘ğ‘– to each property ğ‘ âˆˆ P and
the classes in the range of ğ‘. If the property name ğ‘‘ğ‘ğ‘– is
sufficiently similar (through word embeddings) with some
property ğ‘ âˆˆ P, and that ğœğ¶,ğ‘‘ğ‘ğ‘– is similarly sufficiently
similar to a class ğ‘˜ğ‘– âˆˆ K, e.g., EmailAddress, that is in the
range of ğ‘, this leads to a vote of ğ‘‘ğ‘ğ‘– for classifying ğ¶, in
every classes ğ‘˜ âˆˆ K such that ğ‘˜ is in the domain of ğ‘.
Each property ğ‘‘ğ‘ğ‘– may â€œvoteâ€ in favor of several classes,
via different domain constraints; the higher the similarity
between ğ‘‘ğ‘ğ‘– and ğ‘, the more frequent ğ‘‘ğ‘ğ‘– is on ğ¶ nodes,
the fewer domain and range constraints ğ‘ has, the stronger
the vote is.

(2) The labels of ğ¶ nodes may also â€œvoteâ€ toward classifying
collection ğ¶. All ğ¶ nodes may have the same label, e.g.,
author, which may resemble the name of a class, e.g., Author.
Or, ğ¶ nodes may all have different names, e.g., RDF URIs of

Nelly Barret, Ioana Manolescu, Prajna Upadhyay

Figure 5: Sample screenshot of the Abstra GUI.

the form http://ns.com/Author123, from which we extract the
component after the last /, eliminate all but alphabet letters,
and use the result(s), weighted by their support among ğ¶
nodes, in a similar fashion.

Finally, ğ¶ is classified with the class ğ‘˜âˆ— âˆˆ K having received
the highest sum of votes. The process resembles domain inference
using RDF Schema ontology constraints, with the difference that
our â€œvotesâ€ are quantified by similarity and support, and, to keep
things simple for the users, we select a single class.
5 SYSTEM AND SCENARIOS

Abstra is implemented in Java, leveraging the graph creation (in-
cluding entity extraction) and Postgres-based store of Connection-
Lens [2]. All Abstra steps scale up linearly in the data size, which
we experimentally verified on datasets of up to tens of millions
of edges. The main memory needs are in TypedStrong partition-
ing, namely ğ‘‚ (|ğ‘ |); other operations are implemented in SQL and
benefit from Postgresâ€™ optimizations.

We have computed abstractions of dozens of synthetic bench-
mark datasets used in the data management literature, such as
XMark [22], BSBM [6], LUBM [13], and real-life datasets about
cooking, NASA flights, clean energy production, Nobel prizes, CSV
datasets from Kaggle, etc. varying the data model, the number of
collections and entity complexity, the presence of relations, etc. Dur-
ing the demonstration, users will be able to: (ğ‘–) change the system
parameters (Section 3.4), and see the impact on the classification
results; (ğ‘–ğ‘–) edit the semantic information K and P to influence the
entity classification; (ğ‘–ğ‘–ğ‘–) edit a set of small RDF, JSON and XML
examples, then abstract them to see the impact.

Abstractions are shown both as HTML text and as a light-
weight E-R diagram of the main entity collections and their rela-
tionships (see Figure 5).

6 CONCLUSION

Abstra extracts the main entities and relationships from heterogeneous-
structure datasets, leveraging Information Extraction, language

Abstra: Toward Generic Abstractions for Data of Any Model

models, knowledge bases, and user input to generate compact, easy-
to-understand abstractions, and categorize them according to userâ€™s
interest. Abstra complements schema extraction [5, 10, 19, 23] or
data profiling [1], aimed at more technical uses and users; Abstra
aims to help novice, first-time users discover and start interacting
with the data. More advanced visualization techniques then be used
once, see, for instance, [18] for RDF graphs.

REFERENCES
[1] Z. Abedjan, L. Golab, F. Naumann, and T. Papenbrock. Data Profiling. Synthesis

Lectures on Data Management. Morgan & Claypool Publishers, 2018.

[2] A. C. Anadiotis, O. Balalau, C. Conceicao, H. Galhardas, M. Y. Haddad,
I. Manolescu, T. Merabti, and J. You. Graph integration of structured, semistruc-
tured and unstructured data for data journalism.
Information Systems, July
2021.

[3] R. Angles. The property graph database model. In D. Olteanu and B. Poblete,
editors, Proceedings of the 12th Alberto Mendelzon International Workshop on
Foundations of Data Management, Cali, Colombia, May 21-25, 2018, volume 2100
of CEUR Workshop Proceedings. CEUR-WS.org, 2018.

[4] S. Auer et al. DBpedia: A nucleus for a web of open data. In The Semantic Web,

pages 722â€“735, Berlin, Heidelberg, 2007. Springer Berlin Heidelberg.

[5] M. A. Baazizi, C. Berti, D. Colazzo, G. Ghelli, and C. Sartiani. Human-in-the-loop

schema inference for massive JSON datasets. In EDBT, 2020.

[6] C. Bizer and A. Schultz. The Berlin SPARQL benchmark. IJSWIS, 5(2):1â€“24, 2009.
[7] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine.

Comput. Networks, 30(1-7):107â€“117, 1998.

[8] S. Cebiric, F. GoasdouÃ©, H. Kondylakis, D. Kotzinos, I. Manolescu, G. Troullinou,
and M. Zneika. Summarizing Semantic Graphs: A Survey. The VLDB Journal,
28(3), June 2019.

[9] C. Chanial, R. Dziri, H. Galhardas, J. Leblay, M. L. Nguyen, and I. Manolescu.
ConnectionLens: Finding connections across heterogeneous data sources (demon-
stration). PVLDB, 11(12), 2018.

[10] D. Colazzo, G. Ghelli, and C. Sartiani. Schemas for safe and efficient XML

processing. In ICDE. IEEE Computer Society, 2011.

[11] F. GoasdouÃ©, P. Guzewicz, and I. Manolescu. RDF graph summarization for

first-sight structure discovery. The VLDB Journal, 29(5), Apr. 2020.

[12] R. Goldman and J. Widom. DataGuides: Enabling query formulation and opti-

mization in semistructured databases. In VLDB, 1997.

[13] Y. Guo, Z. Pan, and J. Heflin. LUBM: A benchmark for OWL knowledge base

systems. Journal of Web Semantics, 3(2-3):158â€“182, 2005.

[14] R. Hai, S. Geisler, and C. Quix. Constance: An intelligent data lake system. In
Proceedings of the 2016 international conference on management of data, pages
2097â€“2100, 2016.
J. Han, M. Kamber, and J. Pei. Data mining concepts and techniques, third edition.
Morgan Kaufmann Publishers, 2012.

[15]

[16] M. Hulsebos, Ã‡. Demiralp, and P. Groth. GitTables: A large-scale corpus of

relational tables. CoRR, abs/2106.07258, 2021.

[17] M. Hulsebos, K. Hu, M. Bakker, E. Zgraggen, A. Satyanarayan, T. Kraska, Ã‡. Demi-
ralp, and C. A. Hidalgo. Sherlock: A Deep Learning Approach to Semantic Data
Type Detection. SIGKDD Explorations, May 2019.

[18] M. Krommyda and V. Kantere. Visualization systems for linked datasets. In 2020
IEEE 36th International Conference on Data Engineering (ICDE), pages 1790â€“1793,
2020.

[19] H. Lbath, A. Bonifati, and R. Harmer. Schema inference for property graphs. In

EDBT. OpenProceedings.org, 2021.

[20] C. Quix, R. Hai, and I. Vatov. Metadata extraction and management in data

lakeswith gemms. Complex Syst. Informatics Model. Q., 9:67â€“83, 2016.

[21] R. Ramakhrishnan and J. Gehrke. Database Management Systems (3rd edition).

McGraw-Hill, 2003.

[22] A. Schmidt, F. Waas, M. L. Kersten, M. J. Carey, I. Manolescu, and R. Busse.

Xmark: A benchmark for XML data management. In PVLDB, 2002.

[23] W. Spoth, O. A. Kennedy, Y. Lu, B. Hammerschmidt, and Z. H. Liu. Reducing

ambiguity in JSON schema discovery. In SIGMOD, 2021.

