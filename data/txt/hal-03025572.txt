NegPSpan: eï¬€icient extraction of negative sequential
patterns with embedding constraints
Thomas Guyet, RenÃ© Quiniou

To cite this version:

Thomas Guyet, RenÃ© Quiniou. NegPSpan: eï¬€icient extraction of negative sequential patterns with em-
bedding constraints. Data Mining and Knowledge Discovery, 2020, 34, pp.563-609. ï¿¿10.1007/s10618-
019-00672-wï¿¿. ï¿¿hal-03025572ï¿¿

HAL Id: hal-03025572

https://inria.hal.science/hal-03025572

Submitted on 26 Nov 2020

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

Lâ€™archive ouverte pluridisciplinaire HAL, est
destinÃ©e au dÃ©pÃ´t et Ã  la diffusion de documents
scientifiques de niveau recherche, publiÃ©s ou non,
Ã©manant des Ã©tablissements dâ€™enseignement et de
recherche franÃ§ais ou Ã©trangers, des laboratoires
publics ou privÃ©s.

Data Mining and Knowledge Discovery manuscript No.
(will be inserted by the editor)

NegPSpan: eï¬ƒcient extraction of negative sequential patterns
with embedding constraints

Thomas Guyet Â· RenÃ© Quiniou

Received: date / Accepted: date

Abstract Mining frequent sequential patterns consists in extracting recurrent behav-
iors, modeled as subsequences, in a big sequence dataset. Such patterns inform about
which events are frequently observed in sequences, i.e. events that really happen.
Sometimes, knowing that some speciï¬c event does not happen is more informative
than extracting observed events. Negative sequential patterns (NSPs) capture recur-
rent behaviors by patterns having the form of sequences mentioning both observed
events and the absence of events. Few approaches have been proposed to mine such
NSPs. In addition, the syntax and semantics of NSPs diï¬€er in the diï¬€erent methods
which makes it diï¬ƒcult to compare them. This article provides a uniï¬ed framework
for the formulation of the syntax and the semantics of NSPs. Then, we introduce a new
algorithm, NegPSpan, that extracts NSPs using a Preï¬xSpan depth-ï¬rst scheme, en-
abling maxgap constraints that other approaches do not take into account. The formal
framework highlights the diï¬€erences between the proposed approach and the methods
from the literature, especially with the state of the art approach eNSP. Intensive ex-
periments on synthetic and real datasets show that NegPSpan can extract meaningful
NSPs and that it can process bigger datasets than eNSP thanks to signiï¬cantly lower
memory requirements and better computation times.

Keywords Sequential patterns mining Â· pattern semantics Â· absence modeling Â·
negative containment

T. Guyet
Agrocampus Ouest/IRISA-UMR6074
65 rue de Saint Brieuc, 35042 Rennes, France
E-mail: thomas.guyet@irisa.fr

R. Quiniou
Inria, Univ Rennes, CNRS, IRISA

2

1 Introduction

Thomas Guyet, RenÃ© Quiniou

In many application domains such as diagnosis or marketing, decision makers show
a strong interest for rules that associate speciï¬c events (a context) to undesirable
events to which they are correlated or that are frequently triggered in such a context.
In the pattern mining ï¬eld, such rules are extracted from dataset and formalized by
patterns. Patterns are substructures that appear in a structured dataset. The patterns can
refer to diï¬€erent structural forms: itemsets, subsequences, subgraphs. For example, a
sequential pattern is a subsequence, such as buying ï¬rst milk, then bread, and then
chocolate. This subsequence of purchases is interesting when it occurs in a shopping
history database (the structure dataset). Sequential pattern mining algorithms can
extract such hidden rules from execution traces or transactions. In the classical setting,
sequential patterns contain only positive events, i.e. really observed events. However,
the absence of a speciï¬c action or event can often better explain the occurrence
of an undesirable situation (Cao et al., 2015). For example in diagnosis, if some
maintenance operations have not been performed, e.g. damaged parts have not been
replaced, then a fault will likely occur in a short delay while if these operations
would have been performed in time the fault would not occur. In marketing, if some
market-place customer has not received special oï¬€ers or coupons for a long time then
she/he has a high probability of churning while if she/he would have been provided
such special oï¬€ers she/he should remain loyal to her/his market-place. In these two
cases, mining speciï¬c events, some present and some absent, to discover under which
context some undesirable situation occurs or not, may provide interesting so-called
actionable information for determining which action should be performed to avoid
the undesirable situation, i.e. fault in diagnosis, churn in marketing.

We aim at discovering sequential patterns that take into account the absence of
some events called negative events (Cao et al., 2015). Moreover, we want to take into
account some aspects of the temporal dimension as well, maximal span of pattern
occurrences or maximal gap between the occurrences of pattern events. For example,
suppose that from a sequence dataset, we want to mine a sequential pattern (cid:104)ğ‘ ğ‘(cid:105) with
the additional negative constraint telling that the event ğ‘ should not appear between
events ğ‘ and ğ‘ (the scope of the constraint). The corresponding negative pattern is
represented as (cid:104)ğ‘ Â¬ğ‘ ğ‘(cid:105), where the logical sign Â¬ indicates the absence of the event or
the set of events in its scope. Once the general idea of introducing negative events in a
pattern has been stated, the syntax and semantics of such negative patterns should be
clearly formulated since they have a strong impact both on algorithms outcome and
their computational eï¬ƒciency. As we will see, the few algorithms from literature do
not use the same syntactical constraints and rely on very diï¬€erent semantical principles
(see Section 7). More precisely, the eï¬ƒciency of eNSP (Cao et al., 2016), the state-of-
the-art algorithm for NSP mining, comes from a semantic for negation that enables
eï¬ƒcient operations on the sets of supported sequences. The two major computational
limits of eNSP are memory requirements and the impossibility for eNSP to handle
embedding constraints such as the classical maxgap and maxspan constraints. Maxgap
and maxspan constraints enforce the events of a pattern occurrence to not be too distant
in time. Intuitively, if events are far from each others, they are not related/ Thus, they

NegPSpan: eï¬ƒcient extraction of negative sequential patterns

3

might not match the pattern. In addition, such constraints can prune eï¬ƒciently the
exploration of occurrences during the sequence dataset scan.
The two main contributions of this article are as follows:

â€“ a clariï¬cation of the syntactic deï¬nition of negative sequential patterns and dif-

ferent negation semantics with their associated properties.

â€“ a complete and correct algorithm called NegPSpan, inspired by algorithm Preï¬xS-
pan, to extract negative sequential patterns with maxgap and maxspan constraints.

Intensive experiments compare, on synthetic and real datasets, the performance of
NegPSpan and eNSP as well as the pattern sets extracted by each of them. As results,
they show that algorithm NegPSpan is more time-eï¬ƒcient than eNSP for mining long
sequences thanks to the maxgap constraint and that its memory requirement is several
orders of magnitude lower, enabling to process much larger datasets. In addition, they
highlight that eNSP misses interesting patterns on real datasets due to semantical
restrictions.

2 Frequent sequential pattern mining

This section introduces frequent sequential pattern mining, its basic concepts and
main results. This data mining task was introduced at the early ages of the pattern
mining ï¬eld (Srikant and Agrawal, 1996).

Let (I, <) be the ï¬nite set of items (alphabet) associated with a total order
(e.g. lexicographic order). In the sequel, [ğ‘›] = {1, Â· Â· Â· , ğ‘›} denotes the set of the ï¬rst
ğ‘› strictly positive integers.

Deï¬nition 1 (Sequence) An itemset ğ´ = {ğ‘1 ğ‘2 Â· Â· Â· ğ‘ğ‘š} âŠ† I is a set of items. | ğ´|
denotes the size of the itemset ğ´. A sequence ğ’” is a ï¬nite set of sequentially ordered
itemsets ğ’” = (cid:104)ğ‘ 1 ğ‘ 2 Â· Â· Â· ğ‘ ğ‘›(cid:105). This means that ğ‘ ğ‘– appears before ğ‘  ğ‘— in sequence ğ’” for
all ğ‘–, ğ‘— âˆˆ [ğ‘›], ğ‘– < ğ‘—. This sequence starts by ğ‘ 1 and ï¬nishes by ğ‘ ğ‘›. ğ‘› is the length
of the sequence, denotes |ğ’”|. The size of the sequence is the total number of items it
contains.

2 Â· Â· Â· ğ‘ (cid:48)
ğ‘ (cid:48)

Deï¬nition 2 (Subsequence and embedding) Let ğ’” = (cid:104)ğ‘ 1 ğ‘ 2 Â· Â· Â· ğ‘ ğ‘›(cid:105) and ğ’”(cid:48) =
ğ‘š(cid:105) be two sequences, ğ’”(cid:48) is a subsequence of ğ’”, denoted ğ’”(cid:48) (cid:22) ğ’”, iï¬€ there
(cid:104)ğ‘ (cid:48)
1
exists an increasing sequence of ğ‘š indexes ğ‘’ğ‘– âˆˆ [ğ‘›] such that ğ‘’ğ‘– < ğ‘’ğ‘–+1 for all
ğ‘– âŠ† ğ‘ ğ‘’ğ‘– for all ğ‘– âˆˆ [ğ‘š]. (ğ‘’ğ‘–)ğ‘– âˆˆ [ğ‘š] âˆˆ [ğ‘›] ğ‘š is called an embedding of
ğ‘– âˆˆ [ğ‘š âˆ’ 1] and ğ‘ (cid:48)
ğ’”(cid:48) in ğ’”.

Deï¬nition 3 (Sequential pattern) A sequential pattern ğ’‘ = (cid:104)ğ‘1 ğ‘2 Â· Â· Â· ğ‘ğ‘š(cid:105) is a
sequence over the set of items I. Let ğ’” be a sequence and ğ’‘ be a sequential pattern.
Sequence ğ’” supports pattern ğ’‘ (or ğ’‘ occurs in sequence ğ’”) iï¬€ ğ’‘ is a subsequence of
ğ’”, i.e. ğ’‘ (cid:22) ğ’”.

Example 1 (Sequence and sequential patterns) In this example, we illustrate the
diï¬€erent notions of frequent sequential pattern mining. We assume that I = {ğ‘, ğ‘, ğ‘, ğ‘‘,

4

Thomas Guyet, RenÃ© Quiniou

ğ‘’, ğ‘“ } and we consider the following dataset of six sequences:

D =

ğ’”1 = (cid:104)ğ‘ (ğ‘’ ğ‘“ ) ğ‘ ğ‘ (ğ‘ğ‘’)(cid:105)
ğ’”2 = (cid:104)ğ‘’ ğ‘ (ğ‘ğ‘‘) ğ‘ ğ‘‘ ğ‘ (ğ‘ğ‘’) (ğ‘‘ğ‘’)(cid:105)
ğ’”3 = (cid:104)ğ‘ ğ‘‘ ğ‘ (ğ‘ğ‘’) ğ‘“ (cid:105)
ğ’”4 = (cid:104)ğ‘ (ğ‘ğ‘’) ğ‘ ğ‘‘ ğ‘“ ğ‘(cid:105)
ğ’”5 = (cid:104)(ğ‘ğ‘ğ‘‘) ğ‘ ğ‘ ğ‘’ ğ‘ (ğ‘ğ‘’) ğ‘‘(cid:105)
ğ’”6 = (cid:104)ğ‘ ğ‘’ ğ‘ ğ‘‘(cid:105)

ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´

ï£³

.

ï£¼ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£½
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´

ï£¾

The length of sequence ğ’”1 is 5. (ğ‘’ ğ‘“ ) and (ğ‘ğ‘’) denote itemsets containing 2 items

occurring at the same time in the sequence.

Let ğ’‘ = (cid:104)ğ‘ ğ‘ ğ‘ (ğ‘ğ‘’)(cid:105) be a sequential pattern, ğ’‘ occurs in sequences ğ’”1, ğ’”2, ğ’”5.
The embedding of ğ’‘ in ğ’”5 is (1, 3, 5, 6). Deï¬nition 2 requires that each itemset of the
pattern is a subset of some sequence itemset and respects the pattern and the sequence
orderings. In addition, according to the deï¬nition of a subsequence (Deï¬nition 2),
successive itemsets of a sequential pattern do not need to occur in a row in the
sequence. Some items or itemsets can appear between the sequence occurrences of
two successive itemsets of the sequential pattern. Indeed, ğ‘, the ï¬rst item of ğ’‘, is a
subset of (ğ‘ğ‘ğ‘‘) in ğ’”5. Also, item ğ‘ appears between the occurrences of ğ‘ and ğ‘ in ğ’”5,
and item ğ‘’ appears between the occurrences of the second and third itemsets of ğ’‘.
But, ğ’‘ does not occur in ğ’”6 because ğ’”6 does not contain any item ğ‘. ğ’‘ does not occur
in ğ’”4 because the items of the pattern ğ’‘ are not in the correct order. ğ’‘ does not occur
in ğ’”3 because item ğ‘ must occur twice (the embedding must strictly increase).

Let us now consider the problem of mining frequent sequential patterns from
a dataset of sequences, denoted D. The main idea behind mining frequent patterns
assumes that the more frequent a pattern the more interesting it is. In practice, a pattern
is said to be frequent when it occurs more frequently than a user-deï¬ned threshold ğœ.
The following deï¬nitions formalize these intuitive deï¬nitions.

Deï¬nition 4 (Sequential pattern support) Let ğ’‘ be a sequential pattern and D =
{ğ’”ğ’Š } a dataset of sequences where ğ’”ğ’Š is the ğ‘–-th sequence of D.

The support of ğ’‘ in D, denoted ğ‘ ğ‘¢ ğ‘ ğ‘( ğ’‘), is the number of sequences that

support ğ’‘:

ğ‘ ğ‘¢ ğ‘ ğ‘( ğ’‘) = |{ğ’”ğ’Š âˆˆ D | ğ’‘ (cid:22) ğ’”ğ’Š }| .

The support of some sequential pattern ğ’‘ is the number of sequences from the
dataset in which ğ’‘ occurs. It is worth noting that the support is the number of
sequences that contains a pattern and not the number of embeddings of the pattern in
the sequences. If the pattern occurs several times in a sequence, this sequence counts
only for 1 in the support.

Deï¬nition 5 (Frequent sequential pattern mining) Let D be a dataset of sequences
and ğœ be a threshold, mining frequent sequential patterns consists in extracting the
complete list of patterns having a support greater than a given threshold ğœ:

{ ğ’‘ | ğ‘ ğ‘¢ ğ‘ ğ‘( ğ’‘) â‰¥ ğœ}.

NegPSpan: eï¬ƒcient extraction of negative sequential patterns

5

A naive approach to solve the problem of mining frequent sequential pattern
mining consists in enumerating all potential sequential patterns and to evaluate their
support. But, this approach is practically intractable on large datasets due to the
large size of the set of sequential patterns. The algorithmic strategies that have been
developed in the ï¬eld of sequential pattern mining are based on the anti-monotonicity
property of the support measure.

Proposition 1 (Support anti-monotonicity (Srikant and Agrawal, 1996)) Let D a
dataset of sequences and ğ’‘, ğ’‘(cid:48) two sequential patterns, the anti-monotonicity states
that:

ğ’‘ (cid:22) ğ’‘(cid:48) =â‡’ ğ‘ ğ‘¢ ğ‘ ğ‘( ğ’‘) â‰¥ ğ‘ ğ‘¢ ğ‘ ğ‘( ğ’‘(cid:48))

The anti-monotonicity property of the support with respect to pattern inclusion
states that if a pattern ğ’‘(cid:48) is a super-pattern of ğ’‘ (in other words, that ğ’‘ is a subsequence
of ğ’‘(cid:48)), then the support of ğ’‘(cid:48) is lower than the support of ğ’‘(cid:48). The intuition behind
this property, is that each time the super-pattern ğ’‘(cid:48) occurs in a sequence of D, then
any of its sub-pattern ğ’‘ also occurs in the same sequence, thus the support of ğ’‘ is at
least equal to or greater than the support of ğ’‘(cid:48).

Example 2 (Anti-monotonicity of the support) Continuing Example 1, let ğ’‘(cid:48) = (cid:104)ğ‘ ğ‘ ğ‘
(ğ‘ğ‘’) ğ‘‘(cid:105) be a super-pattern of ğ’‘, ğ’‘ (cid:22) ğ’‘(cid:48). Indeed, ğ’‘(cid:48) occurs in sequence ğ’”2 and ğ’”5, but
not in ğ’”1. ğ‘ ğ‘¢ ğ‘ ğ‘( ğ’‘) = 3 whereas ğ‘ ğ‘¢ ğ‘ ğ‘( ğ’‘(cid:48)) = 2 and so ğ‘ ğ‘¢ ğ‘ ğ‘( ğ’‘(cid:48)) â‰¤ ğ‘ ğ‘¢ ğ‘ ğ‘( ğ’‘). It is
useless to look at other sequences, if ğ’‘ does not occur in some sequence, ğ’‘(cid:48) does not
as well.

It follows from Proposition 1 that if ğ’‘ and ğ’‘(cid:48) are two sequential patterns such that
ğ’‘ (cid:22) ğ’‘(cid:48) then if ğ’‘(cid:48) is frequent (ğ‘ ğ‘¢ ğ‘ ğ‘( ğ’‘(cid:48)) â‰¥ ğœ), then ğ’‘ is also frequent (ğ‘ ğ‘¢ ğ‘ ğ‘( ğ’‘) â‰¥ ğœ).
Conversely, if ğ’‘ is not frequent, then ğ’‘(cid:48) is not frequent.

The anti-monotonicity property of the support is used by all mining algorithms to
make the exploration of the pattern space eï¬ƒcient. Intuitively, each time the algorithm
encounters a not frequent pattern ğ’‘, it avoids the exploration of the super-patterns
of ğ’‘. Decades of research have proposed many algorithmic strategies to exploit this
anti-monotonicity property and numerous algorithms have been proposed. They can
be classiï¬ed in three main approaches:

â€“ bread-ï¬rst search: all potential frequent patterns of size ğ‘› + 1 are generated from
frequent patterns of size ğ‘› and then evaluated. GSP (Srikant and Agrawal, 1996)
uses this strategy.

â€“ depth-ï¬rst search: a frequent pattern is extended until being not frequent, and
then alternative extensions are explored. FreeSpan (Han et al., 2000) and Preï¬xS-
pan (Pei et al., 2004) uses this strategy.

â€“ vertical dataset: this strategy uses a vertical representation of the dataset of se-
quences to explore the search space using equivalent classes of patterns. This
strategy has been introduced by SPADE (Zaki, 2001).

Several variations of the problem of sequential pattern mining have been proposed.
We refer the reader wishing to have a broad and deep view of this speciï¬c ï¬eld to a
survey of the literature, such as Mooney and Roddick (2013).

6

Thomas Guyet, RenÃ© Quiniou

Sequential pattern mining has many applications but sometimes a simple sequence
of items is not precise enough to capture the behaviors that are looked for. In addition,
under the general setting, sequential pattern mining may yield too many patterns
for low support thresholds or trivial patterns for high thresholds. To address these
limitations, the user would like to enrich the pattern syntax to obtain the information
she/he is looking for.

Some of these variations consist in integrating constraints in sequential pattern
mining (Pei et al., 2007), e.g. constraints on the size of pattern embeddings (maxspan)
or on the number of sequence itemsets that may be skipped between the occurrences
of two successive pattern itemsets (maxgap). The constraints that preserve the anti-
monotonicity property of the support reduce the number of patterns and thus decrease
the computation times. Some other variations consist in using alternative domains
of sequential patterns. A pattern domain speciï¬es new pattern shapes. For instance,
temporally-annotated sequences (Giannotti et al., 2006) are sequential patterns with
inter-event durations. There are many such extensions of sequential patterns devoted
to answering speciï¬c questions.

3 Negative sequential patterns

This section introduces negative sequential pattern mining which aims at capturing the
notion of the absence of some items in sequences. First, we introduce a syntax for the
domain of negative patterns. Then, we present several possible semantics for patterns
based on this syntax and show that some enjoys the anti-monotonicity property. Finally,
we extend negative sequential patterns to constrain negative sequential patterns.

3.1 Syntax of negative sequential patterns

The negative sequential pattern (NSP) model extends classical sequential patterns
by enabling the speciï¬cation of the absence of some itemsets. For example, ğ’‘ =
(cid:104)ğ‘ ğ‘ Â¬ğ‘ ğ‘’ ğ‘“ (cid:105) is a negative pattern. The symbol Â¬ before ğ‘ denotes that item ğ‘ must be
absent. Â¬ğ‘ is called a negative item. Semantically, ğ’‘ speciï¬es that items ğ‘, ğ‘, ğ‘’ and ğ‘“
occur in a row, but no item ğ‘ occurs between the occurrence of ğ‘ and the occurrence
of ğ‘’.

In the ï¬eld of string matching, negation is classically deï¬ned for regular ex-
pression. In this case, a pattern is an expression that can hold any kind of negated
pattern. The same principle gives the following most generic deï¬nition of nega-
tive sequential patterns: Let N be the set of negative patterns. A negative pattern
ğ’‘ = (cid:104)ğ‘1 Â· Â· Â· ğ‘ğ‘›(cid:105) âˆˆ N is a sequence where, for all ğ‘– âˆˆ [ğ‘›], ğ‘ğ‘– is a positive itemset
(ğ‘ğ‘– âŠ† I) or a negated pattern (ğ‘ğ‘– = Â¬{ğ‘ğ‘– }, ğ‘ğ‘– âˆˆ N ).

Due to its inï¬nite recursive deï¬nition, N appears to be too huge to be an interesting
and tractable search space for pattern mining. For instance, with I = {ğ‘, ğ‘, ğ‘}, it is
possible to express simple patterns like (cid:104)ğ‘ Â¬ğ‘ ğ‘(cid:105) but also complex patterns like
(cid:104)ğ‘ Â¬ (cid:104)ğ‘ ğ‘(cid:105)(cid:105). The combinatorics for such patterns is inï¬nite.

Now we provide our deï¬nition of negative sequential patterns (NSP) which intro-
duces some syntactic restrictions compared with the most generic case. These simple

NegPSpan: eï¬ƒcient extraction of negative sequential patterns

7

restrictions are broadly pointed in the literature (Kamepalli et al., 2014) and enable
us to propose an eï¬ƒcient algorithm.

Deï¬nition 6 (Negative sequential patterns (NSP)) A negative pattern ğ’‘ is a se-
quence (cid:104)ğ‘1 Â· Â· Â· ğ‘ğ‘›(cid:105) where, for all ğ‘– âˆˆ [ğ‘›], ğ‘ğ‘– is a positive itemset (ğ‘ğ‘– = {ğ‘ ğ‘—
ğ‘– } ğ‘— âˆˆ [ğ‘šğ‘– ], ğ‘ ğ‘—
I) or a negated itemset (ğ‘ğ‘– = Â¬{ğ‘ ğ‘—
ğ‘– âˆˆ I) under the two following con-
straints: consecutive negative itemsets are forbidden as well as negative itemsets at
the pattern boundaries. ğ‘šğ‘– (resp. ğ‘š(cid:48)
ğ‘–) is the size of the ğ‘–-th positive (resp. negative)
itemset. ğ’‘+, the positive part1 of pattern ğ’‘ denotes the subsequence of ğ’‘ restricted to
its positive itemsets.

ğ‘– ], ğ‘ ğ‘—

ğ‘– } ğ‘— âˆˆ [ğ‘š(cid:48)

ğ‘– âˆˆ

According to the non consecutive negative itemsets constraint, a negative pattern
ğ’‘ has the general form ğ’‘ = (cid:104)ğ‘1 Â¬ğ‘1 ğ‘2 Â¬ğ‘2 Â· Â· Â· ğ‘ğ‘›âˆ’1 Â¬ğ‘ğ‘›âˆ’1 ğ‘ğ‘›(cid:105) where ğ‘ğ‘– âˆˆ
2ğ¼ \ {âˆ…} and ğ‘ğ‘– âˆˆ 2ğ¼ for all ğ‘– âˆˆ [ğ‘›] (ğ‘ğ‘– may be empty). Under this notation, ğ’‘+ =
(cid:104)ğ‘1 ğ‘2 Â· Â· Â· ğ‘ğ‘›âˆ’1 ğ‘ğ‘›(cid:105).

Let us illustrate the syntactic restrictions by some pattern counterexamples that

our approach does not extract:

â€“ ï¬rst of all, a pattern is a sequence of positives and negative itemsets. It is not

possible to have patterns such as (cid:104)ğ‘ Â¬ (cid:104)ğ‘ ğ‘(cid:105)(cid:105)

â€“ then, successive negated itemsets such as (cid:104)ğ‘ Â¬ğ‘ Â¬ğ‘ ğ‘‘(cid:105) are not allowed.
â€“ ï¬nally, a pattern starting or ï¬nishing by a negated itemsets such as (cid:104)Â¬ğ‘ ğ‘‘(cid:105) is not

allowed as well.

Finally, it is worth noticing that the syntax of NSP introduced in Deï¬nition 6 can

not handle patterns with positive and negative events in the same itemset.

3.2 Semantics of negative sequential patterns

The semantics of negative sequential patterns relies on negative containment: a se-
quence ğ’” supports pattern ğ’‘ if ğ’” contains a sub-sequence ğ’”(cid:48) such that every positive
itemset of ğ’‘ is included in some itemset of ğ’”(cid:48) in the same order and for any negative
itemset Â¬ğ‘ğ‘– of ğ’‘, ğ‘ğ‘– is not included in any itemset occurring in the sub-sequence of
ğ’”(cid:48) located between the occurrence of the positive itemset preceding Â¬ğ‘ğ‘– in ğ’‘ and the
occurrence of the positive itemset following Â¬ğ‘ğ‘– in ğ’‘.

So far in the literature, the absence or non-inclusion of itemsets (represented
here as a negative itemset) has been speciï¬ed by loose formulations. The authors of
PNSP (Hsueh et al., 2008) have proposed the set symbol (cid:42) to specify non-inclusion.
This symbol is misleading since it does not correspond to the associated semantics
given in PNSP: an itemset ğ¼ is absent from an itemset ğ¼ (cid:48) if the entire set ğ¼ is absent
from ğ¼ (cid:48) (as opposed to at least some item from ğ¼ is absent from ğ¼ (cid:48)) which corresponds
to ğ¼ âˆ© ğ¼ (cid:48) = âˆ… in standard set notation, i.e. disjoint sets, and not ğ¼ (cid:42) ğ¼ (cid:48). We will call
PNSP interpretation total non inclusion, i.e. disjointness. It should be distinguished
from partial non inclusion which corresponds (correctly) to the set symbol (cid:42). The

1 Called the maximal positive subsequence in PNSP (Hsueh et al., 2008) and NegGSP (Zheng et al.,

2009) or the positive element id-set in eNSP.

8

Thomas Guyet, RenÃ© Quiniou

Table 1 Lists of supported sequences in D by negative patterns ğ’‘ğ’Š, ğ‘– = 1..4 under the total and partial non
inclusion semantics. Every pattern has the shape (cid:104)ğ‘ Â¬ğ‘ğ‘– ğ‘(cid:105) where ğ‘ğ‘– are itemsets such that ğ‘ğ‘– âŠ‚ ğ‘ğ‘–+1.

partial

non inclusion
(cid:42)ğº

ğ’‘1 = (cid:104)ğ‘Â¬ğ‘ğ‘(cid:105)
ğ’‘2 = (cid:104)ğ‘Â¬(ğ‘ğ‘‘) ğ‘(cid:105)
ğ’‘3 = (cid:104)ğ‘Â¬(ğ‘ğ‘‘ğ‘’) ğ‘(cid:105)
ğ’‘4 = (cid:104)ğ‘Â¬(ğ‘ğ‘‘ğ‘’ğ‘”) ğ‘(cid:105)

{ğ’”1, ğ’”3, ğ’”4 }
{ğ’”1, ğ’”2, ğ’”3, ğ’”4 }
{ğ’”1, ğ’”2, ğ’”3, ğ’”4 }
{ğ’”1, ğ’”2, ğ’”3, ğ’”4, ğ’”5 }

total

non inclusion
(cid:42)ğ·

{ğ’”1, ğ’”3, ğ’”4 }
{ğ’”1, ğ’”4 }
{ğ’”1 }
{ğ’”1 }

monotonic

anti monotonic

symbol (cid:42) was further used by the authors of NegGSP (Zheng et al., 2009) and
eNSP (Cao et al., 2016). The semantics of non inclusion is not detailed in NegGSP
and one cannot determine whether it means total or partial non inclusion.2 eNSP does
not deï¬ne explicitly the semantics of non inclusion but, from the procedure used to
compute the support of patterns, one can deduce that it uses total non inclusion.

Deï¬nition 7 (Non inclusion) We introduce two relations between itemsets ğ‘ƒ and ğ¼:

â€“ partial non inclusion: ğ‘ƒ (cid:42)ğº ğ¼ â‡” âˆƒğ‘’ âˆˆ ğ‘ƒ, ğ‘’ âˆ‰ ğ¼
â€“ total non inclusion: ğ‘ƒ (cid:42)ğ· ğ¼ â‡” âˆ€ğ‘’ âˆˆ ğ‘ƒ, ğ‘’ âˆ‰ ğ¼

Choosing one non inclusion interpretation or the other has consequences on ex-
tracted patterns as well as on pattern search. Let us illustrate this on related pattern
support in sequence dataset D

D =

ğ’”1 = (cid:104)(ğ‘ğ‘) ğ‘“ ğ‘(cid:105)
ğ’”2 = (cid:104)(ğ‘ğ‘) (ğ‘ ğ‘“ ) ğ‘(cid:105)
ğ’”3 = (cid:104)(ğ‘ğ‘) (ğ‘‘ğ‘“ ) ğ‘(cid:105)
ğ’”4 = (cid:104)(ğ‘ğ‘) (ğ‘’ ğ‘“ ) ğ‘(cid:105)
ğ’”5 = (cid:104)(ğ‘ğ‘) (ğ‘ğ‘‘ğ‘’ ğ‘“ ) ğ‘(cid:105)

ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´

ï£³

.

ï£¼ï£´ï£´ï£´ï£´ï£´ï£´ï£½
ï£´ï£´ï£´ï£´ï£´ï£´

ï£¾

Table 1 compares the support of progressively extended patterns under the two se-
mantics to show whether anti-monotonicity is respected or not. Let us consider the
inclusion of pattern ğ’‘2 = (cid:104)ğ‘ Â¬(ğ‘ğ‘‘) ğ‘(cid:105) in sequence ğ’”2. Since the positive part of ğ’‘2
is in ğ’”2, ğ’‘2 occurs in sequence ğ’”2 iï¬€ (ğ‘ğ‘‘) (cid:42)âˆ— (ğ‘ ğ‘“ ) ((ğ‘ğ‘‘) is not included in (ğ‘ ğ‘“ )). In
case of total non inclusion, it is false that (ğ‘ğ‘‘) (cid:42)ğ· (ğ‘ ğ‘“ ) because ğ‘ occurs in (ğ‘ ğ‘“ ),
and thus ğ’‘2 does not occur in ğ’”2. But in case of partial non inclusion, it is true that
(ğ‘ğ‘‘) (cid:42)ğº (ğ‘ ğ‘“ ), because ğ‘‘ does not occur in (ğ‘ ğ‘“ ), and thus ğ’‘2 occurs in ğ’”2.

Obviously, partial non inclusion satisï¬es anti-monotonicity while total non inclu-
sion satisï¬es monotonicity. In the sequel we will denote the general form of itemset
non inclusion by the symbol (cid:42)âˆ—, meaning either (cid:42)ğº or (cid:42)ğ·.

2 Actually, though not clearly stated, it seems that the negative elements of NegGSP patterns consist of

items rather than itemsets. In this case, total and partial inclusion are equivalent (see Proposition 6).

NegPSpan: eï¬ƒcient extraction of negative sequential patterns

9

Now, we formulate the notions of sub-sequence, non inclusion and absence by

means of the concept of embedding borrowed from (Negrevergne and Guns, 2015).

Deï¬nition 8 (Positive pattern embedding) Let ğ’” = (cid:104)ğ‘ 1 Â· Â· Â· ğ‘ ğ‘›(cid:105) be a sequence and
ğ’‘ = (cid:104)ğ‘1 Â· Â· Â· ğ‘ğ‘š(cid:105) be a (positive) sequential pattern. ğ’† = (ğ‘’ğ‘–)ğ‘– âˆˆ [ğ‘š] âˆˆ [ğ‘›] ğ‘š is an
embedding of pattern ğ’‘ in sequence ğ’” iï¬€ ğ‘ğ‘– âŠ† ğ‘ ğ‘’ğ‘– for all ğ‘– âˆˆ [ğ‘š] and ğ‘’ğ‘– < ğ‘’ğ‘–+1 for all
ğ‘– âˆˆ [ğ‘š âˆ’ 1]
Deï¬nition 9 (Strict and soft embeddings of negative patterns) Let ğ’” = (cid:104)ğ‘ 1 Â· Â· Â· ğ‘ ğ‘›(cid:105)
be a sequence, ğ’‘ = (cid:104)ğ‘1 Â· Â· Â· ğ‘ğ‘š(cid:105) be a negative sequential pattern and (cid:42)âˆ—âˆˆ {(cid:42)ğº, (cid:42)ğ·}.
ğ’† = (ğ‘’ğ‘–)ğ‘– âˆˆ [ğ‘š] âˆˆ [ğ‘›] ğ‘š is a soft-embedding of pattern ğ’‘ in sequence ğ’” iï¬€ for all

ğ‘– âˆˆ [ğ‘š]:
â€“ ğ‘ğ‘– âŠ† ğ‘ ğ‘’ğ‘– if ğ‘ğ‘– is positive
â€“ ğ‘ğ‘– (cid:42)âˆ— ğ‘  ğ‘— , âˆ€ ğ‘— âˆˆ [ğ‘’ğ‘–âˆ’1 + 1, ğ‘’ğ‘–+1 âˆ’ 1] if ğ‘ğ‘– is negative

ğ’† = (ğ‘’ğ‘–)ğ‘– âˆˆ [ğ‘š] âˆˆ [ğ‘›] ğ‘š is a strict-embedding of pattern ğ’‘ in sequence ğ’” iï¬€ for all

ğ‘– âˆˆ [ğ‘š]:
â€“ ğ‘ğ‘– âŠ† ğ‘ ğ‘’ğ‘– if ğ‘ğ‘– is positive
â€“ ğ‘ğ‘– (cid:42)âˆ— (cid:208) ğ‘— âˆˆ [ğ‘’ğ‘–âˆ’1+1,ğ‘’ğ‘–+1âˆ’1] ğ‘  ğ‘— if ğ‘ğ‘– is negative
Proposition 2 soft- and strict-embeddings are equivalent when (cid:42)âˆ—
Proof The proofs of all propositions are in Appendix A.

def

=(cid:42)ğ·.

Let ğ’‘+ = (cid:104)ğ‘ğ‘˜1 Â· Â· Â· ğ‘ğ‘˜ğ‘™ (cid:105) be the positive part of some pattern ğ’‘, where ğ‘™ denotes the
number of positive itemsets in ğ’‘. If ğ’† is an embedding of pattern ğ’‘ in some sequence
ğ’”, then ğ’†+ = (cid:104)ğ‘’ğ‘˜1 Â· Â· Â· ğ‘’ğ‘˜ğ‘™ (cid:105) is an embedding of the positive sequential pattern ğ’‘+ in ğ’”.
The following examples illustrate the impact of the itemset non-inclusion relation

and of the embedding type on pattern occurrence.

Example 3 (Itemset absence semantics) Let ğ’‘ = (cid:104)ğ‘ Â¬(ğ‘ğ‘) ğ‘‘(cid:105) be a pattern and four
sequences:

Sequence

(cid:42)ğ· (cid:42)ğº / strict-embedding (cid:42)ğº / soft-embedding

ğ’”1 = (cid:104)ğ‘ ğ‘ ğ‘ ğ‘’ ğ‘‘(cid:105)
ğ’”2 = (cid:104)ğ‘ (ğ‘ğ‘) ğ‘’ ğ‘‘(cid:105)
ğ’”3 = (cid:104)ğ‘ ğ‘ ğ‘’ ğ‘‘(cid:105)
ğ’”4 = (cid:104)ğ‘ ğ‘’ ğ‘‘(cid:105)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

(cid:51)

def

One can notice that each sequence contains a unique occurrence of (cid:104)ğ‘ ğ‘‘(cid:105), the
positive part of pattern ğ’‘. Using the soft-embedding and partial non-inclusion se-
=(cid:42)ğº), ğ’‘ occurs in sequences ğ’”1, ğ’”3 and ğ’”4 but not in ğ’”2. Using the
mantics ((cid:42)âˆ—
strict-embedding and partial non-inclusion semantics, ğ’‘ occurs in sequences ğ’”3 and
ğ’”4 considering that items ğ‘ and ğ‘ occur between occurrences of ğ‘ and ğ‘‘ in sequences
ğ’”1 and ğ’”2. With total non inclusion ((cid:42)âˆ—
=(cid:42)ğ·) and either type of embeddings, the
absence of an itemset is satisï¬ed if any of its item is absent. As a consequence, ğ’‘
occurs only in sequence ğ’”4.

def

10

Thomas Guyet, RenÃ© Quiniou

Another point that determines the semantics of negative containment concerns
the multiple occurrences of some pattern in a sequence: should every or only one
occurrence of the pattern positive part in the sequence satisfy the non inclusion
constraints? This point is not discussed in previous propositions for negative sequential
pattern mining. Actually, PNSP (Hsueh et al., 2008) and NegGSP (Zheng et al., 2009)
require a weak absence (at least one occurrence should satisfy the non inclusion
constraints) while eNSP requires a strong absence (every occurrence should satisfy
non inclusion constraints).

Deï¬nition 10 (Negative pattern occurrence) Let ğ’” be a sequence, ğ’‘ be a negative
sequential pattern. ğ’‘+ is the positive part of ğ’‘.
â€“ Pattern ğ’‘ weakly-occurs in sequence ğ’”, denoted ğ’‘ (cid:22) ğ’”, iï¬€ there exists at least one

(strict/soft)-embedding of ğ’‘ in ğ’”.

â€“ Pattern ğ’‘ strongly-occurs in sequence ğ’”, denoted ğ’‘ (cid:118) ğ’”, iï¬€ for any embedding ğ’†(cid:48)

of ğ’‘+ in ğ’” there exists an embedding ğ’† of ğ’‘ in ğ’” such that ğ’†(cid:48) = ğ’†.

Deï¬nition 10 allows for formulating two notions of absence semantics for negative

sequential patterns depending on the occurrence of the positive part:

â€“ strong occurrence: a negative pattern ğ’‘ occurs in a sequence ğ’” iï¬€ there exists at
least one occurrence of the positive part of pattern ğ’‘ in sequence ğ’” and every such
occurrence satisï¬es the negative constraints,

â€“ weak occurrence: a negative pattern ğ’‘ occurs in a sequence ğ’” iï¬€ there exists at
least one occurrence of the positive part of pattern ğ’‘ in sequence ğ’” and one of
these occurrences satisï¬es the negative constraints.

Example 4 (Strong vs weak occurrence semantics) Let ğ’‘ = (cid:104)ğ‘ ğ‘ Â¬ğ‘ ğ‘‘ (cid:105) be a pattern
and ğ’”1 = (cid:104)ğ‘ ğ‘ ğ‘’ ğ‘‘(cid:105) and ğ’”2 = (cid:104)ğ‘ ğ‘ ğ‘ ğ‘ ğ‘‘ ğ‘’ ğ‘ ğ‘‘(cid:105) be two sequences. The positive part of
ğ’‘ is (cid:104)ğ‘ ğ‘ ğ‘‘(cid:105). It occurs once in ğ’”1 so there is no diï¬€erence for occurrences under the
two semantics. But, it occurs three times in ğ’”2 with embeddings (1, 2, 5), (1, 2, 8) and
(4, 7, 8). The two ï¬rst occurrences do not satisfy the negative constraint (Â¬ğ‘) while
the third occurrence does. Under the weak occurrence semantics, pattern ğ’‘ occurs in
sequence ğ’”2 whereas under the strong occurrence semantics it does not.

The deï¬nitions of pattern support, frequent pattern and pattern mining task derive
naturally from the notion of negative sequential pattern occurrence, no matter the
choices for embedding (soft or strict), non inclusion (partial or total) and occurrence
(weak or strong). However, these choices concerning the semantics of NSPs impact
directly the number of frequent patterns (under the same minimal threshold) and
further the computation time. The stronger the negative constraints, the fewer the
number of sequences that hold some pattern, and the fewer the number of frequent
patterns.

Finally, we introduce a partial order on NSPs that is the foundation of our eï¬ƒcient

NSP mining algorithm.

Deï¬nition 11 (NSP partial order) Let ğ’‘ = (cid:104)ğ‘1 Â¬ğ‘1 ğ‘2 Â¬ğ‘2 Â· Â· Â· ğ‘ğ‘˜âˆ’1 Â¬ğ‘ğ‘˜âˆ’1 ğ‘ğ‘˜ (cid:105)
ğ‘˜(cid:48)(cid:105) be two NSPs s.t. ğ‘ğ‘– â‰  âˆ… for all
and ğ’‘(cid:48) = (cid:104)ğ‘(cid:48)
ğ‘(cid:48)
ğ‘– âˆˆ [ğ‘˜] and ğ‘(cid:48)

ğ‘˜(cid:48)âˆ’1 Â¬ğ‘(cid:48)
1 Â¬ğ‘(cid:48)
ğ‘– â‰  âˆ… for all ğ‘– âˆˆ [ğ‘˜ (cid:48)]. We write ğ’‘ (cid:67) ğ’‘(cid:48) iï¬€ ğ‘˜ â‰¤ ğ‘˜ (cid:48) and:

2 Â· Â· Â· ğ‘(cid:48)

2 Â¬ğ‘(cid:48)
ğ‘(cid:48)

ğ‘˜(cid:48)âˆ’1

1

NegPSpan: eï¬ƒcient extraction of negative sequential patterns

11

1. âˆ€ğ‘– âˆˆ [ğ‘˜ âˆ’ 1], ğ‘ğ‘– âŠ† ğ‘(cid:48)
2. ğ‘ğ‘˜ âŠ† ğ‘(cid:48)
ğ‘˜
3. ğ‘˜ (cid:48) = ğ‘˜ =â‡’ ğ‘ğ‘˜ â‰  ğ‘(cid:48)

ğ‘– and ğ‘ğ‘– âŠ† ğ‘(cid:48)
ğ‘–

ğ‘˜ (irreï¬‚exive)

Intuitively, ğ’‘ (cid:67) ğ’‘(cid:48) if ğ’‘ is shorter than ğ’‘(cid:48) and the positive and negative itemsets of
ğ’‘ are pairwise included into the itemsets of ğ’‘(cid:48). The classical pattern inclusion fails
to be anti-monotonic (Zheng et al., 2009) because pattern extension may change the
scope of negative itemsets. We illustrate what is happening on two examples. Let us
ï¬rst consider the case of a pattern ending with a negated itemset illustrated by Zheng
et al. (2009). Let ğ’‘(cid:48) = (cid:104)ğ‘ Â¬ğ‘ ğ‘(cid:105) and ğ’‘ = (cid:104)ğ‘ Â¬ğ‘(cid:105) be two NSPs. Removing ğ‘ from ğ’‘(cid:48)
makes the constraint of the pattern positive part weaker, and so the positive part is
more frequent. But, removing ğ‘ extends also the scope of the negative constraint and
makes it stronger and so, the negative pattern may be less frequent which violates the
anti-monotonicity property. This speciï¬c case does not impact our framework. In fact,
our deï¬nition of NSP (see Deï¬nition 6) does not allow ending an NSP with negated
itemsets. Let us now consider patterns ğ’‘(cid:48) = (cid:104)ğ‘ Â¬ğ‘ ğ‘‘ ğ‘(cid:105) and ğ’‘ = (cid:104)ğ‘ Â¬ğ‘ ğ‘(cid:105), and the
sequence ğ’” = (cid:104)ğ‘ ğ‘’ ğ‘‘ ğ‘ ğ‘(cid:105). ğ’‘(cid:48) occurs in ğ’” but not ğ’‘ since the scope of the negated
itemset Â¬ğ‘ is wider: it is restricted to the interval between the occurrences of ğ‘ and ğ‘‘
for ğ’‘(cid:48), but to the interval between the occurrences of ğ‘ and ğ‘ for ğ’‘.

What is important to note for the partial order (cid:67) of Deï¬nition 11, is that the
embedding of the pattern positive part yields an embedding for ğ’‘ that imposes the
negative constraints on the exact same scope than the negative constraints of ğ’‘(cid:48).
Thanks to the anti-monotonicity of (cid:42)ğ·, additional itemsets in negative patterns lead
to stronger constrain the containment relation. These remarks give some intuition
behind the following anti-monotonicity property (Proposition 3).

Proposition 3 (Anti-monotonicity of NSP support) The support of NSP is anti-
monotonic with respect to (cid:67) when (cid:42)âˆ—
=(cid:42)ğ· and weak-occurrences ((cid:22)) are considered.
Proof see Appendix A.

def

We can note that when the strong occurrence semantic ((cid:118)) is used, (cid:67) violates
the anti-monotonicity. Considering ğ’‘(cid:48) = (cid:104)ğ‘ (ğ‘ğ‘) Â¬ğ‘ ğ‘‘(cid:105), ğ’‘ = (cid:104)ğ‘ ğ‘ Â¬ğ‘ ğ‘‘(cid:105) and ğ’” =
(cid:104)ğ‘ b (ğ‘ğ‘) ğ‘’ ğ‘‘(cid:105), then it is true that ğ’‘(cid:48) (cid:118) ğ’”, but not that ğ’‘ (cid:118) ğ’”. There are two possible
embeddings for ğ’‘(cid:48) in ğ’‘(cid:48): (1, 2, 5) and (1, 3, 5). The ï¬rst one ((1, 2, 5), which does
not derive from the embedding of ğ’‘(cid:48)) does not satisfy the negative constraint.

A second example illustrates another case that is encountered when the postï¬x
of a sequence restricts the set of valid embeddings: ğ’‘(cid:48) = (cid:104)ğ‘ Â¬ğ‘ ğ‘‘ c(cid:105), ğ’‘ = (cid:104)ğ‘ Â¬ğ‘ ğ‘‘(cid:105)
and ğ’” = (cid:104)ğ‘ ğ‘’ ğ‘‘ ğ‘ ğ‘ ğ‘‘(cid:105). Again, ğ’‘(cid:48) occurs only once while ğ’‘ occurs twice and one
of its embeddings does not satisfy the negated itemset Â¬ğ‘. This example shows that
a simple postï¬x extension of an NSP leads to violate the anti-monotonicity property
under the strong occurrence semantics.

3.3 Gap constraints over negative sequential patterns

Numerical constraints have been introduced early in sequential constraints. Such
constraints can also be introduced in NSPs yielding constrained negative sequen-
tial patterns. We consider the two most common constraints on sequential patterns:

12

Thomas Guyet, RenÃ© Quiniou

maxgap (ğœƒ âˆˆ N) and maxspan (ğœ âˆˆ N) constraints. These constraints impact NSP
embeddings whatever their semantic. An embedding ğ’† = {ğ‘’ğ‘–, Â· Â· Â· , ğ‘’ğ‘›} of a pattern ğ’‘
in some sequence ğ’” satisï¬es the maxgap (resp. maxspan) constraint iï¬€ ğ’† satisï¬es the
following constraint: ğ‘’ğ‘–+1 âˆ’ ğ‘’ğ‘– â‰¤ ğœƒ (resp. ğ‘’ğ‘› âˆ’ ğ‘’1 â‰¤ ğœ) for all ğ‘– âˆˆ [ğ‘› âˆ’ 1].

Example 5 (Embedding of a constrained NSP) Let ğ’‘ = (cid:104)ğ‘ Â¬ğ‘ ğ‘ ğ‘‘(cid:105) be a NSP and
we consider the following constraints: not more than one itemset in between positive
itemsets occurrences (ğœƒ = 2) and not more than three itemsets in between the ï¬rst and
the last itemset occurrences (ğœ = 4). Table below illustrates on ï¬ve sequences whether
the sequence supports pattern ğ’‘ or not. These simple patterns and sequences lead to
the same results whatever the containment relation between NSP and sequences.

Sequence

(cid:104)ğ‘ ğ‘ ğ‘ ğ‘‘(cid:105)
(cid:104)ğ‘ ğ‘’ ğ‘ ğ‘‘(cid:105)
(cid:104)ğ‘ ğ‘’ ğ‘’ ğ‘ ğ‘‘(cid:105)
(cid:104)ğ‘ ğ‘’ ğ‘ ğ‘’ ğ‘‘(cid:105)
(cid:104)ğ‘ ğ‘’ ğ‘ ğ‘ ğ‘‘(cid:105)

ğœ = âˆ
ğœƒ = âˆ

ğœ = 4
ğœƒ = âˆ

ğœ = âˆ
ğœƒ = 2

ğœ = 4
ğœƒ = 2

(cid:51)
(cid:51)
(cid:51)
(cid:51)

(cid:51)

(cid:51)
(cid:51)

(cid:51)
(cid:51)
(cid:51)

(cid:51)
(cid:51)

It is worth noticing that the anti-monotonicity of the support is preserved with a
maxspan constraint but not with a maxgap constraint for frequent sequential patterns.
Nonetheless, the support becomes anti-monotonic for both constraints with the partial
order that takes only into consideration extensions at the end of sequential patterns
and not elsewhere. The partial order speciï¬ed in Deï¬nition 11 for negative sequential
patterns is based on the same idea of having extensions only at the end of a pattern.
Thus, the support of constrained NSP also enjoys the anti-monotonicity property.

Proposition 4 (Anti-monotonicity of constrained NSP support) The support of
NSP with maxgap or maxspan constraints is anti-monotonic with respect to (cid:67) when
(cid:42)âˆ—

=(cid:42)ğ· and weak-occurrences ((cid:22)) are considered.

def

Proof see Appendix A.

4 Algorithm NegPSpan

def

In this section, we introduce algorithm NegPSpan for mining NSPs from a dataset of
sequences under maxgap and maxspan constraints and under a soft absence semantics
with (cid:42)âˆ—
=(cid:42)ğ· for itemset inclusion. As stated in Proposition 2, no matter the embedding
strategy, they are equivalent under total itemset inclusion. Considering occurrences,
NegPSpan uses the weak-occurrence semantics: at least one occurrence of the negative
pattern is suï¬ƒcient to consider that it is supported by the sequence.

For computational reasons, we make an additional assumption for admissible
itemsets as negative itemsets: the negative itemsets are restricted to one element

NegPSpan: eï¬ƒcient extraction of negative sequential patterns

13

belonging to some language Lâˆ’ âŠ† IN. Considering all possible itemsets (i.e. IN) for
negatives is computationally costly. The deï¬nition of a subset Lâˆ’ allows to control
partially the combinatorics introduced by negative itemsets. In algorithm NegPSpan
presented below, Lâˆ’ = {ğ¼ = {ğ‘–1, Â· Â· Â· , ğ‘–ğ‘›}|âˆ€ğ‘˜, ğ‘ ğ‘¢ ğ‘ ğ‘(ğ‘–ğ‘˜ ) â‰¥ ğœ} denotes the set of
itemsets that can be built from frequent items. But this set could be arbitrarily deï¬ned
when the user is interested in the absence of some speciï¬c events. For instance, the
deï¬nition of Lâˆ’ could be changed into the set of frequent itemsets, which would be
more restrictive than the set of itemsets made of frequent items.

4.1 Main algorithm

NegPSpan is based on algorithm Preï¬xSpan (Pei et al., 2004) which implements a
depth ï¬rst search and uses the principle of database projection to reduce the number
of sequence scans. NegPSpan adapts the pseudo-projection principle of Preï¬xSpan
which uses a projection pointer to avoid copying the data. For NegPSpan, a projection
pointer of some pattern ğ’‘ is a triple (cid:104)ğ‘ ğ‘–ğ‘‘, ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘, ğ‘ğ‘œğ‘ (cid:105) where ğ‘ ğ‘–ğ‘‘ is a sequence
identiï¬er in the dataset, ğ‘ğ‘œğ‘  is the position in sequence ğ‘ ğ‘–ğ‘‘ that matches the last
itemset of the pattern (necessarily positive) and ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ is the position of the previous
positive pattern. In the Preï¬xSpan algorithm, the projection pointer is the couple
(cid:104)ğ‘ ğ‘–ğ‘‘, ğ‘ğ‘œğ‘ (cid:105). NegPSpan adds the position of the previous positive pattern to locate
the region of the sequence in which evaluate the presence or absence of a candidate
extensions of the last negative itemset.

Algorithm 1 details the main recursive function of NegPSpan for extending
a current pattern ğ’‘. The principle of this function is similar to Preï¬xSpan. Every
pattern ğ’‘ is associated with a pseudo-projected database represented by both the
original set of sequences S and a set of projection pointers ğ‘œğ‘ğ‘ğ‘ . First, the function
evaluates the size of ğ‘œğ‘ğ‘ğ‘  to determine whether pattern ğ’‘ is frequent or not. If so, it is

Algorithm 1: NegPSpan: recursive function for negative sequential pattern
extraction

in: S: set of sequences, ğ’‘: current pattern of length ğ‘˜, ğœ: minimum support threshold, ğ‘œğ‘ğ‘ğ‘ : list

of occurrences, I ğ‘“ : set of frequent items, ğœƒ: maxgap, ğœ: maxspan

1 Function NegPSpan (S, ğœ, ğ’‘, ğ‘œğ‘ğ‘ğ‘ , I ğ‘“ , ğœƒ, ğœ):

//Support evaluation of pattern ğ’‘
if |ğ‘œğ‘ğ‘ğ‘  | â‰¥ ğœ then

OutputPattern(ğ’‘, ğ‘œğ‘ğ‘ğ‘ );

else

return;

//Positive itemset composition
PositiveComposition(S, ğœ, ğ’‘, ğ‘œğ‘ğ‘ğ‘ , I ğ‘“ , ğœƒ, ğœ);
//Positive sequential extension
PositiveSequence(S, ğœ, ğ’‘, ğ‘œğ‘ğ‘ğ‘ , I ğ‘“ , ğœƒ, ğœ);
if |ğ’‘ | â‰¥ 2 and | ğ‘ğ‘˜ | = 1 then

//Negative sequential extension
NegativeExtension(S, ğœ, ğ’‘, ğ‘œğ‘ğ‘ğ‘ , I ğ‘“ , ğœƒ, ğœ);

2

3

4

5

6

7

8

9

14

Thomas Guyet, RenÃ© Quiniou

outputted, otherwise, the recursion is stopped because no larger patterns are possible
(anti-monotonicity property).

Then, the function explores three types of pattern extensions of pattern ğ’‘ into a

pattern ğ’‘(cid:48):

â€“ the positive sequence composition ((cid:32)ğ‘) consists in adding one item to the last
itemset of ğ’‘ (following the notations of Deï¬nition 11, this extension corresponds
to the case in which ğ’‘(cid:48) is the extension of ğ’‘ where ğ‘˜ (cid:48) = ğ‘˜, ğ‘ğ‘– = ğ‘(cid:48)
ğ‘– for all ğ‘– âˆˆ [ğ‘˜ âˆ’1]
and | ğ‘(cid:48)

ğ‘˜ | = | ğ‘ğ‘˜ | + 1),

â€“ the positive sequence extension ((cid:32)ğ‘ ) consists in adding a new positive singleton

itemset at the end of ğ’‘ (ğ‘˜ (cid:48) = ğ‘˜ + 1, ğ‘ğ‘– = ğ‘(cid:48)

ğ‘– for all ğ‘– âˆˆ [ğ‘˜ âˆ’ 1] and | ğ‘(cid:48)

ğ‘˜(cid:48) | = 1),

â€“ the negative sequence extension ((cid:32)ğ‘›) consists in appending a negative itemset
ğ‘– for all ğ‘– âˆˆ [ğ‘˜ âˆ’ 2],
ğ‘˜ = ğ‘ğ‘˜ ). In addition, NSP are negatively extended iï¬€

between the last two positive itemsets of ğ’‘ (ğ‘˜ (cid:48) = ğ‘˜, ğ‘ğ‘– = ğ‘(cid:48)
|ğ‘(cid:48)
| ğ‘ğ‘˜ | = 1. It prevents redundant exploration of same patterns (see Section 4.3).

ğ‘˜âˆ’1| = |ğ‘ğ‘˜âˆ’1| + 1 and ğ‘(cid:48)

The negative pattern extension is speciï¬c to our algorithm and is detailed in
next section. The ï¬rst two extensions are identical to Preï¬xSpan pattern extensions,
including their gap constraints management, i.e. maxgap and maxspan constraints on
positive patterns.

Proposition 5 The proposed algorithm is correct and complete.

Proof see Appendix A.

Intuitively, the algorithm is complete considering that the three extensions enable
to generate any NSP. For instance, pattern (cid:104)ğ‘ Â¬ğ‘’ ğ‘ (ğ‘ğ‘’) Â¬(ğ‘ğ‘‘) ğ‘(cid:105) would be generated
after evaluating the following patterns: (cid:104)ğ‘(cid:105) (cid:32)ğ‘  (cid:104)ğ‘ ğ‘(cid:105) (cid:32)ğ‘› (cid:104)ğ‘ Â¬ğ‘’ ğ‘(cid:105) (cid:32)ğ‘  (cid:104)ğ‘ Â¬ğ‘’ ğ‘ ğ‘(cid:105) (cid:32)ğ‘
(cid:104)ğ‘ Â¬ğ‘’ ğ‘ (ğ‘ğ‘’)(cid:105) (cid:32)ğ‘  (cid:104)ğ‘ Â¬ğ‘’ ğ‘ (ğ‘ğ‘’) ğ‘(cid:105) (cid:32)ğ‘› (cid:104)ğ‘ Â¬ğ‘’ ğ‘ (ğ‘ğ‘’) Â¬ğ‘ ğ‘(cid:105) (cid:32)ğ‘› (cid:104)ğ‘ Â¬ğ‘’ ğ‘ (ğ‘ğ‘’) Â¬(ğ‘ğ‘‘) ğ‘(cid:105).
Secondly, according to Proposition 3, the pruning strategy is correct.

4.2 Extension of patterns with negated itemsets ((cid:32)ğ‘›)

Algorithm 2 extends the current pattern ğ’‘ with negative items. It generates new
candidates by inserting an item ğ‘–ğ‘¡ âˆˆ I ğ‘“ , the set of frequent items. Let ğ‘ğ‘˜âˆ’1 and
ğ‘ğ‘˜ denote respectively the penultimate itemset and the last itemset of ğ’‘. If ğ‘ğ‘˜âˆ’1 is
positive, then a new negated itemset is inserted between ğ‘ğ‘˜âˆ’1 and ğ‘ğ‘˜ . Otherwise, if
ğ‘ğ‘˜âˆ’1 is negative, item ğ‘–ğ‘¡ is appended to ğ‘ğ‘˜âˆ’1. To prevent the redundant enumeration
of negative itemsets, only items ğ‘–ğ‘¡ (lexicographically) greater than the last item of
ğ‘ğ‘˜âˆ’1 can be added.

Once the pattern has been extended, lines 10 to 20 evaluate the candidate by
computing the pseudo-projection of the current database. According to the selected
semantics associated with (cid:42)ğ·, i.e. total non inclusion (see Deï¬nition 9), it is suï¬ƒcient
to check the absence of ğ‘–ğ‘¡ in the subsequence included between the occurrences of
the two positive itemsets surounding ğ‘–ğ‘¡. To achieve this, the algorithm checks the
sequence positions in the interval [ğ‘œğ‘ğ‘.ğ‘ ğ‘ğ‘Ÿğ‘’ğ‘‘ + 1, ğ‘œğ‘ğ‘.ğ‘ğ‘œğ‘  âˆ’ 1]. If ğ‘–ğ‘¡ does not occur
in itemsets from this interval, then the extended pattern occurs in the sequence ğ‘œğ‘ğ‘.ğ‘ ğ‘–ğ‘‘.

NegPSpan: eï¬ƒcient extraction of negative sequential patterns

15

Algorithm 2: NegPSpan: negative extensions

in: S: set of sequences, ğ’‘: current pattern of length ğ‘˜, ğœ: minimum support threshold, ğ‘œğ‘ğ‘ğ‘ : list

of occurrences, I ğ‘“ : set of frequent items, ğœƒ: maxgap, ğœ: maxspan

1 Function NegativeExtension(S, ğœ, ğ’‘, ğ‘œğ‘ğ‘ğ‘ , I ğ‘“ , ğœƒ, ğœ):
2

for ğ‘–ğ‘¡ âˆˆ I ğ‘“ do

if ğ‘ğ‘˜âˆ’1 is pos then

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

//Insert the negative item at the penultimate position
ğ’‘.ğ‘–ğ‘›ğ‘ ğ‘’ğ‘Ÿ ğ‘¡ (Â¬ğ‘–ğ‘¡);

else

if ğ‘–ğ‘¡ > ğ‘ğ‘˜âˆ’1.ğ‘ğ‘ğ‘ğ‘˜ () then

//Append an item to the penultimate (negative) itemset
ğ‘ğ‘˜âˆ’1.ğ‘ ğ‘ ğ‘ğ‘’ğ‘›ğ‘‘ (Â¬ğ‘–ğ‘¡);

else

continue;

ğ‘›ğ‘’ğ‘¤ğ‘œğ‘ğ‘ğ‘  â† âˆ…;
for ğ‘œğ‘ğ‘ âˆˆ ğ‘œğ‘ğ‘ğ‘  do

ğ‘“ ğ‘œğ‘¢ğ‘›ğ‘‘ â† ğ‘“ ğ‘ğ‘™ğ‘ ğ‘’;
for ğ‘  ğ‘ = [ğ‘œğ‘ğ‘. ğ‘ğ‘Ÿ ğ‘’ğ‘‘ + 1, ğ‘œğ‘ğ‘. ğ‘ğ‘œğ‘  âˆ’ 1] do

if ğ‘–ğ‘¡ âˆˆ ğ’”ğ’ğ’„ğ’„.ğ’”ğ’Šğ’… [ğ‘  ğ‘] then
ğ‘“ ğ‘œğ‘¢ğ‘›ğ‘‘ â† ğ‘¡ğ‘Ÿ ğ‘¢ğ‘’;
break;

if not ğ‘“ ğ‘œğ‘¢ğ‘›ğ‘‘ then

ğ‘›ğ‘’ğ‘¤ğ‘œğ‘ğ‘ğ‘  â† ğ‘›ğ‘’ğ‘¤ğ‘œğ‘ğ‘ğ‘  âˆª {ğ‘œğ‘ğ‘ };

else

//Look for an alternative occurrence
ğ‘›ğ‘’ğ‘¤ğ‘œğ‘ğ‘ğ‘  â† ğ‘›ğ‘’ğ‘¤ğ‘œğ‘ğ‘ğ‘ âˆª Match(ğ’”ğ’ğ’„ğ’„.ğ’”ğ’Šğ’…, ğ’‘, ğœƒ, ğœ);

NegPSpan (D, ğœ, ğ’‘, ğ‘›ğ‘’ğ‘¤ğ‘œğ‘ğ‘ğ‘ , I ğ‘“ );
ğ‘ğ‘˜âˆ’1. ğ‘ğ‘œ ğ‘ ();

Otherwise, to ensure the completeness of the algorithm, another occurrence of the
pattern has to be searched in the sequence (cf. Match function that takes into account
gap constraints).

For example, the ï¬rst occurrence of pattern ğ’‘ = (cid:104)ğ‘ ğ‘ ğ‘(cid:105) in sequence (cid:104)ğ‘ ğ‘ ğ‘’ ğ‘ ğ‘ ğ‘ ğ‘(cid:105)
is ğ‘œğ‘ğ‘ ğ’‘ = (cid:104)ğ‘ ğ‘–ğ‘‘, 2, 4(cid:105). Let us now consider ğ’‘(cid:48) = (cid:104)ğ‘ ğ‘ Â¬ğ‘’ ğ‘(cid:105), a negative extension of
ğ’‘. The extension of the projection-pointer ğ‘œğ‘ğ‘ ğ’‘ does not satisfy the absence of ğ‘’. So
a new occurrence of ğ’‘ has to be searched for. (cid:104)ğ‘ ğ‘–ğ‘‘, 6, 7(cid:105), the next occurrence of ğ’‘,
satisï¬es the negative constraint. Then, NegPSpan is called recursively for extending
the new current pattern (cid:104)ğ‘ ğ‘ Â¬ğ‘’ ğ‘(cid:105).

We can note that the gap constraints ğœ and ğœƒ are not mentioned explicitly in this
algorithm (except when a complete matching is required), but they impact indirectly
the algorithm by narrowing the interval of admissible sequence positions (line 13).

4.2.1 Extracting NSP without surrounding negations

An option may be used to restrict negated items to not be surrounded by itemsets
containing this item. This option is motivated by the objective to ease pattern under-

16

Thomas Guyet, RenÃ© Quiniou

standing. A pattern (cid:104)ğ‘ Â¬ğ‘ ğ‘ ğ‘(cid:105) may be interpreted as â€œthere is exactly one occurrence
of ğ‘ between ğ‘ and ğ‘â€. But, this may also lead to redundant patterns. For instance,
(cid:104)ğ‘ ğ‘ Â¬ğ‘ ğ‘(cid:105) matches exactly the same sequences as (cid:104)ğ‘ Â¬ğ‘ ğ‘ ğ‘(cid:105) (see section 4.3). This
second restriction can be disabled in our algorithm implementation. If so and for the
sake of simplicity, we choose to yield only pattern (cid:104)ğ‘ ğ‘ Â¬ğ‘ ğ‘(cid:105).

The set of such restricted NSPs can be extracted using the same algorithm, simply
by changing the candidate generation in Algorithm 2, line 2 by ğ‘–ğ‘¡ âˆˆ I ğ‘“ \ ( ğ‘ğ‘˜ âˆª ğ‘ğ‘˜âˆ’1).
The items to be added to a negative itemset are among frequent items where items of
surrounding itemsets are ï¬ltered out.

4.2.2 Extracting NSP with partial non inclusion ((cid:42)âˆ—

def

=(cid:42)ğº)

def

Algorithm 3 presents the variant of the negative extension algorithm for the partial
non-inclusion ((cid:42)âˆ—
=(cid:42)ğº). The backbone of the algorithm is similar: a candidate pattern
with a negated itemset at the penultimate position is generated and it assesses whether
this candidate is frequent or not. The absence of the itemset ğ‘–ğ‘  is checked in the
itemsets of the sequence at positions between the occurrence of the preceding itemset
and the occurrence of last itemset. The test in line 8 assesses that it is false that
ğ‘–ğ‘  (cid:42)ğº ğ’”ğ’ğ’„ğ’„.ğ’”ğ’Šğ’… [ğ‘ ğ‘]: ğ‘–ğ‘  is not partially non-included in one sequence itemset iï¬€ ğ‘–ğ‘  is a
subset of this itemset.

Contrary to Algorithm 2, it is not possible to use negative sequence extension
((cid:32)ğ‘›) to extend negative itemsets. With partial non inclusion the support is monotonic
(and not anti-monotonic). Thus, candidate patterns extended with negative itemsets
are generated based on a user deï¬ned Lâˆ’ âŠ† IN. We remind that the default Lâˆ’ is
the set of itemsets that can be built from frequent items, i.e. Lâˆ’ = {ğ¼ = {ğ‘–1, Â· Â· Â· , ğ‘–ğ‘›}|
âˆ€ğ‘˜, ğ‘ ğ‘¢ ğ‘ ğ‘(ğ‘–ğ‘˜ ) â‰¥ ğœ}. According to the monotony property if a pattern (cid:104)ğ‘ Â¬(ğ‘ğ‘) ğ‘‘(cid:105)
is not frequent, the pattern (cid:104)ğ‘ Â¬ğ‘ ğ‘‘(cid:105) is not frequent either. But, in practice, both
candidates will be evaluate by Algorithm 3. Thus, the combinatorics of this variant is
signiï¬cantly higher in practice because each element of Lâˆ’ is evaluated.

4.3 Redundancy avoidance

Algorithm NegPSpan is syntactically non-redundant but it can in practice generate
patterns that are semantically redundant.

For instance, pairs of patterns like (cid:104)ğ‘ Â¬ğ‘ ğ‘ ğ‘(cid:105) and (cid:104)ğ‘ ğ‘ Â¬ğ‘ ğ‘(cid:105) are syntactically
diï¬€erent but match exactly the same sequences. Semantically, such pattern could be
interpreted as â€œthere is not more than one occurrence of ğ‘ between ğ‘ and ğ‘â€. It is
possible to avoid generating both type of patterns eï¬ƒciently. Our solution is to avoid
the generation of candidate patterns with negative items that are in the last itemset.
Thus, only (cid:104)ğ‘ ğ‘ Â¬ğ‘ ğ‘(cid:105) would be generated. In Algorithm 2 line 2, the list of frequent
items I ğ‘“ is replaced by I ğ‘“ \ ğ‘ğ‘˜ . But, this modiï¬cation leads to loose the completeness
of the algorithm. In fact, the pattern (cid:104)ğ‘ Â¬ğ‘ ğ‘(cid:105) is not generated neither its semantically
equivalent pattern (cid:104)ğ‘ ğ‘ Â¬ğ‘(cid:105) because of the syntactic constraint on NSP that can not
end with a negative itemset. In practice, we do not manage this kind of redundancy.

NegPSpan: eï¬ƒcient extraction of negative sequential patterns

17

Algorithm 3: NegPSpan: negative extensions with partial non-inclusion
(alternative to Algorithm 2)

in: S: set of sequences, ğ’‘: current pattern, ğœ: minimum support threshold, ğ‘œğ‘ğ‘ğ‘ : list of

occurrences, I ğ‘“ : set of frequent items, ğœƒ: maxgap, ğœ: maxspan

1 Function NegativeExtension(S, ğœ, ğ’‘, ğ‘œğ‘ğ‘ğ‘ , I ğ‘“ , ğœƒ, ğœ):
2

for ğ‘–ğ‘  âˆˆ Lâˆ’ do

3

4

5

6

7

8

9

10

11

12

13

14

15

16

//Insert the negative itemset at the penultimate position
ğ’‘.ğ‘–ğ‘›ğ‘ ğ‘’ğ‘Ÿ ğ‘¡ (Â¬ğ‘–ğ‘ );
ğ‘›ğ‘’ğ‘¤ğ‘œğ‘ğ‘ğ‘  â† âˆ…;
for ğ‘œğ‘ğ‘ âˆˆ ğ‘œğ‘ğ‘ğ‘  do

ğ‘“ ğ‘œğ‘¢ğ‘›ğ‘‘ â† ğ‘“ ğ‘ğ‘™ğ‘ ğ‘’;
for ğ‘  ğ‘ = [ğ‘œğ‘ğ‘. ğ‘ğ‘Ÿ ğ‘’ğ‘‘ + 1, ğ‘œğ‘ğ‘. ğ‘ğ‘œğ‘  âˆ’ 1] do

if ğ‘–ğ‘  âŠ† ğ’”ğ’ğ’„ğ’„.ğ’”ğ’Šğ’… [ğ‘  ğ‘] then
ğ‘“ ğ‘œğ‘¢ğ‘›ğ‘‘ â† ğ‘¡ğ‘Ÿ ğ‘¢ğ‘’;
break;

if ! ğ‘“ ğ‘œğ‘¢ğ‘›ğ‘‘ then

ğ‘›ğ‘’ğ‘¤ğ‘œğ‘ğ‘ğ‘  â† ğ‘›ğ‘’ğ‘¤ğ‘œğ‘ğ‘ğ‘  âˆª {ğ‘œğ‘ğ‘ };

else

//Look for an alternative occurrence
ğ‘›ğ‘’ğ‘¤ğ‘œğ‘ğ‘ğ‘  â† ğ‘›ğ‘’ğ‘¤ğ‘œğ‘ğ‘ğ‘ âˆª Match(ğ’”ğ’ğ’„ğ’„.ğ’”ğ’Šğ’…, ğ’‘, ğœƒ, ğœ);

NegPSpan (D, ğœ, ğ’‘, ğ‘›ğ‘’ğ‘¤ğ‘œğ‘ğ‘ğ‘ , I ğ‘“ );
ğ‘ğ‘˜âˆ’1 = âˆ…;

We prefer the sound and correct option of not surrounding negative itemsets (see
section 4.2.1).

A syntactic redundancy is introduced by extending patterns with negative items.
For instance, the pattern (cid:104)ğ‘ Â¬ğ‘ (ğ‘ğ‘‘)(cid:105) may be reached by two distinct paths ğ‘1 :
(cid:104)ğ‘ ğ‘(cid:105) (cid:32)ğ‘ (cid:104)ğ‘ (ğ‘ğ‘‘)(cid:105) (cid:32)ğ‘› (cid:104)ğ‘ Â¬ğ‘ (ğ‘ğ‘‘)(cid:105) or ğ‘2 : (cid:104)ğ‘ ğ‘(cid:105) (cid:32)ğ‘› (cid:104)ğ‘ Â¬ğ‘ ğ‘(cid:105) (cid:32)ğ‘ (cid:104)ğ‘ Â¬ğ‘ (ğ‘ğ‘‘)(cid:105). To
solve this problem, the algorithm ï¬rst speciï¬es the negative itemsets as a composition
of negative items and then it composes the last itemset with new items. This discards
the path ğ‘1. In Algorithm 1, line 8 enables negative extension only if the last (positive)
itemset is of size 1.

4.4 Execution example

This section illustrates the execution of the algorithm on a small example. Let us
consider the dataset of sequences illustrated in Table 2 and the minimal support
threshold ğœ = 2. In this example, we consider the following semantics for negative
patterns: total non inclusion and strict absence. No gap constraints are considered
(ğœƒ = âˆ and ğœ = âˆ) and I = {ğ‘, ğ‘, ğ‘, ğ‘‘, ğ‘’, ğ‘“ }. In this example, candidate negative
itemsets are simply frequent items: Lâˆ’ = {ğ‘, ğ‘, ğ‘, ğ‘‘, ğ‘’}. Event ğ‘“ is not in Lâˆ’ because
it occurs only once and thus is not frequent under the minimal support ğœ.

Figure 1 illustrates the execution of NegPSpan algorithm on the dataset of Table 2
starting from pattern (cid:104)ğ‘(cid:105). The search tree illustrates the successive patterns explored by
the depth-ï¬rst search strategy. Each node details both the pattern and the corresponding

18

Thomas Guyet, RenÃ© Quiniou

Table 2 Dataset of sequences used in the execution example.

SID

Sequence

ğ’”1
ğ’”2
ğ’”3
ğ’”4

(cid:104)ğ‘ ğ‘ ğ‘ ğ‘’ ğ‘‘ (cid:105)
(cid:104)ğ‘ (ğ‘ğ‘) ğ‘’(cid:105)
(cid:104)ğ‘ ğ‘ ğ‘’ ğ‘‘ (cid:105)
(cid:104)ğ‘ ğ‘’ ğ‘‘ ğ‘“ (cid:105)

projected database. For the sake of space, the tree is simpliï¬ed and some nodes are
missing.

For patterns longer than two, projected sequences are represented in two gray
levels: the bold part of the sequence can be used to make positive extensions and the
gray part of the sequence is used to assess the absence of items for negative extension.
Two markers locate the projection pointer positions. The second pointer is the same
as the one computed by Preï¬xSpan.

Let us consider projected sequences of pattern (cid:104)ğ‘ ğ‘’(cid:105). In the ï¬rst sequence, ğ‘‘ is
green as it is the part of the sequence ending the sequence after the position of ğ‘’. ğ‘
and ğ‘ are in red because these events are in between occurrences of ğ‘ and ğ‘’. Pattern
(cid:104)ğ‘ ğ‘’(cid:105) can be extended in two ways:

â€“ with negative items among L \ {ğ‘, ğ‘’} (ğ‘ and ğ‘’ are removed if the restriction on

second restriction is activated),

â€“ with positive items among items that are frequent in the bold parts.

Considering extension of pattern (cid:104)ğ‘ ğ‘’(cid:105) with a negative item, e.g. Â¬ğ‘, each sequence
whose gray part contains the item is discarded, the others remain identical. The
extension by Â¬ğ‘ leads to pattern (cid:104)ğ‘ Â¬ğ‘ ğ‘’(cid:105) which is supported by sequences ğ’”3 and ğ’”4
only.

The extension of pattern (cid:104)ğ‘ ğ‘’(cid:105) by a positive item follows the same strategy as
Preï¬xSpan. In this case, the algorithm explores only the extension by item ğ‘‘ and the
projected pointers are updated to reduce further scanning.

Adding a new negative item while the penultimate item is negative consists in
appending it to the negative itemset. In case of pattern (cid:104)ğ‘ Â¬ğ‘ ğ‘’(cid:105), ğ‘‘ is the only candidate
because ğ‘’ is one of the surrounding events and ğ‘ is before ğ‘ in the lexicographic order.
Under total non inclusion, again, we simply have to discard sequences that contain
the item ğ‘‘ within their gray part.
For extending (cid:104)ğ‘ Â¬ğ‘‘ ğ‘’ ğ‘‘(cid:105), we can see that every combination of itemsets may quickly
satisfy all the negation constraints. This suggests ï¬rst to carefully select the appropriate
Lâˆ’ and second to use as big as possible negative itemsets to avoid pattern explosion.
Finally, we also notice that extensions with negative items are not terminal recur-
sive steps. Once negative items have been inserted, new positive items can be append
to the pattern. We encounter this case with pattern (cid:104)ğ‘ Â¬ğ‘‘ ğ‘’(cid:105) which is extended by
item ğ‘‘.

NegPSpan: eï¬ƒcient extraction of negative sequential patterns

19

Fig. 1 Example of algorithm NegPSpan search tree on the dataset of Table 2. Each node tree represents the
pattern on the left and the projected database on the right. The gray part of sequences is used to assess future
negations while the bold part of sequences is used for sequential ((cid:32)ğ‘ ). Dashed arrows represent negative
extensions ((cid:32)ğ‘›) while plain arrows are sequential ((cid:32)ğ‘ ) or compositional extensions ((cid:32)ğ‘). Arrow label
holds the item that is used in the extension.

5 Experiments

This section presents experiments on synthetic and real data. Experiments on synthetic
data aims at exploring and comparing NegPSpan and eNSP for negative sequential
pattern mining. The other experiments were conducted on real dataset to illustrate
results for negative patterns. NegPSpan and eNSP have been implemented in C++.3

5.1 Benchmark on synthetic datasets

This section presents experiments on synthetically generated data. We ï¬rst provide
some details about the random sequence generator. This sequence generator enables
to study the behavior of eNSP and NegPSpan for some speciï¬c dataset features and

3 Code, data generator and synthetic benchmark datasets can be tested online and downloaded here:

http://people.irisa.fr/Thomas.Guyet/negativepatterns/.

20

Thomas Guyet, RenÃ© Quiniou

algorithm parameters. First, we study the time-eï¬ƒciency and the number of extracted
patterns with respect to the minimal frequency threshold. Then, we complement the
analysis of the number of patterns with a study of the capacity of eNSP or NegPSpan
to extract accurately some NSPs known to be frequent in the synthetic dataset. Note
that additional experiments on the behavior of the algorithms with respect to the
vocabulary size and the average sequence length can be found in Appendix C.

5.1.1 Random sequence generator

The principle of our sequence generator is the following: generate random negative
patterns and randomly plant or not some of their occurrences into randomly generated
sequences. The main parameters are the total number of sequences (ğ‘›, default value
is ğ‘› = 500), the mean sequences length (ğ‘™ = 20), the vocabulary size i.e. the number
of items (ğ‘‘ = 20), the total number of patterns to plant (3), their mean length (4) and
the pattern minimal occurrence frequency in the dataset ( ğ‘“ = 10%).

In addition, the generator ensures that the positive partners of a NSP occurs in
a controlled number of the generated sequences. The number of occurrences of the
positive partners is in [ğœ†ğ‘šğ‘–ğ‘› Ã— ğ‘“ Ã— ğ‘›, ğœ†ğ‘šğ‘ğ‘¥ Ã— ğ‘“ Ã— ğ‘›].

For sake of fairness of our evaluation, the generated sequences ensures that eNSP
extracts less patterns than NegPSpan on our synthetic datasets. It is the case under
two speciï¬c conditions. First, the generated sequences are sequences of items (not
itemsets). In this case, Proposition 6 (see Appendix B) shows that any frequent NSP
with the semantic of eNSP is also frequent with the semantic of NegPSpan. Second,
we restrict Lâˆ’ to the set of frequent items also for fairness of evaluation. Indeed, for
sequences of items, patterns extracted by eNSP are only made of items and not itemsets
because positive partners must be a frequent sequential pattern in sequences of items.
This means that for sequence of items, eNSP does not manage any conjunction of
negative items. The restriction of Lâˆ’ for NegPSpan ensures that NegPSpan will not
extract patterns with negative itemsets of size strictly larger than 1.

The occurrences of negative pattern are randomly generated without restriction
on the gaps between item occurrences of a pattern. This means that the number of
occurrences of planted NSP may be higher than its evaluated support while taking
gap constraints into consideration.

5.1.2 Computation time and number of patterns

Figure 2 displays the computation time and the number of patterns extracted by eNSP
and NegPSpan on sequences of length 20 and 30, under three minimal thresholds
(ğœ = 10%, 15% and 20%) and with diï¬€erent values for the maxgap constraint (ğœ = 4,
7, 10 and âˆ). For eNSP, the minimal support of positive partners, denoted ğœ, is set to
70% of the minimal threshold ğœ. For both approaches, we bound the pattern length to
5 items. Each boxplot has been obtained with a 20 diï¬€erent sequence datasets. Each
run has a timeout of 5 minutes (300 ğ‘ ).

The main conclusion from Figure 2 is that NegPSpan is more eï¬ƒcient than
eNSP when maxgap constraints are used. As expected, eNSP is more eï¬ƒcient than
NegPSpan without any maxgap constraint. This is mainly due to the number of

NegPSpan: eï¬ƒcient extraction of negative sequential patterns

21

Fig. 2 Comparison of eNSP and NegPSpan on number of extracted patterns (left) and computation time
(right) for diï¬€erent values of maxgap (ğœ). Top (resp. bottom) ï¬gures correspond to datasets with average
sequence length equal to 20 (resp. 30). Boxplot colors correspond to diï¬€erent values of ğœ (10%, 15% and
20%).

extracted patterns. NegPSpan extracts signiï¬cantly more patterns than eNSP because
of diï¬€erent choices for the semantics of NSPs. First, eNSP uses a stronger negation
semantics. Without maxgap constraints, the set of patterns extracted by NegPSpan is
a superset of those extracted by eNSP (see proof in Appendix B).

An interesting result is that, for reasonably long sequences (20 or 30), even a weak
maxgap constraint (ğœ = 10) signiï¬cantly reduces the number of patterns and makes
NegPSpan more eï¬ƒcient. ğœ = 10 is said to be a weak constraint because it does not
cut early the search of a next occurring item compared to the length of the sequence
(20 or 30). This is of particular interest because maxgap is a quite natural constraint
when mining long sequences. It prevents from taking into account long distance
correlations that are likely irrelevant. Note that section 5.1.4 discusses the fairness
of this setting as NegPSpan extracts more planted patterns that eNSP does. Another
interesting question raised by these results is the real meaning of extracted patterns by
eNSP. In fact, under low frequency thresholds, it extracts numerous patterns that are
not frequent when weak maxgap constraints are considered. As a consequence, the
signiï¬cance of most of the patterns extracted by eNSP seems low when processing
datasets containing â€œlongâ€ sequences. Extensive experiments on the inï¬‚uence of
sequence length can be found in Appendix C.2.

5.1.3 Inï¬‚uence of minimum threshold

Figure 3 illustrates computation time and memory consumption with respect to min-
imum threshold for diï¬€erent settings: eNSP is run with diï¬€erent values for ğœ, the
minimal frequency of the positive partner of negative patterns (100%, 80% and 20%

llllllllllllllllllllllll=20l=30eNSPNegPSpanno constraintNegPSpant =15NegPSpant =10NegPSpant =7NegPSpant =41e+011e+031e+051e+011e+031e+05Nb negative patternsllllllllllllllllllllllllllll=20l=30eNSPNegPSpanno constraintNegPSpant =15NegPSpant =10NegPSpant =7NegPSpant =40.110.00.110.0Time (s)s0.10.150.222

Thomas Guyet, RenÃ© Quiniou

Fig. 3 Comparison of eNSP and NegPSpan computation time (left) and memory consumption (right) wrt
minimal support.

of the minimal frequency threshold) and NegPSpan is run with maxgap = 10 or
maxgap = 0. Computation times show similar behaviors as in previous experiments:
NegPSpan becomes as eï¬ƒcient as eNSP with a (weak) maxgap constraint. We can
also notice that the minimal frequency of the positive partners does not impact eNSP
computation times neither memory requirements.

The main result illustrated by Figure 3 is that NegPSpan consumes signiï¬cantly
less memory than eNSP. This comes from the depth-ï¬rst search strategy used by
NegPSpan which prevents from storing in memory many patterns. On the opposite,
eNSP requires to keep in memory all frequent positive patterns and their occurrence
lists. The lower the threshold, the more memory is required. When ğœ is low, then
the number of positive patterns is huge. We can see that the memory consumption
for ğœ = .2ğœ is 2 orders of magnitude larger than for ğœ = .8ğœ. The strategy of eNSP
appears to be practically intractable for large or dense datasets for which the number
of ğœ-frequent positive patterns is very high and requires a huge amount of memory.
This conclusion is consolidated by the experiment on the inï¬‚uence of vocabulary size
in Appendix C.1.

5.1.4 Study of the pattern recall

In this section, we study and compare the recall of eNSP and NegPSpan on simulated
data. The recall is the proportion of planted patterns in the dataset that are actually
extracted by the mining algorithm. The higher the recall, the better the mining al-
gorithm. NegPSpan is complete (i.e. a recall 1.0), but in case of the use of maxgap
constraints, it may miss some of the planted patterns. Conversely, eNSP is incomplete
due to the constraint on positive partners (that must be frequent). A negative pattern
can be frequent without satisfying this constraint and could be missed by algorithm
eNSP.

The recall is computed as the number of negative patterns extracted from 10
datasets of ğ‘› = 1000 random sequences each in which at least 10 negative patterns
were planted in at least 25, 5% of the sequences (ğœ†ğ‘šğ‘–ğ‘› = 0.15 and ğœ†ğ‘šğ‘ğ‘¥ = 0.2) and
with positive partners in 30% of the data.

The incompleteness of eNSP comes from the minimal support of positive partner
which is ğœ Ã— ğ‘‘. The incompleteness of NegPSpan comes from the maxgap constraint

â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—âˆ’5.0âˆ’2.50.02.55.00.10.20.3Frequency ThresholdTime (s)â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—1012140.10.20.3Frequency ThresholdMemory (bytes)lNegPSpanNegPSpant =10eNSP ( V = s )eNSP ( V = 0.8 s )eNSP ( V = 0.2 s )NegPSpan: eï¬ƒcient extraction of negative sequential patterns

23

Fig. 4 Recall of algorithms eNSP (on the left) and NegPSpan (on the right) wrt rthe minimal threshold
(ğœ) and parameters ğœ (minimal support of positive partners) or ğœ (maxgap constraint).

ğœ and also the minimal support threshold. In fact, the maxgap constraint reduces the
number of occurrences of an NSP. If the minimal support threshold is suï¬ƒciently low,
all the patterns are eï¬€ectively extracted.

Figure 4 illustrates the recall wrt the minimal support threshold (ğœ). Each curve
corresponds to one value of some algorithm parameter â€“ maxgap (ğœ) for NegPSpan
and the positive pattern threshold (ğœ) for eNSP â€“ that impacts the recall.

We notice that:

â€“ For eNSP and NegPSpan: the lower the support, the higher the recall but the
longer the mining process. In fact, if the minimal support is low enough, it is
always possible to ï¬nd the planted pattern (recall is 1 with very low ğœ). But
decreasing the support increases computational costs (see previous experiments).
â€“ For eNSP: curves with lower ğœ are below, so the lower ğœ, the higher the recall.
In fact, low ğœ values (e.g. .1ğœ) means that the constraint on the positive partner
is very weak. But, this choice may lead to generate many positive partners that
eNSP must analyse. Low ğœ values make the computation time increase and make
the memory requirement explode.

â€“ In addition, we can note that eNSP does not extract the complete set of patterns
with a minimal threshold set to 25% where NegPSpan does. This means that
eNSP requires to set the minimal support threshold lower than actually required
by the dataset to reach the perfect recall of 1.

â€“ For NegPSpan, curves with small maxgap value, ğœ, are below. The smaller ğœ, the

lower the support of a pattern and the fewer patterns are frequent.

â€“ Finally, curves with ğœ = .1ğœ or ğœ = 5 exhibit a perfect recall whatever the minimal
support between 1% and 25%. This means that NegPSpan extracts all the planted
patterns. We can note that the recall could decrease for higher support. In fact,
the generated datasets ensure that planted patterns occur in at least 25.5% of the
sequences, if the minimal frequency threshold ğœ is above this value, some planted
patterns may not be frequent at all.

Figure 5 compares the recall of eNSP with ğœ = 0.7ğœ and NegPSpan with ğœ = 10.
We can see on Figure 5 left that the recall curve of NegPSpan is above the recall curve
of eNSP and, at the same time, on Figure 5, on the right side, that the computation
times are lower. This demonstrates that NegPSpan extracts eï¬ƒciently and accurately
the planted negative patterns.

llllllllll0.000.250.500.751.000.100.150.200.25Frequency threshold ( s )recallllV =.1 sV =.2 sV =.5 sV =.8 sV = sllllllllll0.000.250.500.751.000.100.150.200.25Frequency threshold ( s )recallllt =5t =8t =12t =15No maxgap24

Thomas Guyet, RenÃ© Quiniou

Fig. 5 Recall of algorithms eNSP (on the left) and NegPSpan (on the right) wrt the minimal threshold (ğœ)
and parameters ğœ (ratio of positive patterns) or ğœ (maxgap constraint).

Let us now sum up the results on simulated datasets:

â€“ NegPSpan extracts a correct and complete set of NSP while eNSP is correct but

incomplete

â€“ NegPSpan with ğœ = 10 is more time-eï¬ƒcient than eNSP with ğœ = .7ğœ and has a

better recall

â€“ NegPSpan has a memory consumption several orders of magnitude lower than
eNSP, and its memory consumption increases several orders of magnitude slower
than eNSP with the average sequence length

â€“ NegPSpan is more time-eï¬ƒcient than eNSP on dense datasets (small vocabulary

size)

NegPSpan is then the best compromise for extracting eï¬ƒciently a set of negative

sequential patterns as much complete as possible.

These conclusions were drawn from synthetic datasets. Now, we supplement them

with an analysis of results obtained from real datasets.

5.2 Experiments on real datasets

In this section, we present experiments on real datasets coming from the SPMF
repository.4 These datasets consist of click-streams or texts represented as sequences
of items. For every dataset, we have computed the negative sequential patterns with
a maximum length ğ‘™ = 5 and a minimal frequency threshold set to ğœ = 5%. we set
maxgap to ğœ = 10 for NegPSpan and ğœ is set to .7ğœ for eNSP. Table 3 provides the
computation time, the memory consumption and the numbers of positive and negative
extracted patterns. Note that the numbers of positive patterns for eNSP are given for
ğœ threshold, i.e. the support threshold for positive partners used to generate negative
patterns.

For the sign dataset, the execution has been stopped after 10ğ‘šğ‘–ğ‘›ğ‘  to avoid running
out of memory. The number of positive patterns extracted by eNSP considering the ğœ
threshold is not equal to NegPSpan simply because of the maxgap constraint.

The results presented in Table 3 conï¬rm the results from experiments on synthetic
datasets. First, it highlights that NegPSpan requires signiï¬cant less memory for mining

4 http://www.philippe-fournier-viger.com/spmf/index.php?link=datasets.php

llllllllll0.000.250.500.751.000.100.150.200.25Frequency threshold ( s )recallâ—â—â—â—â—â—â—â—â—â—âˆ’2âˆ’1010.100.150.200.25Frequency threshold (Ïƒ)Time (s)lleNSPNegPSpanNegPSpan: eï¬ƒcient extraction of negative sequential patterns

25

Table 3 Results on real datasets with settings ğœ = 5%, ğ‘™ = 5, ğœ = 10, ğœ = .7ğœ. Bold faces highlight the
lowest computation times or memory consumptions.

NegPSpan

eNSP

time (ğ‘ ) mem (ğ‘˜ğ‘) #pos

#neg

time (ğ‘ ) mem (ğ‘˜ğ‘)

#pos

#neg

Sign
Leviathan
Bible
BMS1
BMS2
kosarak25k
MSNBC

15.51
6.07
38.82
0.16
0.37
0.92
40.97

6,220
19,932
68,944
22,676
39,704
24,424
41,560

348 1,357,278
39797
110
43,701
102
0
5
0
1
409
23
56,418
613

349.84 13,901,600 1,190,642 1,257,177
17,220
28.43
2,621
27.38
7
0.18
2
0.35
51
0.53
5,439
41.44

428,916
552,288
34,272
53,608
43,124
808,744

7,691
1,364
8
3
50
2,441

every dataset. Second, NegPSpan outperforms eNSP for datasets having a long mean
sequence length (Sign, Leviathan, and MSNBC). In case of the Bible dataset, the
number of extracted patterns by eNSP is very low compared to NegPSpan due to
the constraint on minimal frequency of positive partners. NegPSpan fails to identify
negative patterns from BMS datasets, but eNSP extracts few negative patterns. It can
be explained by the maxgap constraint for NegPSpan that lowers the support of the
few eNSP patterns below the frequency threshold.

6 Case studies

In this section, we presents the use of NSPs on three case studies that show both the
wide range of applications of NSPs and some beneï¬ts of NegPSpan for extracting
NSPs. It presents the output of the algorithms and analyze them. The computational
performances of the algorithms on these case studies datasets are presented in Ap-
pendix C.3.

First, we present brieï¬‚y some outputs on the classical market basket dataset, then
we present a complete study of care pathway data to compare patterns extracted by
eNSP and NegPSpan, and ï¬nally we apply NSP mining for customer relationship
management purpose and we present how to take decisions with extracted NSPs.

6.1 Instacart data

Instacart is an online grocery service. It has published a dataset containing information
on 3 million grocery orders from more than 200,000 users from 2017.5 The dataset
contains information on what products users purchased, the sequence they bought
them in and the amount of time between Instacart orders. The number of items is
above 50, 000 with a lot of items that are rarely bought. The objective of Instacart
was to foster research on algorithms that can predict what items customers will buy
or may be interested in.

5 The

Instacart

dataset

is

available

on

Kaggle:

https://www.kaggle.com/c/

instacart-market-basket-analysis

26

Thomas Guyet, RenÃ© Quiniou

The motivation for extracting negative sequential patterns from purchase data is to
identify which products are surprisingly absent from sequences of orders. A product
is surprisingly absent when it is absent but the analyst assumes it should not. It is
the case when retailers assume that some products are very correlated. For instance,
customers that bought breakfast cereal without buying milk at any time could be
surprising. This information can help retailers to better understand customer habits
and to suggest commercial actions to increase sales. For instance, it could be proï¬table
to advertise some milk products to the above customers.

In these experiments, NSPs were extracted with two minimal frequency thresh-
olds: 0.5% and 1%. The vocabulary size requires to set such very low thresholds
to identify non-trivial patterns, but such low thresholds prevent eNSP from process-
ing the whole dataset. For this reason, we analyzed only the ï¬rst 50, 000 customers.
The main parameter values of each algorithm were set to ğœ âˆˆ {2, 3, 4, 6, 8} and
ğœ âˆˆ {0.4, 0.5, 0.6, 0.7}.

Let us now analyze the set of patterns extracted with a minimal support ğœ = 1% and
the settings ğœ = 0.4 for eNSP and ğœ = 4 for NegPSpan. eNSP extracted 6, 499 patterns,
while NegPSpan extracted 8, 096 patterns. With higher values for ğœ, the number of
NSPs makes the comparison diï¬ƒcult.

Despite the low threshold, a large part of the patterns extracted by NegPSpan falls

into one of the following shapes of patterns (ğ‘‹ and ğ‘Œ represent itemsets):

â€“ ğ’‘ğ‘°

â€“ the pattern is positive (contains no negation),
â€“ ğ’‘ğ‘°
â€“ ğ’‘ğ‘°

1 = (cid:104)13176 Â¬ğ‘‹ 13176(cid:105) where 13176 is bag (86 patterns)
2 = (cid:104)21903 Â¬ğ‘‹ 21903(cid:105) where 21903 is organic baby spinach ravioli
(86 patterns)
3 = (cid:104)24852 Â¬ğ‘‹ 24852 Â¬ğ‘Œ 24852(cid:105) where 24852 is banana (7, 830 patterns)
The patterns of type ğ’‘ğ‘°
3

represent more than 90% of the extracted patterns. Among
these NSPs, some seems interesting because they involve diï¬€erent fruits. It is inter-
esting to identify customers purchasing lots of bananas and who do not purchase
any other fruit during the same period. Nevertheless, most of these patterns involves
products that are not fruits, and are not really meaningful. In this case, the posi-
tive partner constraint of eNSP would discard the patterns whose positive pattern,
(cid:104)24852 Â¬ğ‘‹ 24852 Â¬ğ‘Œ 24852(cid:105), is not ğœ-frequent. And we can expect that eNSP would
extract a small set of meaningful patterns, but eNSP extracts a similar amount of
patterns (6, 499).

eNSP extracts similar patterns but also a collection of more diverse patterns. The
diversity comes from the absence of gap constraints. The embedding of a sequential
pattern without gap constraint may involve purchases that are very distant in the
sequence of purchases. This case study questions the signiï¬cance of such patterns.
Retailers are in reality not interested in relationships involving client purchases that
are very distant in time. In some way, we can say that the absence of gap constraints
makes eNSP over-estimates the support of patterns.

The conclusion of this case study is that eNSP has better computation time results
on this dataset, but it did not succeed in processing the whole dataset. NegPSpan is
time-consuming but ï¬t in memory even on a large and sparse dataset. The computation

NegPSpan: eï¬ƒcient extraction of negative sequential patterns

27

costs of NegPSpan comes from the enumeration of patterns with the same positive
shape: it combines positive patterns with all possible negative items that could make
them frequent. At the end, the patterns extracted by each algorithm are diï¬ƒcult to
interpret. The patterns extracted by eNSP may be somehow more signiï¬cant thanks
to the positive partner constraint. But without gap constraint, most of the extracted
NSPs are irrelevant for retailers.

To process such large but sparse dataset, it would be interesting to beneï¬t from the
best of the two approaches: a relevant NSP semantics including gap constraints and
a reasonable number of diverse NSPs. Thanks to the anti-monotonicity property of
its semantics for negative patterns, NegPSpan could be adapted to ï¬t in some recent
frameworks for mining diverse patterns eï¬ƒciently (Bosc et al., 2018). Alternatively,
the frequency constraint on positive partners could be applied as a post-processing
step to prevent NegPSpan from extracting useless patterns. It would not be eï¬ƒcient,
but could provide more interpretable results for this case study.

6.2 Care pathway analysis

This section presents the use of NSPs for analyzing epileptic patient care pathways.
Recent studies suggest that medication changes may be associated with epileptic
seizures for patients with long term treatment with anti-epileptic medication (Polard
et al., 2015). NSP mining algorithms are used to extract patterns of drugs deliveries
that may inform the suppression of a drug from a patient treatment. In (Dauxais et al.,
2017), we studied discriminant temporal patterns but it does not explicitly extract the
information about medication absence as a possible explanation of epiletic seizures.
Our dataset was obtained from the french insurance database (Moulis et al., 2015).
8,379 epileptic patients were identiï¬ed by their hospitalization related to an epileptic
seizure. For each patient, we built a sequence of drugs deliveries within the 90 days
before the ï¬rst epileptic seizure. For each drug delivery, an event is a tuple (ğ‘š, ğ‘”)
where ğ‘š is the ATC6 code of the active molecule, ğ‘” âˆˆ {0, 1} is the brand-name (0) vs
generic (1) status of the drug. For the sake of readability, an identiï¬er is assigned to
each event tuple. Table 4 gives the mapping between event identiï¬ers and event tuples.
For example, the following sequence representing a sequence of drug deliveries with
a switch from generic to brand-name valproic acid:

(cid:104) (valproic acid, ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿğ‘–ğ‘) ((valproic acid, ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿğ‘–ğ‘), (paracetamol, ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿğ‘–ğ‘)) . . .
(valproic acid, ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿğ‘–ğ‘) (valproic acid, ğ‘ğ‘Ÿğ‘ğ‘›ğ‘‘) (valproic acid, ğ‘ğ‘Ÿğ‘ğ‘›ğ‘‘) (cid:105)

is translated into the sequence:

(cid:104)383 (383, 86) 383 114 114(cid:105)

The dataset contains 251,872 events over 7,180 diï¬€erent drugs. The mean length of
a sequence is 7.89Â±8.44 itemsets. The sequence length variance is high. This is due to
the heterogeneous nature of care pathways. Some of them represent complex therapies

6 ATC: Anatomical Therapeutic Chemical Classiï¬cation System is a drug classiï¬cation system that

classiï¬es the active ingredients of drugs.

28

Thomas Guyet, RenÃ© Quiniou

Table 4 Selection of events identiï¬ers that are used in the discussed patterns.

Event tuple

Event id

(levetiracetam, generic)
(paracetamol, generic)
(zolpidem, generic)
(valproic acid, brand)
(clobazam, generic)
(zopiclone, generic)
(phenobarbital, generic)
(valproic acid, generic)

7
86
112
114
115
151
158
383

Table 5 Patterns involving valproic acid switches with their supports computed by eNSP and NegPSpan.
Empty supports indicate that the pattern has not been extracted. Drug names associated with identiï¬ers in
the ï¬gures are given within the text.

Pattern

ğ’‘ğ‘ªğ‘·
1
ğ’‘ğ‘ªğ‘·
2
ğ’‘ğ‘ªğ‘·
3
ğ’‘ğ‘ªğ‘·
4
ğ’‘ğ‘ªğ‘·
5
ğ’‘ğ‘ªğ‘·
6
ğ’‘ğ‘ªğ‘·
7
ğ’‘ğ‘ªğ‘·
8

= (cid:104)383 Â¬(86, 383) 383(cid:105)
= (cid:104)383 Â¬86 383(cid:105)
= (cid:104)383 Â¬112 383(cid:105)
= (cid:104)383 Â¬114 383(cid:105)
= (cid:104)383 Â¬115 383(cid:105)
= (cid:104)383 Â¬151 383(cid:105)
= (cid:104)383 Â¬158 383(cid:105)
= (cid:104)383 Â¬7 383(cid:105)

support
eNSP

support
NegPSpan

1,579
1,251
1,610
1,543
1,568
1,611
1,605

1,243

1,232
1,236

1,243

involving the consumption of many diï¬€erent drugs while others are simple cases
consisting of few deliveries of anti-epileptic drugs. In the french health system, drug
deliveries are renewed every month. For epileptic patients that require a continuous
medication, we expect to have mostly sequences with three or four deliveries of anti-
epileptic drugs. Other items are additional medical treatments that are not necessarily
related to epilepsy.

Let us now compare the pattern sets extracted by eNSP and NegPSpan. In fact,
expressible pattern constraints are diï¬€erent (maxgap constraints for NegPSpan and
minimal support of positive partners for eNSP) and the extracted pattern sets are
diï¬€erent. In this qualitative experiment, the parameters is set to ğœ = 14.3% (1, 200
sequences), ğ‘™ = 3 (maximal pattern length), ğœ = 3 for NegPSpan and ğœ = .1 Ã— ğœ the
minimal support for positive partners for eNSP. eNSP extracted 1,120 patterns and
NegPSpan only 10 patterns (including positive and negative patterns). Due to a low ğœ
threshold, many positive patterns were extracted by eNSP leading to generate a lot of
patterns having a single negated item. In this analysis, we pay attention to the specialty
of valproic acid which exists in generic form (event 383) or brand-named form
(event 114). Table 5 presents all patterns starting and ï¬nishing with event 383. Other
events correspond to alternative anti-epileptic drugs (levetiracetam, phenobarbital)
or psycholeptic drugs (zolpidem, clobazam, zopiclone) except paracetamol.

NegPSpan: eï¬ƒcient extraction of negative sequential patterns

29

5

2

, ğ’‘ğ‘ªğ‘·
4

and ğ’‘ğ‘ªğ‘·

It is interesting to note that only 3 patterns ( ğ’‘ğ‘ªğ‘·

) are extracted
by both algorithms. Their supports are lower with NegPSpan because of the maxgap
constraint. This constraint also explains that patterns ğ’‘ğ‘ªğ‘·
are not extracted
by NegPSpan. These patterns illustrate that in some cases, the patterns extracted by
eNSP may not be really interesting because they involve distant events in the sequence.
is not extracted by NegPSpan due to the syntactic constraints. On the
Pattern ğ’‘ğ‘ªğ‘·
opposite, NegPSpan extracts patterns that eNSP misses. For instance, pattern ğ’‘ğ‘ªğ‘·
is
not extracted by eNSP because its positive partner, (cid:104)383 7 383(cid:105), is not frequent. In this
case, it leads eNSP to miss a potentially interesting pattern involving two anti-epileptic
drugs.

and ğ’‘ğ‘ªğ‘·

1

6

8

3

Next, we examine a particular case to illustrate that NSPs are essential to have true
insights about the meaning of frequent positive patterns. Remind that our objective
is to conclude about the potential link between switches in epileptic drug deliver-
ies and epileptic seizures. Our point is that positive patterns can be misinterpreted,
because they only show what actually happens. Negative patterns may avoid some
misinterpretation.

For this experiment, we changed the NegPSpan settings to focus on patterns
involving a switch from the generic form to the brand-named form of valproic acid.
The settings were ğœ = 1.2%, ğ‘™ = 3 and ğœ = 5. The only two frequent positive
patterns are (cid:104)114 383 114(cid:105) and (cid:104)114 114(cid:105). In our experiment, it means that these
patterns occur frequently in patient having an epileptic seizure. Because (cid:104)114 114(cid:105) is
a positive pattern, it does not means that there is no switches in sequences that hold
this pattern. Thus, positive patterns are not conclusive about the impact of a switch
from 114 to 383 on epileptic seizures.

NegPSpan also extracts the pattern (cid:104)114 Â¬383 114(cid:105), which speciï¬es that the
absence of a switch is frequent for patients having epileptic seizure. This time, it
sheds light on the fact that switches and non-switches from generic form to brand-
named form of valproic acid are both frequent behaviors in care pathways of patients
having epileptic seizure. We can conclude that pattern mining techniques does not
establish a statistical link between epileptic drug switches and epileptic seizure. This
result is consistent with (Polard et al., 2015).

6.3 Customer Relationship Management

Customer Relationship Management (CRM) refers to a set of tools that is used for
managing the interactions between a company and its customers. As its main objective
is to maintain regular customers, by developing long-term relationships with them, and
to acquire new ones, CRM is being used by any organization to identify the problems
of customers and to improve the consistency with them. To fullï¬ll the objectives
above, it is required from the company to perform customer relationship analysis in
order to meet customer needs and improve customer services. However, due to the
increased number of business data generated by companies during the last years, the
most useful and valuable information required for customer analysis is often hidden
in the large CRM databases and is not easily accessible.

30

Thomas Guyet, RenÃ© Quiniou

Table 6 Frequent contact reasons in the CRM database.

Id

Name

Frequency (%)

1CTR
2VIE
1RES
6DIS
8DOS
3SER
4FAC
5REC
Unknown
3RCTR

Contract subscription
Contract modiï¬cation
Contract termination
Relationship with supplier
File handling
Advices and services
Billing
Recovery
Unknown
Contract

30.73%
20.67%
12.42%
6.62%
6.47%
4.56%
4.08%
3.04%
2.4%
1.9%

According to Ngai et al. (2009), a common set of supporting tools (statistical
analysis, cluster analysis, probability theory, artiï¬cial neural networks, etc) widely
used from companies for making relevant CRM decisions is data mining, which are
good at extracting and identifying useful knowledge from large customer databases.
Speciï¬c data mining techniques like association rules mining or sequential pattern
mining are especially useful for analyzing customer data. Frequent sequential pattern
mining is particularly useful in commercial applications, as it can be used to discover
customersâ€™ behavioral and purchasing patterns over time (Mallick et al., 2013).

The negative events are meaningful in CRM to inform stakeholders about which
actions was absent in the customer interactions. An absent interaction may be an action
missed by customer services. This way, NSPs enable decision makers to identify
potential improvements in the customer relationship management.

In this case study, we illustrate how some extracted NSPs can be interpreted
to make decisions for improving customer relationship management. We analyze
sequences corresponding to the interactions of customers with the services of an
electricity supplier in order to prevent contract cancellations.

The dataset has 375, 142 customers (one sequence per customer) with a vocabulary
of 93 diï¬€erent items. Items are tags that label the contact reasons (for instance invoicing
procedures, technical issues or commercial purposes). Table 6 presents the 16 items
that occurs more than 1% in the dataset. The dataset does not specify whether a contact
was initiated by the company or by the customer. A contact between the company and
the customer can be labeled with several tags. The average number of tags per contact
is 1.21Â±0.34. This means that most of the itemsets hold only 1 item (a contact has
only one contact reason), but it raises up to 10 tags for a unique contact. The average
length of the sequences is 2.52Â±7.26: most of the sequences are short (2 or 3 contacts
within a period of one year) but some customers have much more interactions with
the company.

The frequent NSPs is run with a minimal support threshold ğœ = 0.5% (at least
1,875 occurrences of an NSP), ğœ = 0.4ğœ for eNSP and ğœ = 4 NegPSpan. NegPSpan
(resp. eNSP) extracts 18, 652 (resp. 6, 746) patterns. The two sets of extracted NSPs
have 1, 463 patterns in common. So, NegPSpan extracts more NSPs than eNSP does

NegPSpan: eï¬ƒcient extraction of negative sequential patterns

31

Table 7 Selection of patterns involving 3SER (on the left) or 2VIE (on the right) negative items with their
supports computed by NegPSpan but not by eNSP.

Pattern

ğ’‘ğ‘ª ğ‘¹ğ‘´
1
ğ’‘ğ‘ª ğ‘¹ğ‘´
2
ğ’‘ğ‘ª ğ‘¹ğ‘´
3
ğ’‘ğ‘ª ğ‘¹ğ‘´
4
ğ’‘ğ‘ª ğ‘¹ğ‘´
5
ğ’‘ğ‘ª ğ‘¹ğ‘´
6
ğ’‘ğ‘ª ğ‘¹ğ‘´
7

= (cid:104)6ğ·ğ¼ ğ‘† Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
= (cid:104)1ğ‘…ğ¸ğ‘† Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
= (cid:104)(1ğ¶ğ‘‡ ğ‘…, 8ğ·ğ‘‚ğ‘†) Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
= (cid:104)(1ğ¶ğ‘‡ ğ‘…, 2ğ‘‰ ğ¼ ğ¸ , 8ğ·ğ‘‚ğ‘†) Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ ğ‘† (cid:105)
= (cid:104)(3ğ‘†ğ¸ ğ‘…, 8ğ·ğ‘‚ğ‘†) Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
= (cid:104)(1ğ¶ğ‘‡ ğ‘…, 3ğ‘†ğ¸ ğ‘…), Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
= (cid:104)1ğ¶ğ‘‡ ğ‘… 2ğ‘‰ ğ¼ ğ¸ 8ğ·ğ‘‚ğ‘† Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)

support
NegPSpan

5,352
5,265
6,597
1,946
3,032
2,258
1,912

in previous case studies but it does it faster. NegPSpan requires 113ğ‘  computation time
while eNSP requires 786ğ‘ . In the following, we do not compare the set of extracted
patterns, but we focus our attention on the interpretation of patterns extracted by
NegPSpan.

Among the 18, 652 patterns extracted by NegPSpan, we selected only the patterns
ending by the contract termination item (1ğ‘…ğ¸ ğ‘†) and which have only one negative
event. There are 281 patterns satisfying these conditions. This amount of patterns is
still too large to be analyzed. Then, the practical objective of this case study suggests to
focus our interest on actionable patterns. The information extracted aims at suggesting
actions to improve customer relation management. The customer services can impact
customer decisions mainly by providing services (3SER item) or by modifying the
customer contract (2VIE item). For this reason, we look at the NSPs that involve the
3SER item or the 2VIE item in the negative part.

Table 7 illustrates some interesting patterns extracted only by NegPSpan. The
complete lists of selected patterns is in Appendix D: 20 NSP are extracted by both
eNSP and NegPSpan, 18 are extracted only by eNSP and 20 are extracted only by
NegPSpan.

1

, ğ’‘ğ‘ª ğ‘¹ğ‘´
2

and ğ’‘ğ‘ª ğ‘¹ğ‘´
3

Patterns ğ’‘ğ‘ª ğ‘¹ğ‘´

are interesting because they are almost frequent
but are not extracted by eNSP. It seems that the support of their positive partner does
not exceed ğœ = 0.4ğœ (750 sequences). Nonetheless, the following comments show
that they are insightful and can provide interesting information about the behavior of
customers:

â€“ ğ’‘ğ‘ª ğ‘¹ğ‘´
1

are customers that contacted the company with a concern about distribution
of the electricity (6ğ· ğ¼ğ‘†) and that did not get any advice or service (3ğ‘†ğ¸ ğ‘…). For the
5, 368 customers, it leads to a contract termination. It can be notice that customers
that experience this pattern amounts to about 2% but contacts for the 6ğ· ğ¼ğ‘† purpose
amounts to 6.62% of the contact reasons. Thus, it is a relatively frequent pathway
for customers having this contact reason. This pattern highlights that customer
services have to provide advice or services to prevent customer churn in this case.
are customers who contacted the company about contract cancellation but
were not contacted for advice or services proposal afterwards, and then contacted
again the customer services about contract cancellation (maybe to ï¬nalize their

â€“ ğ’‘ğ‘ª ğ‘¹ğ‘´
2

32

Thomas Guyet, RenÃ© Quiniou

contract cancellation procedure). This pattern identiï¬es customers undergoing a
potential failure in the customer relationship management: if a customer contacts
the customer services about contract cancellation, it seems quite normal to investi-
gate this customerâ€™s situation with a 3ğ‘†ğ¸ ğ‘… contact. This pattern shows that 5, 265
customers were not contacted back after asking for cancellation information.

â€“ ğ’‘ğ¶ ğ‘…ğ‘€
3

4

(and its similar pattern ğ’‘ğ¶ ğ‘…ğ‘€

) represents customers who subscribed to
a contract and ï¬nally canceled their contract without having been contacted for
advice. The maximum gap constraint that have been imposed by NegPSpan, ğœ = 4,
indicates that the customer relationship was faulty: for these customers there are
at most three interactions between their contract subscription and their contract
cancellation.

5

and ğ’‘ğ‘ª ğ‘¹ğ‘´
6

Patterns ğ’‘ğ‘ª ğ‘¹ğ‘´

involve absence of contract modiï¬cations (2ğ‘‰ ğ¼ ğ¸)
because advice or service oï¬€ers do not necessarily meet customersâ€™ needs. In these
cases, they do not modify their contract. A possible explanation of these two patterns
is that customers received some advice (3ğ‘†ğ¸ ğ‘…) but did not modify their contracts
(2ğ‘‰ ğ¼ ğ¸) and ï¬nally churned. For the customer services, these patterns can be used to
identify which advice or services have been oï¬€er to these particular customers and to
assess whether an alternative advice would not lead to a contract termination.

7

Pattern ğ’‘ğ‘ª ğ‘¹ğ‘´

is relatively frequent regarding its length. We interpret this pattern
as a failure in the ï¬le handling procedure. The customer subscribed to a contract, then
she/he had a contact to modify it. This modiï¬cation may not have been correctly taken
into consideration and the customer had another contact about her/his ï¬le handling.
Without eï¬€ective contract modiï¬cation after the last contact, the customer churned.

Let us now conclude about the three case studies. The ï¬rst case study is a dataset on
which both algorithms have diï¬ƒculties to extract meaningful NSPs. The case study on
medical care pathways highlights two important conclusions about the interpretation
of sequential patterns. First, NSPs provide meaningful information about sequential
patterns and may prevent from their misinterpretation. Second, the diï¬€erent semantics
adopted by the two algorithms lead to diï¬€erent sets of extracted pattern. Without
maxgap constraints, eNSP outputs some patterns whose frequency is over-estimated
compared to the frequency computed by NegPSpan. But, eNSP misses some possibly
important patterns at the boundary of the frequent NSPs. Finally, the third case study
illustrates some complex interpretations of NSPs that would be useful for further
analysis of customer relationship data and management procedures.

7 Related work

Kamepalli et al. (2014) and more recently Wang and Cao (2019) provide surveys
of the approaches proposed for mining negative patterns. The three most signiï¬cant
algorithms appear to be PNSP (Hsueh et al., 2008), NegGSP (Zheng et al., 2009) and
eNSP (Cao et al., 2016). We brieï¬‚y review each of them in the following paragraphs.
PNSP (Positive and Negative Sequential Patterns mining) (Hsueh et al., 2008) is
the ï¬rst algorithm proposed for mining full negative sequential patterns where nega-
tive itemsets are not only located at the end of the pattern. PNSP extends algorithm

NegPSpan: eï¬ƒcient extraction of negative sequential patterns

33

GSP (Srikant and Agrawal, 1996) to cope with mining negative sequential patterns.
PNSP consists of three steps: i) use algorithm GSP for mining frequent positive
sequential patterns, ii) preselect negative sequential itemsets â€” for PNSP, negative
itemsets must not be too infrequent (should have a support lower than a threshold
miss_freq) â€” iii) generate candidate negative sequences levelwise and scan the se-
quence dataset again to compute the support of these candidates and prune the search
when the candidate is infrequent. This algorithm is incomplete: the second parameter
reduces the set of potential negative itemsets. Moreover, the pruning strategy of PNSP
is not correct (Zheng et al., 2009) and PNSP misses potentially frequent negative
patterns.

Zheng et al. (2009) also proposed a negative version of algorithm GSP, called
NegGSP, to extract negative sequential patterns. They showed that traditional Apriori-
based negative pattern mining algorithms relying on support anti-monotonicity have
two main problems. The ï¬rst one is that the Apriori principle does not apply to negative
sequential patterns. They gave an example of sequence that is frequent even if one of
its sub-sequence is not frequent. The second problem has to do with the eï¬ƒciency
and the eï¬€ectiveness of ï¬nding frequent patterns due to a vast candidate space. Their
solution was to prune the search space using the support anti-monotonicity over
positive parts. This pruning strategy is correct but incomplete and it is not really
eï¬ƒcient considering the huge number of remaining candidates whose support has to
be evaluated. To improve the eï¬ƒciency of their approach, the authors proposed an
incomplete heuristic search based on Genetic Algorithm to ï¬nd negative sequential
patterns (Zheng et al., 2010).

eNSP (eï¬ƒcient NSP) has been recently proposed by Cao et al. (2016). It identiï¬es
NSPs by computing only frequent positive sequential patterns and deducing nega-
tive sequential patterns from positive patterns. Precisely, Cao et al. showed that the
support of some negative pattern can be computed by arithmetic operations on the
support of its positive sub-patterns, thus avoiding additional sequence dataset scans
to compute the support of negative patterns. However, this necessitates to store all the
(positive) sequential patterns with their set of covered sequences (tid-lists) which may
be impossible in case of big dense datasets and low minimal support thresholds. This
approach makes the algorithm more eï¬ƒcient but it hides some restrictive constraints
on the extracted patterns. First, a frequent negative pattern whose so-called positive
partner (the pattern where all negative events have been switched to positive) is not
frequent will not be extracted. Second, every occurrence of a negative pattern in a
sequence should satisfy absence constraints. We call this strong absence semantics
(see Section 3.2). These features lead eNSP to extract fewer patterns than previous
approaches. In some practical applications, eNSP may miss potentially interesting
negative patterns from the dataset.

The ï¬rst constraint has been partly tackled by Dong et al. with algorithm eNSPFI,
an extension of eNSP which mines NSPs from frequent and some infrequent positive
sequential patterns from the negative border (Gong et al., 2017). E-msNSP (Xu et al.,
2017b) is another extension of eNSP which uses multiple minimum supports: an NSP
is frequent if its support is greater than a local minimal support threshold computed
from the content of the pattern and not a global threshold as in classical approaches.
A threshold is associated with each item, and the minimal support of a pattern is

34

Thomas Guyet, RenÃ© Quiniou

deï¬ned from the most constrained item it contains. Such kind of adaptive support
prevents from extracting some useless patterns still keeping the pattern support anti-
monotonic. The same authors also proposed high utility negative sequential patterns
based on the same principles (Xu et al., 2017a) and applied the framework on smart
city data (Xu et al., 2018). An alternative approach has been proposed by Lin et al.
(2016) consisting in mining high-utility itemsets with negative unit proï¬ts but it
was not applied on sequential patterns. It is worth noting that this algorithm relies
basically on the same principle as eNSP and so, presents the same drawbacks, heavy
memory requirements, strong absence semantics for negation. F-NSP+ (Dong et al.,
2018b) extends the eNSP algorithm to use bitmap representations of itemsets. Using
bitmap representations enables to speed up algorithm eNSP, thanks to very eï¬ƒcient set
operations on bitmaps. Algorithm F-NSP has a poor memory usage, while F-NSP+,
which adapts the bitmap size to the dataset, requires slightly less memory.

e-RNSP (Dong et al., 2018a) extracts repetition NSP (RNSP). A RNSP is a NSP
for which the support takes into account the repetition of the pattern in a sequence
of the sequence dataset. Thus, it is not really a new pattern domain but more a new
deï¬nition of support measure. The e-RNSP uses the same strategy as eNSP with a hash
map to store all occurrences of the positive patterns (all repetition must be stored).
The experiments have been conducted on small datasets with a maximum number
of 10ğ‘˜ sequences (of at most 15 itemsets). Unsurprisingly, the computation times of
e-RNSP are slightly above e-RNSP. The memory consumption of the approach is not
discussed but it is at most as expensive as eNSP.

SAPNSP (Liu et al., 2015) tackles the problem of large amount of patterns by
selecting frequent negative and positive patterns that are actionable. Patterns are
actionable if they conform to special rules.

NegI-NSP (Qiu et al., 2017) proposes additional syntactic constraints on negative

itemsets and uses the same strategy as e-NSP.

To conclude this state of the art section, we provide in Table 8 a comparison of
several negative sequential pattern mining approaches wrt several features investigated
in section 3. It is also important to emphasize that one semantics is â€œmore correctâ€
than another one. Its relevance depends on the information the data scientists want
to capture in datasets, and the nature of the data at hand. In this work, one of our
objectives is to provide a sound and insightful framework for negative patterns to
enable users to choose the tool to use and to make this choice according to the
semantics of the negation they want to use. Execution time is obviously an important
choice criteria but it must be consistent with semantical choices to provide interesting,
intuitive and sound results.

8 Conclusion and perspectives

This article has investigated negative sequential pattern mining (NSP). It highlights
that state of the art algorithms do not extract the same patterns, not only depending on
their syntax and algorithm speciï¬cities, but also depending on the semantical choices
adopted for each of them. In this article, we have proposed deï¬nitions that clarify the

NegPSpan: eï¬ƒcient extraction of negative sequential patterns

35

Table 8 Comparison of negative pattern mining proposals. Optional constraints are speciï¬ed in Italic.
Question marks indicates that the article does not clearly state their semantic.

PNSP (Hsueh
et al., 2008)

NegGSP
(Zheng et al.,
2009)

eNSP (Cao
et al., 2016)

NegPSpan

Negative elements

Itemsets

Embeddings

Occurrences

Constraints on negative
itemsets

Global constraints on pat-
terns

itemsets

(cid:42)ğº?
strict

weak

not too in-
frequent
(ğ‘ ğ‘¢ ğ‘ ğ‘ (cid:54)
ğ‘™ğ‘’ğ‘ ğ‘ _ ğ‘“ ğ‘Ÿ ğ‘’ğ‘)

items?

(cid:42)ğº
strict?

weak

itemsets

(cid:42)ğ·
strict

strong

itemsets

(cid:42)ğ·
strict/soft
weak

frequent items

positive part-
ner is frequent

frequent items,
bounded size

positive part
is frequent
(second greater
threshold)

maxspan,
maxgap

negation semantics encountered in the literature. We have shown that the support of
NSP depends on the semantics of itemset non-inclusion, two possible alternatives for
considering negation of itemsets and two ways for considering multiple embeddings
in a sequence. So, we could point out the limits of the state of the art algorithm eNSP
that imposes a minimum support for positive partners and that is not able to deal with
embedding constraints, and more especially maxgap constraints.

We have proposed NegPSpan, a new algorithm for mining negative sequential
patterns that overcomes these limitations. The experiments show that NegPSpan is
more eï¬ƒcient than eNSP on datasets with medium-long sequences (more than 20
itemsets) even when weak maxgap constraints are applied and that it prevents from
missing possibly interesting patterns.

In addition, NegPSpan is based on well-founded theoretical principles that makes
possible to extend it to the extraction of closed or maximal patterns to reduce the
number of extracted patterns even more.

Acknowledgements The authors would like to thank REPERES Team from Rennes University Hospital
for spending time to discuss our case study results. We also would like to thanks M. Boumghar, L. Pierre and
D. Lagarde for raising interesting issues and providing the dataset about customer relationship management.
Finally, we would also like to thanks the reviewers for their insightful comments.

References

Bosc G, Boulicaut JF, RaÃ¯ssi C, Kaytoue M (2018) Anytime discovery of a diverse set
of patterns with monte carlo tree search. Data Mining and Knowledge Discovery
32(3):604â€“650

Cao L, Yu PS, Kumar V (2015) Nonoccurring behavior analytics: A new area. Intel-

ligent Systems 30(6):4â€“11

36

Thomas Guyet, RenÃ© Quiniou

Cao L, Dong X, Zheng Z (2016) e-NSP: Eï¬ƒcient negative sequential pattern mining.

Artiï¬cial Intelligence 235:156â€“182

Dauxais Y, Guyet T, Gross-Amblard D, Happe A (2017) Discriminant chronicles
mining - application to care pathways analytics. In: Proceedings of 16th Conference
on Artiï¬cial Intelligence in Medicine (AIME), pp 234â€“244

Dong X, Gong Y, Cao L (2018a) e-RNSP: An eï¬ƒcient method for mining repetition
negative sequential patterns. IEEE Transactions on Cybernetics pp 1â€“13, DOI
10.1109/TCYB.2018.2869907

Dong X, Gong Y, Cao L (2018b) F-NSP+: A fast negative sequential patterns mining

method with self-adaptive data storage. Pattern Recognition 84:13â€“27

Giannotti F, Nanni M, Pedreschi D (2006) Eï¬ƒcient mining of temporally annotated
sequences. In: Proceedings of the SIAM International Conference on Data Mining,
pp 348â€“359

Gong Y, Xu T, Dong X, Lv G (2017) e-NSPFI: Eï¬ƒcient mining negative sequential
pattern from both frequent and infrequent positive sequential patterns. International
Journal of Pattern Recognition and Artiï¬cial Intelligence 31(02):1750002

Han J, Pei J, Mortazavi-Asl B, Chen Q, Dayal U, Hsu MC (2000) FreeSpan: fre-
quent pattern-projected sequential pattern mining. In: Proceedings of the sixth
international conference on Knowledge discovery and data mining (SIGKDD), pp
355â€“359

Hsueh SC, Lin MY, Chen CL (2008) Mining negative sequential patterns for e-
commerce recommendations. In: Proceedings of Asia-Paciï¬c Services Computing
Conference, pp 1213â€“1218

Kamepalli S, Sekhara R, Kurra R (2014) Frequent negative sequential patterns â€“ a
survey. International Journal of Computer Engineering and Technology 5, 3:115â€“
121

Lin JCW, Fournier-Viger P, Gan W (2016) FHN: An eï¬ƒcient algorithm for min-
ing high-utility itemsets with negative unit proï¬ts. Knowledge-Based Systems
111:283â€“298

Liu C, Dong X, Li C, Li Y (2015) SAPNSP: Select actionable positive and negative
sequential patterns based on a contribution metric. In: Proceedings of the 12th
International Conference on Fuzzy Systems and Knowledge Discovery, pp 811â€“
815

Mallick B, Garg D, Grover PS (2013) CRM customer value based on constrained se-
quential pattern mining. International Journal of Computer Applications 64(9):21â€“
29

Mooney CH, Roddick JF (2013) Sequential pattern mining â€“ approaches and algo-

rithms. ACM Computing Survey 45(2):1â€“39

Moulis G, Lapeyre-Mestre M, Palmaro A, Pugnet G, Montastruc JL, Sailler L (2015)
French health insurance databases: What interest for medical research? La Revue
de MÃ©decine Interne 36:411â€“417

Negrevergne B, Guns T (2015) Constraint-based sequence mining using constraint
programming. In: Michel L (ed) Proceedings of the conference on Integration of AI
and OR Techniques in Constraint Programming (CPAIOR), Springer International
Publishing, pp 288â€“305

NegPSpan: eï¬ƒcient extraction of negative sequential patterns

37

Ngai E, Xiu L, Chau D (2009) Application of data mining techniques in customer
relationship management: A literature review and classiï¬cation. Expert Systems
with Applications 36(2, Part 2):2592 â€“ 2602

Pei J, Han J, Mortazavi-Asl B, Wang J, Pinto H, Chen Q, Dayal U, Hsu MC (2004)
Mining sequential patterns by pattern-growth: The Preï¬xSpan approach. IEEE
Transactions on knowledge and data engineering 16(11):1424â€“1440

Pei J, Han J, Wang W (2007) Constraint-based Sequential Pattern Mining: The Pattern-

growth Methods. Journal of Intelligent Information Systems 28(2):133â€“160

Polard E, Nowak E, Happe A, Biraben A, Oger E (2015) Brand name to generic
substitution of antiepileptic drugs does not lead to seizure-related hospitalization:
a population-based case-crossover study. Pharmacoepidemiology and drug safety
24:1161â€“1169

Qiu P, Zhao L, Dong X (2017) NegI-NSP: Negative sequential pattern mining based
on loose constraints. In: Proceedings of the 43rd Annual Conference of the IEEE
Industrial Electronics Society (IECON), pp 3419â€“3425

Srikant R, Agrawal R (1996) Mining sequential patterns: Generalizations and perfor-
mance improvements. In: Proceedings of the International Conference on Extending
Database Technology (EDBT), Springer, pp 1â€“17

Wang W, Cao L (2019) Negative sequences analysis: A review. ACM Computing

Survey 52

Xu T, Dong X, Xu J, Dong X (2017a) Mining high utility sequential patterns with
negative item values. International Journal of Pattern Recognition and Artiï¬cial
Intelligence 31(10):1750035

Xu T, Dong X, Xu J, Gong Y (2017b) E-msNSP: Eï¬ƒcient negative sequential patterns
mining based on multiple minimum supports. International Journal of Pattern
Recognition and Artiï¬cial Intelligence 31(02):1750003

Xu T, Li T, Dong X (2018) Eï¬ƒcient high utility negative sequential patterns mining

in smart campus. IEEE Access 6:23839â€“23847

Zaki MJ (2001) SPADE: An Eï¬ƒcient Algorithm for Mining Frequent Sequences.

Machine Learning 42(1/2):31â€“60

Zheng Z, Zhao Y, Zuo Z, Cao L (2009) Negative-GSP: An eï¬ƒcient method for min-
ing negative sequential patterns. In: Proceedings of the Australasian Data Mining
Conference, pp 63â€“67

Zheng Z, Zhao Y, Zuo Z, Cao L (2010) An eï¬ƒcient GA-based algorithm for mining
negative sequential patterns. In: Proceedings of the Paciï¬c-Asia Conference on
Knowledge Discovery and Data Mining (PAKDD), Springer, pp 262â€“273

38

A Proofs

Thomas Guyet, RenÃ© Quiniou

Proof (Proof of Proposition 2) Let ğ’” = (cid:104)ğ‘ 1 Â· Â· Â· ğ‘ ğ‘› (cid:105) be a sequence and ğ’‘ = (cid:104) ğ‘1 Â· Â· Â· ğ‘ğ‘š (cid:105) be a
negative sequential pattern. Let ğ’† = (ğ‘’ğ‘–)ğ‘–âˆˆ [ğ‘š] âˆˆ [ğ‘›]ğ‘š be a soft-embedding of pattern ğ’‘ in sequence
ğ’”. Then, the deï¬nition matches the one for strict-embedding if ğ‘ğ‘– is positive. If ğ‘ğ‘– is negative then
âˆ€ ğ‘— âˆˆ [ğ‘’ğ‘–âˆ’1 + 1, ğ‘’ğ‘–+1 âˆ’ 1], ğ‘ğ‘– (cid:42)ğ· ğ‘  ğ‘— , i.e. âˆ€ ğ‘— âˆˆ [ğ‘’ğ‘–âˆ’1 + 1, ğ‘’ğ‘–+1 âˆ’ 1], âˆ€ğ›¼ âˆˆ ğ‘ğ‘– , ğ›¼ âˆ‰ ğ‘  ğ‘— and then
âˆ€ğ›¼ âˆˆ ğ‘ğ‘– , âˆ€ ğ‘— âˆˆ [ğ‘’ğ‘–âˆ’1 + 1, ğ‘’ğ‘–+1 âˆ’ 1], ğ›¼ âˆ‰ ğ‘  ğ‘— . Thus, it implies that âˆ€ğ›¼ âˆˆ ğ‘ğ‘– , ğ›¼ âˆ‰ (cid:208) ğ‘—âˆˆ [ğ‘’ğ‘–âˆ’1+1,ğ‘’ğ‘–+1âˆ’1] ğ‘  ğ‘— ,
i.e. by deï¬nition, ğ‘ğ‘– (cid:42)ğ· (cid:208) ğ‘—âˆˆ [ğ‘’ğ‘–âˆ’1+1,ğ‘’ğ‘–+1âˆ’1] ğ‘  ğ‘— .

The exact same reasoning is done in the reverse way to prove the equivalence.

Proof (Proof of Proposition 3 (Anti-monotonicity of NSP)) Let ğ’‘ = (cid:104) ğ‘1 Â¬ğ‘1 ğ‘2 Â¬ğ‘2 Â· Â· Â· ğ‘ğ‘˜âˆ’1
ğ‘˜(cid:48) (cid:105) be two NSP s.t. ğ’‘ (cid:67) ğ’‘(cid:48). By deï¬ni-
ğ‘(cid:48)
Â¬ğ‘ğ‘˜âˆ’1 ğ‘ğ‘˜ (cid:105) and ğ’‘(cid:48) = (cid:104) ğ‘(cid:48)
ğ‘˜(cid:48)âˆ’1
tion we have that ğ‘˜ â‰¤ ğ‘˜(cid:48).

2 Â· Â· Â· ğ‘(cid:48)

2 Â¬ğ‘(cid:48)
ğ‘(cid:48)

1 Â¬ğ‘(cid:48)
1

Â¬ğ‘(cid:48)

ğ‘˜(cid:48)âˆ’1

To prove the anti-monotonicity, we prove that any embedding (ğ‘’ğ‘–)ğ‘–âˆˆ [ğ‘˜ ] of ğ’‘(cid:48) in a sequence ğ’” generates

an embedding of ğ’‘ in ğ’”.

Let ğ’” = (cid:104)ğ‘ 1 Â· Â· Â· ğ‘ ğ‘› (cid:105) be a sequence s.t. ğ’‘(cid:48) (cid:22) ğ’”, i.e. there exists an embedding (ğ‘’ğ‘–)ğ‘–âˆˆ [ğ‘˜(cid:48) ] :

â€“ âˆ€ğ‘–, ğ‘’ğ‘–+1 > ğ‘’ğ‘–,
â€“ âˆ€ğ‘–, ğ‘(cid:48)
ğ‘– âŠ† ğ‘ ğ‘’ğ‘– ,
â€“ âˆ€ ğ‘— âˆˆ [ğ‘’ğ‘– + 1, ğ‘’ğ‘–+1 âˆ’ 1], ğ‘(cid:48)

ğ‘– (cid:42)ğ· ğ‘ ğ‘’ ğ‘—
By deï¬nitions of (cid:67) and embedding,

(i) âˆ€ğ‘– âˆˆ [ğ‘˜ ], ğ‘ğ‘– âŠ† ğ‘(cid:48)
(ii) âˆ€ğ‘– âˆˆ [ğ‘˜ âˆ’ 1], âˆ€ ğ‘— âˆˆ [ğ‘’ğ‘– + 1, ğ‘’ğ‘–+1 âˆ’ 1], ğ‘(cid:48)

ğ‘– âŠ† ğ‘ ğ‘’ğ‘– , (ğ‘’ğ‘– exists because ğ‘˜ â‰¤ ğ‘˜(cid:48))

anti-monotone and ğ‘ğ‘– âŠ† ğ‘(cid:48)ğ‘–)

This means that (ğ‘’ğ‘–)ğ‘–âˆˆ [ğ‘˜ ] is an embedding of ğ’‘ in ğ’”.

ğ‘— (cid:42)ğ· ğ‘ ğ‘’ğ‘– , and thus ğ‘ ğ‘— (cid:42)ğ· ğ‘ ğ‘’ğ‘– (because (cid:42)ğ· is

Proof (Proof of Proposition 4 (Anti-monotonicity of the support of Constrained NSP)) The proof of this
property is similar to the proof of Proposition 3.

Let ğ’‘ = (cid:104) ğ‘1 Â¬ğ‘1 ğ‘2 Â¬ğ‘2 Â· Â· Â· ğ‘ğ‘˜âˆ’1 Â¬ğ‘ğ‘˜âˆ’1 ğ‘ğ‘˜ (cid:105) and ğ’‘(cid:48) = (cid:104) ğ‘(cid:48)

ğ‘(cid:48)
ğ‘˜(cid:48) (cid:105)
be two NSP s.t. ğ’‘ (cid:67) ğ’‘(cid:48). And let ğ’” = (cid:104)ğ‘ 1 Â· Â· Â· ğ‘ ğ‘› (cid:105) be a sequence s.t. ğ’‘(cid:48) (cid:22) ğ’”, i.e. there exists an embedding
(ğ‘’ğ‘–)ğ‘–âˆˆ [ğ‘˜(cid:48) ] :

2 Â· Â· Â· ğ‘(cid:48)

ğ‘(cid:48)
2 Â¬ğ‘(cid:48)

1 Â¬ğ‘(cid:48)
1

Â¬ğ‘(cid:48)

ğ‘˜(cid:48)âˆ’1

ğ‘˜(cid:48)âˆ’1

â€“ âˆ€ğ‘–, ğ‘’ğ‘–+1 > ğ‘’ğ‘– (embedding), ğ‘’ğ‘–+1 âˆ’ ğ‘’ğ‘– â‰¤ ğœƒ (maxgap) and ğ‘’ğ‘˜(cid:48) âˆ’ ğ‘’1 â‰¤ ğœ (maxspan),
â€“ âˆ€ğ‘–, ğ‘(cid:48)
â€“ âˆ€ ğ‘— âˆˆ [ğ‘’ğ‘– + 1, ğ‘’ğ‘–+1 âˆ’ 1], ğ‘(cid:48)

ğ‘– âŠ† ğ‘ ğ‘’ğ‘– ,

ğ‘– (cid:42)ğ· ğ‘ ğ‘’ ğ‘—

To prove that ğ’‘ (cid:22) ğ’”, we prove that (ğ‘’ğ‘–)ğ‘–âˆˆ [ğ‘˜ ] is an embedding of ğ’‘ in ğ’”.
Let us ï¬rst consider that ğ‘˜ = ğ‘˜(cid:48), then by deï¬nitions of (cid:67) and the embedding,

(i) âˆ€ğ‘– âˆˆ [ğ‘˜ ], ğ‘ğ‘– âŠ† ğ‘(cid:48)
ğ‘– âŠ† ğ‘ ğ‘’ğ‘– ,
(ii) âˆ€ğ‘– âˆˆ [ğ‘˜ âˆ’ 1], âˆ€ ğ‘— âˆˆ [ğ‘’ğ‘– + 1, ğ‘’ğ‘’+1 âˆ’ 1], ğ‘(cid:48)

ğ‘— (cid:42)ğ· ğ‘ ğ‘’ğ‘– , and thus ğ‘ ğ‘— (cid:42)ğ· ğ‘ ğ‘’ğ‘– (because of anti-

monotonicity of (cid:42)ğ· and ğ‘ğ‘– âŠ† ğ‘(cid:48)ğ‘–)
In addition, we know that maxgap and maxspan constraints are satisï¬ed by the embedding, i.e.

(iv) âˆ€ğ‘– âˆˆ [ğ‘˜ ], ğ‘’ğ‘–+1 âˆ’ ğ‘’ğ‘– â‰¤ ğœƒ
(v) ğ‘’ğ‘˜ âˆ’ ğ‘’1 = ğ‘’ğ‘˜(cid:48) âˆ’ ğ‘’1 â‰¤ ğœ

Let us now consider that ğ‘˜(cid:48) > ğ‘˜, (ğ‘–), (ğ‘–ğ‘–) and (ğ‘–ğ‘–ğ‘–) still holds, and we know that if ğ‘’ğ‘˜ < ğ‘’ğ‘˜(cid:48)

(embedding property), then ğ‘’ğ‘˜ âˆ’ ğ‘’ğ‘– < ğœƒ.

This means that (ğ‘’ğ‘–)ğ‘–âˆˆ [ğ‘˜ ] is an embedding of ğ’‘ in ğ’” that satisï¬es gap constraints.

Proof (Proof of Proposition 5 (Complete and correct algorithm)) The correction of the algorithm is given
by lines 2-3 of Algorithm 1. A pattern is outputted only if it is frequent (line 2).

We now prove the completeness of the algorithm. First of all, we have to prove that any pattern can
be reached using a path of elementary transformations ((cid:32)âˆˆ {(cid:32)ğ‘›, (cid:32)ğ‘  , (cid:32)ğ‘ }). Let ğ’‘(cid:48) = (cid:104) ğ‘(cid:48)
ğ‘š (cid:105) be
a pattern with a total amount of ğ‘› items, ğ‘› > 0, then it is possible to deï¬ne ğ’‘ such that ğ’‘ (cid:32) ğ’‘(cid:48) where
(cid:32)âˆˆ {(cid:32)ğ‘›, (cid:32)ğ‘  , (cid:32)ğ‘ }, and ğ’‘ will have exactly ğ‘› âˆ’ 1 items:

1 Â· Â· Â· ğ‘(cid:48)

â€“ if the last itemset of ğ’‘(cid:48) is such that | ğ‘(cid:48)

1 Â· Â· Â· ğ‘(cid:48)
the same preï¬x as ğ’‘(cid:48) and an additional itemset, ğ‘ğ‘š such that | ğ‘ğ‘š | = | ğ‘(cid:48)
ğ’‘ (cid:32)ğ‘ ğ’‘(cid:48)

ğ‘š | > 1 we deï¬ne ğ’‘ = (cid:104) ğ‘(cid:48)

ğ‘šâˆ’1
ğ‘š | âˆ’ 1 and ğ‘ğ‘š âŠ‚ ğ‘(cid:48)

ğ‘ğ‘š (cid:105) as the pattern with
ğ‘š: then

NegPSpan: eï¬ƒcient extraction of negative sequential patterns

39

â€“ if the last itemset of ğ’‘(cid:48) is such that | ğ‘(cid:48)

ğ‘š | = 1 and ğ‘(cid:48)

ğ‘šâˆ’1

is positive then we deï¬ne ğ’‘ = (cid:104) ğ‘(cid:48)

1 Â· Â· Â· ğ‘(cid:48)

ğ‘šâˆ’2

â€“ if the last itemset of ğ’‘(cid:48) is such that | ğ‘(cid:48)

ğ‘š | = 1 and ğ‘(cid:48)

1 Â· Â· Â· ğ‘ğ‘šâˆ’1 ğ‘(cid:48)

ğ‘š (cid:105) where ğ‘ğ‘šâˆ’1 is such that | ğ‘ğ‘šâˆ’1 | = | ğ‘(cid:48)

ğ‘šâˆ’1

is negative (non-empty) then we deï¬ne
: then

ğ‘šâˆ’1 | âˆ’ 1 and ğ‘ğ‘šâˆ’1 âŠ‚ ğ‘(cid:48)

ğ‘šâˆ’1

ğ‘šâˆ’1 (cid:105): then ğ’‘ (cid:32)ğ‘  ğ’‘(cid:48)
ğ‘(cid:48)

ğ’‘ = (cid:104) ğ‘(cid:48)
ğ’‘ (cid:32)ğ‘› ğ’‘(cid:48)

Applying this rules recursively, for any pattern ğ’‘ there is a path from the empty sequence to ğ’‘: âˆ… (cid:32)âˆ— ğ’‘.
Also, exactly one of the three extensions can be used at any step, meaning that these path is unique. This
prove that our algorithm is not redundant.

Second, the pruning strategy is correct, so, no frequent pattern will be missed. This comes from the

anti-monotonicity property.

Let ğ’‘ and ğ’‘(cid:48) be two patterns such that ğ’‘ (cid:32) ğ’‘(cid:48) where (cid:32)âˆˆ {(cid:32)ğ‘›, (cid:32)ğ‘  , (cid:32)ğ‘ }, then is is quite obvious
that ğ’‘ (cid:67) ğ’‘(cid:48). Let us now consider that ğ’‘ (cid:32)âˆ— ğ’‘(cid:48) from ğ’‘ to ğ’‘(cid:48) then, by transitivity of (cid:67), we also have that
ğ’‘ (cid:67) ğ’‘(cid:48). And then by anti-monotonicity of the support, we have that ğ‘ ğ‘¢ ğ‘ ğ‘ ( ğ’‘) â‰¥ ğ‘ ğ‘¢ ğ‘ ğ‘ ( ğ’‘(cid:48)).

Let us now proceed by absurd and consider that ğ’‘(cid:48) is a pattern with support ğ‘ ğ‘¢ ğ‘ ğ‘ ( ğ’‘(cid:48)) â‰¥ ğœ that was
not found by the algorithm. This means that for all paths7 âˆ… (cid:32)âˆ— ğ’‘(cid:48) there exists some pattern ğ’‘ such that
âˆ… (cid:32)âˆ— ğ’‘ (cid:32)âˆ— ğ’‘(cid:48) with ğ‘ ğ‘¢ ğ‘ ğ‘ ( ğ’‘) < ğœ. ğ’‘ is the pattern that has been used to prune the search for this path
to ğ’‘(cid:48). This is not possible considering that ğ’‘ (cid:32)âˆ— ğ’‘(cid:48) and thus ğ‘ ğ‘¢ ğ‘ ğ‘ ( ğ’‘) â‰¥ ğ‘ ğ‘¢ ğ‘ ğ‘ ( ğ’‘(cid:48)) â‰¥ ğœ.

B NegPSpan extracts a superset of eNSP

Proposition 6 Soft-embedding =â‡’ strict-embedding for patterns consisting of items.

Proof Let ğ’” = (cid:104)ğ‘ 1 Â· Â· Â· ğ‘ ğ‘› (cid:105) be a sequence and ğ’‘ = (cid:104) ğ‘1 Â· Â· Â· ğ‘ğ‘š (cid:105) be a NSP s.t. | ğ‘ğ‘– | = 1 for all ğ‘– âˆˆ [ğ‘›] and
ğ’‘ occurs in ğ’” according to the soft-embedding semantic.

There exists ğœ– = (ğ‘’ğ‘–)ğ‘–âˆˆ [ğ‘š] âˆˆ [ğ‘›]ğ‘š s.t. for all ğ‘– âˆˆ [ğ‘›], ğ‘ğ‘– is positive implies ğ‘ğ‘– âˆˆ ğ‘ ğ‘’ğ‘– and ğ‘ğ‘– is
negative implies that for all ğ‘— âˆˆ [ğ‘’ğ‘–âˆ’1 +1, ğ‘’ğ‘–+1 âˆ’1], ğ‘ğ‘– âˆ‰ ğ‘  ğ‘— (items only) then ğ‘ğ‘– âˆ‰ (cid:208) ğ‘—âˆˆ [ğ‘’ğ‘–âˆ’1+1,ğ‘’ğ‘–+1âˆ’1] ğ‘  ğ‘—
i.e. ğ‘ğ‘– (cid:42)âˆ— (cid:208) ğ‘—âˆˆ [ğ‘’ğ‘–âˆ’1+1,ğ‘’ğ‘–+1âˆ’1] ğ‘  ğ‘— (whatever (cid:42)ğº or (cid:42)ğ·). As a consequence ğœ– is a strict-embedding of ğ‘.
Proposition 7 Let D be a dataset containing sequences made of items and ğ’‘ = (cid:104) ğ‘1 Â· Â· Â· ğ‘ğ‘š (cid:105) be a
sequential pattern extracted by eNSP. Then, without embedding constraints ğ’‘ is extracted by NegPSpan
with the same minimum support.

Proof If ğ’‘ is extracted by eNSP, then its positive partner is frequent in the dataset D. As a consequence,
each ğ‘ğ‘–, ğ‘– âˆˆ [ğ‘š] is a singleton itemset.

According to the search space of NegPSpan deï¬ned by (cid:67) if ğ’‘ is frequent then it will be reached
by the depth-ï¬rst search. Then it is suï¬ƒcient to prove that for any sequence ğ’” = (cid:104)ğ‘ 1 Â· Â· Â· ğ‘ ğ‘› (cid:105) âˆˆ D such
that ğ’‘ occurs in ğ’” according to eNSP semantics (strict-embedding, strong absence), then ğ’‘ also occurs in
ğ’” according to the NegPSpan semantics (soft-embedding, weak absence). Consequently, considering the
same minimum support threshold, ğ’‘ is frequent according to NegPSpan. Proposition 6 gives this result.

Then we conclude that NegPSpan extracts more patterns than eNSP on sequences of items. In fact,

NegPSpan can extract patterns with negative itemsets larger than 2.

eNSP extracts patterns that are not extracted by NegPSpan on sequences of itemsets. Practically,
NegPSpan uses a size limit for negative itemsets ğœˆ â‰¥ 1. eNSP extracts patterns whose positive partners are
frequent. The positive partner, extracted by Preï¬xSpan may hold itemsets larger than ğœˆ, and if the pattern
with negated itemset is also frequent, then this pattern is extracted by eNSP, but not by NegPSpan.

C Additional experiments

C.1 Inï¬‚uence of vocabulary size

Figure 6 shows computation time and memory consumption with respect to vocabulary size: eNSP is run
with diï¬€erent values of ğœ , the minimal frequency of the positive partner of negative patterns (100%, 80%

7 Note that we proved that this path is actually unique.

40

Thomas Guyet, RenÃ© Quiniou

Fig. 6 Comparison of eNSP and NegPSpan computation time (left) and memory consumption (right) wrt
vocabulary size. The dashed line shows the limit for timeout executions.

and 20% of the minimal frequency threshold) and NegPSpan is run with a maxgap of 10 or without. The
timeout is set to 5 minutes.

Similarly to experiments of the previous section, with no constraint, NegPSpan is less time-eï¬ƒcient
than eNSP but it becomes more time-eï¬ƒcient with a gap constraint whatever the vocabulary size. Memory
consumption curves show that NegPSpan requires signiï¬cantly less memory than eNSP.

Figure 6 clearly shows that the smaller the vocabulary is, the more frequent patterns there are, and
thus the more memory is required and time is high. Indeed, the smaller vocabulary (with a given sequence
length), the higher the probability to extract some sequential pattern. This is the case for positive patterns
as well as for negative patterns considering that the positive part of the negative pattern may more likely
occur in a raw (and may necessarily satisfy the negative constraints). Then, generated datasets have more
positive and negative patterns to extract.

More especially, there are more positive patterns to extract and thus the memory required by eNSP
increases because it requires to store all positive patterns (with a support above ğœ ). We can see that when the
vocabulary size decreases, the memory required by eNSP increases very quickly (faster than exponential
growth), while NegPSpan requires almost the same amount of memory for any vocabulary size.

The use of a maxgap constraint (ğœ = 10) makes NegPSpan several orders of magnitude more time-
eï¬ƒcient for small vocabulary size. With very small vocabulary size (< 20), the number of negative patterns
extracted by NegPSpan explodes and the execution time exceeds the timeout of 5 minutes (300ğ‘ ). For
greater vocabulary size, the diï¬€erences between algorithms disappear. NegPSpan (ğœ = âˆ) is more eï¬ƒcient
than eNSP for big vocabulary size because only few frequent negative patterns should be extracted, but there
are many positive patterns: in this case eNSP has to evaluate the support of potential negative sequential
patterns on the basis of the positive patterns while NegPSpan stops the exploration as soon as an unfrequent
negative pattern is found. Thus, eNSP with ğœ = 0.8ğœ, which explores more positive patterns than eNSP
with ğœ = ğœ, is less time-eï¬ƒcient.

C.2 Inï¬‚uence of average sequence length

Figure 7 shows the computation time and memory consumption with respect to average length of sequences
with a minimal support ğœ = 20%. eNSP is run with diï¬€erent values for ğœ , the minimal frequency of the
positive partner of negative patterns (100%, 80% and 20% of the minimal frequency threshold) and
NegPSpan is run with a maxgap ğœ = 10 or without maxgap constraint. The timeout is set to 5 ğ‘šğ‘–ğ‘›.

Computation times and memory consumptions are exponential with respect to the average sequence
length. Curves diï¬€er by their factors of exponential growth. In the remainder of this section ğ›¼ represents
the exponential growth.

Figure 7 on the left compares the computation times. The exponential growth of NegPSpan without
maxgap (ğ›¼ â‰ˆ 10âˆ’11) is high and the timeout is reached for datasets with an average sequence length of about
30 itemsets. eNSP is one order of magnitude more time-eï¬ƒcient and can analyze dataset with an average
sequence length about 45 itemsets. But, parameter ğœ does not changes the computation time signiï¬cantly.
Indeed, the exponential growths are close to each other (ğ›¼ğœ =ğœ â‰ˆ 5.08 Ã— 10âˆ’12, ğ›¼ğœ =0.8ğœ â‰ˆ 2.42 Ã— 10âˆ’12,
ğ›¼ğœ =0.5ğœ â‰ˆ 2.72 Ã— 10âˆ’12). In contrast, the use of the maxgap constraint (ğœ = 10) changes signiï¬cantly

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll11004080120160Vocabulary sizeTime (s)â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—1e+041e+054080120160Vocabulary sizeMemory (bytes)lNegPSpan ( t = Â¥ )NegPSpan ( t =10 )eNSP ( V = 0.8 s )eNSP ( V = s )NegPSpan: eï¬ƒcient extraction of negative sequential patterns

41

Fig. 7 Comparison of computing time (left) and memory consumption (right) between eNSP and Neg-
PSpan wrt average length of sequences. The dashed line shows the limit for timeout executions.

the diï¬ƒculty of the task: the exponential growth is signiï¬cantly lower (ğ›¼ â‰ˆ 10âˆ’53) and the timeout is
not reached even for sequence containing about 120 itemsets. This result was expected considering that
the maxgap constraint avoids to explore the full sequence to evaluate the support of the pattern. Using
NegPSpan with maxgap constraints is thus very time-eï¬ƒcient to mine negative patterns in long sequences.
Figure 7 on the right compares the memory consumption of the two algorithms. The curves look very
similar to the computation time curves, but it is important to note that the memory consumption requirement
increases signiï¬cantly slower with NegPSpan than with eNSP, whatever the use of maxgap constraint. The
explanation is similar to previous benchmark results: the depth-ï¬rst search strategy does not store patterns
while eNSP does to evaluate the support of negative patterns.

C.3 Computational performances on case study datasets

This appendix presents the computational performances of eNSP and NegPSpan on two datasets of the
case studies (see Section 6): instacart and care pathway analysis.

C.3.1 Instacart data

Figure 8 shows the computation times, the memory requirements and the number of NSPs extracted by
both algorithms on the Instacart dataset (see Section 6.1). On this dataset, we can ï¬rst note that the
number of patterns extracted by NegPSpan is about two orders of magnitude larger. As a consequence, the
computation time is higher even with strong gap constraints: NegPSpan takes about 1, 000ğ‘  with ğœ = 2
while eNSP takes always less than 500ğ‘  to extract 1% NSPs (whatever ğœ ). These results can be explained
by the dataset features. With a large vocabulary the support of patterns decreases rapidly when the pattern
length increases. This means that eNSP prunes a lot of patterns while extracting the positive partners. This
explains that few patterns are extracted. In contrast with this fast pruning strategy, NegPSpan explores lots
of potential negative extensions because most of them could be frequent due to the relative low frequency
of each item. eNSP seems more eï¬ƒcient on this dataset, but we recall that it failed to explore large datasets,
not for time reason, but for heavy memory requirements. Figure 8 middle, shows this limitation well: the
memory requirement is several orders larger for eNSP than for NegPSpan and it increases exponentially
when ğœ decreases.

C.3.2 Care pathway analysis

Figure 9 gives a comparison of the computation performances (time and memory usage) between eNSP and
NegPSpan with respect to the minimal frequency threshold (ğœ âˆˆ [0.08, 0.3]) on the care pathway dataset
(see Section 6.2). Each algorithm is run with diï¬€erent settings: the maxgap constraint ğœ âˆˆ {3, 5, 8} for
NegPSpan and the minimal support of positive partners ğœ âˆˆ {0.4ğœ, 0.8ğœ, ğœ } for eNSP. The maximal
pattern length is set to ğ‘™ = 5. The results obtained on real data conï¬rm the results obtained in Section 5 on

lllllllllllllllllllllllllllllllllllllll1100255075100125Sequence lengthTime (s)â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—1e+041e+051e+06255075100125Sequence lengthMemory (bytes)lNegPSpan ( t = Â¥ )NegPSpan ( t =10 )eNSP ( V =0.5 s )eNSP ( V =0.8 s )eNSP ( V = s )42

Thomas Guyet, RenÃ© Quiniou

Fig. 8 Computation times (on top), memory requirements (in the middle) and numbers of NSP (at the
bottom) extracted from the Instacart dataset with respect to the gap constraint ğœ for NegPSpan (on the left)
and with respect to frequent positive ratio ( ğœ ) for eNSP (on the right).

Fig. 9 Time, memory usage and number of extracted patterns for NSP and NegPSpan with respect to the
minimal frequency support, and diï¬€erent algorithm settings. (care pathways dataset, see Section 6.2)

synthetic data. On the one hand, NegPSpan requires several orders of magnitude less memory than eNSP.
eNSP does not terminate with lowest ğœ and ğœ values. Its memory requirement exceeds the computer
capacity (8Go). On the other hand, this heavy memory requirement has consequences on computation
times and NegPSpan is several orders of magnitude more time-eï¬ƒcient than eNSP whatever the setting.
We observe that the computation time increases exponentially when the frequency threshold (ğœ) decreases,
and, the lower maxgap, the lower the computation time. This is mainly due to the number of extracted
patterns that grows also exponentially when the frequency threshold decreases.

llll1e+031e+041e+052468tTime (s)llll10030010000.40.50.60.7VTime (s)â—â—â—â—200002468Ï„Memory (bytes)â—â—â—â—1e+053e+051e+060.40.50.60.7Ï‚Memory (bytes)llll1e+031e+041e+051e+061e+072468t#Patternsllll3e+031e+043e+041e+050.40.50.60.7V#Patternsls =0.5%s =1%lllll1010010000.150.200.250.30Frequency ThresholdTime (s)lllll1e+051e+061e+070.150.200.250.30Frequency ThresholdMemory (bytes)NegPSpaneNSPlt =3t =5t =8V = 0.4 sV = 0.8 sV = sNegPSpan: eï¬ƒcient extraction of negative sequential patterns

43

Table 9 NSP extracted by NegPSpan but not by eNSP

Pattern

support
NegPSpan

(cid:104)6ğ·ğ¼ ğ‘† Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)(1ğ¶ğ‘‡ ğ‘…, 3ğ‘†ğ¸ ğ‘…) Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)(1ğ¶ğ‘‡ ğ‘…, 2ğ‘‰ ğ¼ ğ¸ , 8ğ·ğ‘‚ğ‘†) Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)1ğ‘…ğ¸ğ‘† Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)(1ğ¶ğ‘‡ ğ‘…, 3ğ‘†ğ¸ ğ‘…, 8ğ·ğ‘‚ğ‘†) Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)7ğ´ğ¶ğ¶ Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)(3ğ‘†ğ¸ ğ‘…, 8ğ·ğ‘‚ğ‘†) Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)(2ğ‘‰ ğ¼ ğ¸ , 3ğ‘†ğ¸ ğ‘…) Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)(2ğ‘‰ ğ¼ ğ¸ , 8ğ·ğ‘‚ğ‘†) Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)(1ğ¶ğ‘‡ ğ‘…, 2ğ‘‰ ğ¼ ğ¸) Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)4ğ¹ ğ´ğ¶ 4ğ¹ ğ´ğ¶ Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)(1ğ¶ğ‘‡ ğ‘…, 8ğ·ğ‘‚ğ‘†) Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)2ğ‘‰ ğ¼ ğ¸ 8ğ·ğ‘‚ğ‘† Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)(2ğ‘‰ ğ¼ ğ¸ , 8ğ·ğ‘‚ğ‘†) Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)(1ğ¶ğ‘‡ ğ‘…, 3ğ‘†ğ¸ ğ‘…, 8ğ·ğ‘‚ğ‘†) Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)(1ğ¶ğ‘‡ ğ‘…, 3ğ‘†ğ¸ ğ‘…), Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)1ğ¶ğ‘‡ ğ‘… 2ğ‘‰ ğ¼ ğ¸ 8ğ·ğ‘‚ğ‘† Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)7ğ´ğ¶ğ¶ Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)(3ğ‘†ğ¸ ğ‘…, 8ğ·ğ‘‚ğ‘†) Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)(1ğ¶ğ‘‡ ğ‘…, 2ğ‘‰ ğ¼ ğ¸) Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)

5,352
2,268
1,946
5,265
1,893
1,947
3,060
2,058
3,106
2,294
2,123
6,597
2,048
3,029
1,900
2,258
1,912
1,901
3,032
2,266

D List of extracted patterns for the CRM dataset

This Appendix provides the complete list of negative sequential patterns involving 3SER or 2VIE negative
items extracted by eNSP or NegPSpan.

44

Thomas Guyet, RenÃ© Quiniou

Table 10 NSP extracted by eNSP but not by NegPSpan

Pattern

(cid:104)8ğ·ğ‘‚ğ‘† Â¬(2ğ‘‰ ğ¼ ğ¸ , 3ğ‘†ğ¸ ğ‘…) 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)5ğ‘…ğ¸ğ¶ Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ ğ‘† (cid:105)
(cid:104)5ğ‘…ğ¸ğ¶ 5ğ‘…ğ¸ğ¶ Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)2ğ‘‰ ğ¼ ğ¸ Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)2ğ‘‰ ğ¼ ğ¸(cid:48) (cid:48)3ğ‘†ğ¸ ğ‘… Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)8ğ·ğ‘‚ğ‘† 5ğ‘…ğ¸ğ¶ Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)4ğ¹ ğ´ğ¶ Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)3ğ‘†ğ¸ ğ‘… Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)2ğ‘‰ ğ¼ ğ¸ 2ğ‘‰ ğ¼ ğ¸ Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)8ğ·ğ‘‚ğ‘† Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ ğ‘† (cid:105)
(cid:104)2ğ‘‰ ğ¼ ğ¸ 8ğ·ğ‘‚ğ‘† Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)8ğ·ğ‘‚ğ‘† 8ğ·ğ‘‚ğ‘† Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)4ğ¹ ğ´ğ¶ 4ğ¹ ğ´ğ¶ Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)8ğ·ğ‘‚ğ‘† Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)1ğ¶ğ‘‡ ğ‘… Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)8ğ·ğ‘‚ğ‘† 2ğ‘‰ ğ¼ ğ¸ Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)2ğ‘‰ ğ¼ ğ¸ Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)5ğ‘…ğ¸ğ¶ 5ğ‘…ğ¸ğ¶ Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)4ğ¹ ğ´ğ¶ Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ ğ‘† (cid:105)
(cid:104)5ğ‘…ğ¸ğ¶ Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)6ğ·ğ¼ ğ‘† Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)1ğ‘…ğ¸ğ‘† Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)8ğ·ğ‘‚ğ‘† 8ğ·ğ‘‚ğ‘† Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)1ğ¶ğ‘‡ ğ‘… Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ ğ‘† (cid:105)
(cid:104)3ğ‘†ğ¸ ğ‘… Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)(1ğ¶ğ‘‡ ğ‘…, 8ğ·ğ‘‚ğ‘†) Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ ğ‘† (cid:105)

Table 11 NSP extracted by both eNSP and NegPSpan

Pattern

(cid:104)3ğ‘†ğ¸ ğ‘… Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)2ğ‘‰ ğ¼ ğ¸ Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)1ğ‘…ğ¸ğ‘† Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)8ğ·ğ‘‚ğ‘† Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)8ğ·ğ‘‚ğ‘† 8ğ·ğ‘‚ğ‘† Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)5ğ‘…ğ¸ğ¶ Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)2ğ‘‰ ğ¼ ğ¸ 8ğ·ğ‘‚ğ‘† Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)4ğ¹ ğ´ğ¶ Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ ğ‘† (cid:105)
(cid:104)2ğ‘‰ ğ¼ ğ¸ Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)8ğ·ğ‘‚ğ‘† 8ğ·ğ‘‚ğ‘† Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)(2ğ‘‰ ğ¼ ğ¸ , 3ğ‘†ğ¸ ğ‘…) Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)4ğ¹ ğ´ğ¶ 4ğ¹ ğ´ğ¶ Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)4ğ¹ ğ´ğ¶ Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)1ğ¶ğ‘‡ ğ‘… Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)5ğ‘…ğ¸ğ¶ Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)6ğ·ğ¼ ğ‘† Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)8ğ·ğ‘‚ğ‘† Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)3ğ‘†ğ¸ ğ‘… Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)(1ğ¶ğ‘‡ ğ‘…, 8ğ·ğ‘‚ğ‘†) Â¬2ğ‘‰ ğ¼ ğ¸ 1ğ‘…ğ¸ğ‘† (cid:105)
(cid:104)1ğ¶ğ‘‡ ğ‘… Â¬3ğ‘†ğ¸ ğ‘… 1ğ‘…ğ¸ğ‘† (cid:105)

