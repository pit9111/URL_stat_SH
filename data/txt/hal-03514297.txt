Computing the Shapley Value of Facts in Query
Answering
Daniel Deutch, Nave Frost, Benny Kimelfeld, MikaÃ«l Monet

To cite this version:

Daniel Deutch, Nave Frost, Benny Kimelfeld, MikaÃ«l Monet. Computing the Shapley Value of Facts in
Query Answering. SIGMOD Conference 2022, Jun 2022, Philadelphia, United States. ï¿¿hal-03514297ï¿¿

HAL Id: hal-03514297

https://inria.hal.science/hal-03514297

Submitted on 6 Jan 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

Lâ€™archive ouverte pluridisciplinaire HAL, est
destinÃ©e au dÃ©pÃ´t et Ã  la diffusion de documents
scientifiques de niveau recherche, publiÃ©s ou non,
Ã©manant des Ã©tablissements dâ€™enseignement et de
recherche franÃ§ais ou Ã©trangers, des laboratoires
publics ou privÃ©s.

2
2
0
2

n
a
J

2

]

B
D
.
s
c
[

2
v
4
7
8
8
0
.
2
1
1
2
:
v
i
X
r
a

Computing the Shapley Value of Facts in Query Answering

Daniel Deutch
Tel Aviv University
Blavatnik School of
Computer Science, Tel Aviv,
Israel
danielde@post.tau.ac.il

Nave Frost
Tel Aviv University
Blavatnik School of
Computer Science, Tel Aviv,
Israel
navefrost@mail.tau.ac.il

Benny Kimelfeld
Technion - Israel Institute
of Technology
Faculty of Computer
Science, Haifa, Israel
bennyk@cs.technion.ac.il

MikaÃ«l Monet
Univ. Lille, Inria, CNRS,
Centrale Lille, UMR 9189
CRIStAL, F-59000 Lille,
France
mikael.monet@inria.fr

ABSTRACT
The Shapley value is a game-theoretic notion for wealth distribution
that is nowadays extensively used to explain complex data-intensive
computation, for instance, in network analysis or machine learning.
Recent theoretical works show that query evaluation over rela-
tional databases fits well in this explanation paradigm. Yet, these
works fall short of providing practical solutions to the computa-
tional challenge inherent to the Shapley computation. We present
in this paper two practically effective solutions for computing Shap-
ley values in query answering. We start by establishing a tight
theoretical connection to the extensively studied problem of query
evaluation over probabilistic databases, which allows us to obtain a
polynomial-time algorithm for the class of queries for which prob-
ability computation is tractable. We then propose a first practical
solution for computing Shapley values that adopts tools from prob-
abilistic query evaluation. In particular, we capture the dependence
of query answers on input database facts using Boolean expressions
(data provenance), and then transform it, via Knowledge Compila-
tion, into a particular circuit form for which we devise an algorithm
for computing the Shapley values. Our second practical solution
is a faster yet inexact approach that transforms the provenance to
a Conjunctive Normal Form and uses a heuristic to compute the
Shapley values. Our experiments on TPC-H and IMDB demonstrate
the practical effectiveness of our solutions.

1 INTRODUCTION
Explaining query answers has been the objective of extensive re-
search in recent years [12, 15, 20, 29, 30]. A prominent approach
is to devise an explanation based on the facts that were used for
deriving the answers; these facts are often termed provenance or
lineage of the query answer. For illustration, consider a query ask-
ing whether there exists a route from the USA to France with at
most one connection over a database of airports and flights. The
answer is Boolean, and upon receiving a positive answer one may
seek explanations of why it is so. The basis for such explanations
would include all details of qualifying routes that are used in the
derivation of the answer. Unfortunately, the number of relevant
routes might be huge (specifically, quadratic in the database size).
Moreover, it is conceivable that different facts differ considerably in
their importance to the answer at hand; for instance, some flights
may be crucial to enabling the USA-France connection, while others
may be easily replaced by alternatives.

To address these issues, there have been several proposals for
principled ways of quantifying the contribution of input facts to
query answers [20, 24, 25, 30]. We focus here on the recent approach
of Livshits et al. [20] that applies to this setting the notion of Shapley

values [32]â€”a game-theoretic function for distributing the wealth of
a team in a cooperative game. This function has strong theoretical
justifications [28], and indeed, it has been applied across various
fields such as economics, law, environmental science, and network
analysis. It has also been used for explanations in data-centric
paradigms such as knowledge representation [16, 39] and machine
learning [21, 22]. In the context of relational databases, given a
query ğ‘( Â¯ğ‘¥), a database ğ·, an input fact ğ‘“ âˆˆ ğ· and a tuple Â¯ğ‘¡ of
same arity as Â¯ğ‘¥, the Shapley value of ğ‘“ in ğ· for query ğ‘( Â¯ğ‘¥) and
tuple Â¯ğ‘¡ intuitively represents the contribution of ğ‘“ to the presence
(or absence) of Â¯ğ‘¡ in the query result.

Livshits et al. [20, 27] initiated the study of the computational
complexity of calculating Shapley values in query answering. They
showed mainly lower bounds on the complexity of the problem,
with the exception of the sub-class of self-join free SPJ queries called
hierarchical, where they gave a polynomial-time algorithm. The
results are more positive if imprecision is allowed, as they showed
that the problem admits a tractable approximation scheme (FPRAS,
to be precise) via Monte Carlo sampling. The state of affairs is that
the class of known tractable cases (namely the hierarchical conjunc-
tive queries) is highly restricted, and the approximation algorithms
with theoretical guarantees are impractical in the sense that they
require a large number of executions of the query over database
subsets (the samples). Hence, the theoretical analysis of Livshits et
al. [20, 27] does not provide sufficient evidence of practical feasibil-
ity for adopting the Shapley value as a measure of responsibility
in query answering. Moreover, the results of Livshits et al. [20] im-
ply that, for self-join free SPJ queries the class of tractable queries
for computing Shapley values coincides with the class of tractable
queries in probabilistic tuple-independent databases [7]. Yet, no
direct connection has been made between these two problems and,
theoretically speaking, it has been left unknown whether algorithms
for probabilistic databases can be used for Shapley computation.

Recently, Van den Broeck et al. [36] and Arenas et al. [2, 3] in-
vestigated the computational complexity of the SHAP-score [22], a
notion used in machine learning for explaining the predictions of
a model. While both are based on the general notion of Shapley
value, the SHAP-score for machine learning and Shapley values for
databases are different. In the latter case, the players are the tuples
of the database and the game function that is used is simply the
value of the query on a subset of the database, while in the former
case, the players are the features of the model and the game function
is a conditional expectation of the modelâ€™s output (see Section 6.2
for a more formal definition of the SHAP-score). Remarkably, Van
den Broeck et al. [36] have shown that computing the SHAP-score
is equivalent (in terms of polynomial-time reductions) to the prob-
lem of computing the expected value of the model. One of our

 
 
 
 
 
 
contributions is to show that the techniques developed by [2, 3, 36]
can be adapted to the context of Shapley values for databases. For
instance, by adapting to our context the proof of Van den Broeck
et al. [36] that computing the SHAP-score reduces to computing
the expected value of the model, we resolve the aforementioned
open question affirmatively: we prove that Shapley computation
can be efficiently (polynomial-time) reduced to probabilistic query
answering. Importantly, this applies not only to the restricted class
of SPJ queries without self-joins, but to every database query. Hence,
extending theory to practice, one can compute the Shapley values
using a query engine for probabilistic databases.

In turn, a common approach that was shown to be practically
effective for probabilistic databases is based on Knowledge Compila-
tion [11, 18]. In a nutshell, the idea is to first compute the Boolean
provenance of a given output tuple in the sense of Imielinski and
Lipski [17], and then to â€œcompileâ€ the provenance into a particular
circuit form that is more favorable for probability computation.
Specifically, the target class of this compilation is that of deter-
ministic and decomposable circuits (d-D). In our case, rather than
going through probabilistic databases, we devise a more efficient
approach that computes the Shapley values directly from the d-D
circuit. This is similar to how Arenas at al. [2, 3] directly prove
that the SHAP-score can be computed efficiently over such circuits,
without using the more general results of [36]. By adapting the
proof of [2, 3], we show how, given a d-D circuit representing the
provenance of an output tuple, we can efficiently compute the Shap-
ley value of every input fact. While the aforementioned properties
of the circuit are not guaranteed in general (beyond the class of
hierarchical queries), we empirically show the applicability and
usefulness of the approach even for non-hierarchical queries.

Our experimental results (see below) indicate that our exact com-
putation algorithm is fast in most cases, but is too costly in others.
For the latter cases, we propose a heuristic approach to retrieve the
relative order of the facts by their Shapley values, without actually
computing these values. Indeed, determining the most influential
facts is in many cases already highly useful, even if their precise
contribution remains unknown. The solution that we propose to
this end is termed CNF Proxy; it is based on a transformation of
the provenance to Conjunctive Normal Form (CNF) and using it
to compute proxy values intuitively based on (1) the number of
clauses in which a variable occur and (2) its alternatives in each
clause. These are two aspects that are correlated with Shapley val-
ues. The proxy values may be very different from the real Shapley
values, and yet, when we order facts according to their proxy values
we may intuitively get an ordering that is similar to the order via
Shapley. Our experiments validate that this intuition indeed holds
for examined benchmarks.

We have experimented with multiple queries from the two stan-
dard benchmarks TPC-H and IMDB. Our main findings are as fol-
lows. In most cases (98.67% of the IMDB output tuples and 83.83%
for TPC-H), our exact computation algorithm terminates in 2.5 sec-
onds or less, given the provenance expression. In the vast majority
of remaining cases the execution is very costly, typically running
out of memory already in the Knowledge Compilation step. By
contrast, our inexact solution CNF Proxy is extremely fast even for
these hard cases â€“ it typically terminates in a few milliseconds with
the worst observed case (an outlier) being 4 seconds. In fact, it is

faster by several orders of magnitude than sampling-based approx-
imation techniques (the Monte Carlo sampling proposed in [20]
as well as a popular sampling-based solution for Shapley values
in Machine Learning (Kernel SHAP [22]). To measure quality, we
use CNF Proxy to rank the input tuples, and compared the obtained
ranked lists to ranking by actual Shapley values (in cases where
exact computation has succeeded), using the standard measures of
nDCG and Precision@k. Our solution outperforms the competitors
in terms of quality as well.

We then propose a simple hybrid approach: execute the exact
algorithm until it either terminates or a timeout elapses. If we have
reached the timeout, resort to executing CNF Proxy and rank the
facts based on the obtained values. We show experiments with
different timeout values, justifying our choice of 2.5 seconds.

Hence, our contributions are both of a theoretical and practical

nature and can be summarized as follows.

â€¢ By adapting the proof technique of [36], we establish a fun-
damental result about the complexity of computing Shapley
values over relational queries: Shapley values can be com-
puted in polynomial timeâ€“in data complexityâ€“whenever
the query can be evaluated in polynomial time over tuple-
independent probabilistic databases (Proposition 3.1). This
holds for every query.

â€¢ By adapting the proof technique of [2, 3], we devise a novel
algorithm for computing Shapley values for query evaluation
via compilation to a deterministic and decomposable circuit
(Proposition 4.4). We show that this algorithm is practical
and has the theoretical guarantee of running in polynomial
time in the size of the circuit.

â€¢ We present a novel heuristic, CNF Proxy, that is fast yet inex-
act, and is practically effective if we are interested in ranking
input facts by their contribution rather than computing exact
Shapley values (Section 5).

â€¢ We describe a thorough experimental study of our algorithms
over realistic data and show their efficiency (Section 6).

Related work. Existing models for explaining database query re-
sults may roughly be divided in two categories: (1) models that are
geared for tracking/presenting provenance of output tuples, e.g., the
set of all input facts participating in their computation [6], possibly
alongside a description of the ways they were used, in different
granularity levels (e.g., [4, 5, 14]); (2) models that quantify contri-
butions of input facts [20, 24, 25, 30], which is the approach that
we follow here. Works in the latter context often have connections
with the influential line of work on probabilistic databases [33], and
we show that this is the case for Shapley computation as well.

As already mentioned, an important point of comparison is the
work of Van den Broeck et al. [36] and that of Arenas et al. [2, 3] on
the SHAP-score. While we show that the proof techniques developed
in this area can be adapted to the context of relational databases,
we point out that the two sets of results obtained (for SHAP-score
and for Shapley values for databases) seem incomparable, as we
do not see a way of proving results for Shapley value for query
answering using the results on the SHAP-score, or vice-versa. In
fact, this adaptation only works up to a certain point. For instance,
the efficiency axiom of the Shapley value immediately implies that

2

computing the expected value of a model can be reduced in polyno-
mial time to computing the SHAP-score of its features; in contrast,
this axiom does not seem to yield any clear such implication in our
context (see our Open Problem 1 and the discussion around it).

Paper organization. We formalize the notion of Shapley values for
query answering in Section 2. In Section 3 we present the theoretical
connection to probabilistic databases and its implications. Our exact
computation algorithm is presented in Section 4 and our heuristic
in Section 5. Experimental results are presented in Section 6 and
we conclude in Section 7.

2 THE SHAPLEY VALUE OF FACTS
We define here the main notion and illustrate it with an example.

Relational databases and queries. Let Î£ = {ğ‘…1, . . . , ğ‘…ğ‘› } be a
signature, consisting of relation names ğ‘…ğ‘– each with its associated
arity ar(ğ‘…ğ‘– ) âˆˆ N, and Const be a set of constants. A fact over
(Î£, Const) is simply a term of the form ğ‘…(ğ‘1, . . . , ğ‘ar(ğ‘…) ), for ğ‘… âˆˆ Î£
and ğ‘ğ‘– âˆˆ Const. A (Î£, Const)-database ğ·, or simply a database ğ·,
is a finite set of facts over (Î£, Const). We assume familiarity with
the most common classes of query languages and refer the reader
to [1] for the basic definitions. In particular, we recall the equiva-
lence between relational algebra and relational calculus [1], and the
fact that Select-Project-Join-Union (SPJU) queries are equivalent
to unions of conjunctive queries (UCQs). Depending of the con-
text and for consistency with relevant past publications, we will
use terminology of either relational calculus or relational algebra.
What we call a Boolean query is a query ğ‘ that takes as input a
database ğ· and outputs ğ‘(ğ·) âˆˆ {0, 1}. If ğ‘( Â¯ğ‘¥) is a query with free
variables Â¯ğ‘¥ and Â¯ğ‘¡ is a tuple of constants of same length as Â¯ğ‘¥, we
denote by ğ‘[ Â¯ğ‘¥/Â¯ğ‘¡] the Boolean query defined by: ğ‘[ Â¯ğ‘¥/Â¯ğ‘¡] (ğ·) = 1 if
and only if Â¯ğ‘¡ is in the output of ğ‘( Â¯ğ‘¥) on ğ·.

Shapley values of facts. Following [20, 27], we use the notion of
Shapley values [32] to attribute a contribution to facts of an input
database. In this context, the database ğ· is traditionally partitioned
into two sets of facts: a set ğ·x of so-called exogenous facts, and a
set ğ·n of endogenous facts. The idea is that exogenous facts are
considered as given, while endogenous facts are those to which
we would like to attribute contributions. Let ğ‘ be a Boolean query
and ğ‘“ âˆˆ ğ·n be an endogenous fact. The Shapley value of ğ‘“ in ğ· for
query ğ‘, denoted Shapley(ğ‘, ğ·n, ğ·x, ğ‘“ ), is defined as

Shapley(ğ‘, ğ·n, ğ·x, ğ‘“ )

def
=

âˆ‘ï¸

ğ¸ âŠ†ğ·n\{ğ‘“ }

|ğ¸|!(|ğ·n| âˆ’ |ğ¸| âˆ’ 1)!
|ğ·n|!

(1)

(cid:0)ğ‘(ğ·x âˆª ğ¸ âˆª {ğ‘“ }) âˆ’ ğ‘(ğ·x âˆª ğ¸)(cid:1).

Notice that here, |ğ¸|!(|ğ·n| âˆ’ |ğ¸| âˆ’ 1)! is the number of permutations
of ğ·n with all endogenous facts in ğ¸ appearing first, then ğ‘“ , and
finally, all the other endogenous facts. Intuitively then, the value
Shapley(ğ‘, ğ·n, ğ·x, ğ‘“ ) represents the contribution of ğ‘“ to the queryâ€™s
output: the higher this value is, the more ğ‘“ helps in satisfying ğ‘.

For non-Boolean queries ğ‘( Â¯ğ‘¥), we are interested in the Shapley
value of the fact ğ‘“ for every individual tuple Â¯ğ‘¡ in the output [20].
The extension to non-Boolean ğ‘( Â¯ğ‘¥) is then straightforward: the

Shapley value of the fact ğ‘“ for the answer Â¯ğ‘¡ to ğ‘( Â¯ğ‘¥) is the value
Shapley(ğ‘[ Â¯ğ‘¥/Â¯ğ‘¡], ğ·n, ğ·x, ğ‘“ ). Therefore, the computational challenge
reduces to that of the Boolean query ğ‘ [ Â¯ğ‘¥/Â¯ğ‘¡]. Hence, in the theo-
retical analysis we focus on Boolean queries, and we go back to
considering non-Boolean queries when we study the implementa-
tion aspects (starting in Section 4.2).

Example 2.1. Consider the database ğ· and the Boolean query ğ‘
from Figures 1a and 1c. All facts in table Flights are endogenous,
while facts in Airports are exogenous. To alleviate the notation
we write, e.g., ğ‘1 for Flights(JFK, CDG). The query ğ‘ checks if
there are routes from â€œUSAâ€ to â€œFRâ€ with one or less connect-
ing flights. Let us compute the Shapley value of all endogenous
facts. First, we notice that fact ğ‘8 is not part of any valid route,
so Shapley(ğ‘, ğ·n, ğ·x, ğ‘8) = 0 by Equation (1). Next, let us focus
on ğ‘1. Since ğ‘1 is a valid route on its own, adding it to any subset
of (endogenous) facts ğ¸ such that ğ¸ does not contain a valid route
results in ğ‘(ğ·x âˆª ğ¸ âˆª {ğ‘1}) âˆ’ğ‘(ğ·x âˆª ğ¸) = 1 (for all other subsets the
difference will be 0). The relevant subsets are the empty set, all sin-
gletons {ğ‘ğ‘– } for 2 â‰¤ ğ‘– â‰¤ 8 (7 singletons), all the pairs of tuples from
ğ‘2, . . . , ğ‘8 excluding the pairs {ğ‘2, ğ‘4}, {ğ‘2, ğ‘5}, {ğ‘3, ğ‘4}, {ğ‘3, ğ‘5},
and {ğ‘6, ğ‘7} (so (cid:0)7
(cid:1) âˆ’ 5 = 16 pairs), the quadruples {ğ‘2, ğ‘3, ğ‘6, ğ‘8},
2
{ğ‘2, ğ‘3, ğ‘7, ğ‘8}, {ğ‘4, ğ‘5, ğ‘6, ğ‘8}, and {ğ‘4, ğ‘5, ğ‘7, ğ‘8}, and overall 14
triplets (left to the reader). Summing it all up results in

+ 7 Â·

+ 16 Â·

2!5!
8!

Shapley(ğ‘, ğ·n, ğ·x, ğ‘1) = 1 Â·

0! Â· 7!
8!
3!4!
8!
Similarly one can compute the Shapley value of the remaining facts,
and find that
that
Shapley(ğ‘, ğ·n, ğ·x, ğ‘ğ‘– ) = 23
210 â‰ˆ 0.1095, and that for ğ‘ğ‘– âˆˆ {ğ‘6, ğ‘7}
we have Shapley(ğ‘, ğ·n, ğ·x, ğ‘ğ‘– ) = 8

1! Â· 6!
8!
4!3!
8!

{ğ‘2, ğ‘3, ğ‘4, ğ‘5}

â‰ˆ 0.4095.

105 â‰ˆ 0.0762.

it holds

43
105

for ğ‘ğ‘–

+ 4 Â·

14 Â·

=

+

âˆˆ

3 REDUCTION TO PROBABILISTIC

DATABASES

In this section we investigate the complexity of computing Shapley
values. As explained in the previous section, the non-Boolean set-
ting of the problem may be reduced to that of Boolean queries, so
we will study the following problem for a given Boolean query ğ‘.

PROBLEM:

Shapley(ğ‘)

INPUT: A database ğ· = ğ·x âˆª ğ·n and an endogenous

fact ğ‘“ âˆˆ ğ·n.

OUTPUT: The value Shapley(ğ‘, ğ·n, ğ·x, ğ‘“ ).

Note that for the sake of complexity analysis, the query ğ‘ is
assumed to be fixed, so that each query gives rise to a different
computational problem; we are then considering what is called the
data complexity [37]. This assumption is motivated by the fact that
in practice, the queries are much smaller than the databases.

The complexity of this problem has been studied in [20, 27],
where in particular a dichotomy has been obtained for self-joinâ€“free
Boolean conjunctive queries (sjfbcqs). There, the authors show that,
for every sjfbcq ğ‘, either ğ‘ is hierarchical (we will not need to define
this notion here) and Shapley(ğ‘) can be solved in polynomial time,
or ğ‘ is not hierarchical and then Shapley(ğ‘) is intractable (specif-
ically, FP#P-hard). It turns out that the tractability criterion that

3

Flights (endo)
Dest
Src
CDG
JFK
LHR
EWR
LHR
BOS
CDG
LHR
LHR
ORY
LAX MUC
ORY
LHR MUC

ğ‘1
ğ‘2
ğ‘3
ğ‘4
ğ‘5
ğ‘6
ğ‘7 MUC
ğ‘8

Airports (exo)
Name Country

JFK
ğ‘1
EWR
ğ‘2
BOS
ğ‘3
LAX
ğ‘4
LHR
ğ‘5
ğ‘6 MUC
ORY
ğ‘7
CDG
ğ‘8

USA
USA
USA
USA
EN
GR
FR
FR

(a) Database of flights and airports

ğ‘6

LAX

MUC

ğ‘
7

ğ‘8
ğ‘

5

ORY

BOS

ğ‘
3

LHR

ğ‘

2

Routes from â€œUSAâ€ to â€œFRâ€ with
one or less connecting flights

ğ‘1 = âˆƒğ‘¥, ğ‘¦ : Airports(ğ‘¥, â€œUSAâ€) âˆ§

Airports(ğ‘¦, â€œFRâ€) âˆ§
Flights(ğ‘¥, ğ‘¦)

ğ‘

4

ğ‘2 = âˆƒğ‘¥, ğ‘¦, ğ‘§ : Airports(ğ‘¥, â€œUSAâ€) âˆ§

EWR

JFK

ğ‘1

CDG

(b) Flights in graph view. Dark
and light gray depict â€œUSAâ€
and â€œFRâ€ airports respectively

Airports(ğ‘§, â€œFRâ€) âˆ§
Flights(ğ‘¥, ğ‘¦) âˆ§ Flights(ğ‘¦, ğ‘§)

ğ‘ = ğ‘1 âˆ¨ ğ‘2

(c) ğ‘ is a Boolean union of conjunctive queries
(UCQ)

(ğ‘1 âˆ§ ğ‘1 âˆ§ ğ‘8)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:124)
(cid:125)
(cid:123)(cid:122)
ğ‘1 provenance

âˆ¨ (ğ‘2 âˆ§ ğ‘4 âˆ§ ğ‘2 âˆ§ ğ‘8) âˆ¨ (ğ‘2 âˆ§ ğ‘5 âˆ§ ğ‘2 âˆ§ ğ‘7) âˆ¨ (ğ‘3 âˆ§ ğ‘4 âˆ§ ğ‘3 âˆ§ ğ‘8) âˆ¨ (ğ‘3 âˆ§ ğ‘5 âˆ§ ğ‘3 âˆ§ ğ‘7) âˆ¨ (ğ‘6 âˆ§ ğ‘7 âˆ§ ğ‘5 âˆ§ ğ‘7)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:123)(cid:122)
(cid:125)
ğ‘2 provenance

(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

(cid:124)

(d) The lineage Lin(ğ‘, ğ·) as a formula in Disjunctive Normal Form (DNF)

Figure 1: The database, query ğ‘ and its lineage used for our running example

is obtainedâ€”being hierarchicalâ€”is exactly the same as in the con-
text of probabilistic query evaluation (PQE); see, e.g., [7, 8]. In fact,
the main result of this section is that this is not a coincidence: we
prove that, for every Boolean query ğ‘ (not just for sjfbcqs), if PQE
is tractable for ğ‘ then so is the problem Shapley(ğ‘). Since PQE has
been intensively studied already, our result allows us to vastly ex-
tend the tractable cases identified in [20, 27]. We now proceed with
the definitions and proof of this result, and explain its consequences.

Probabilistic query evaluation. A tuple-independent (TID) data-
base is a pair consisting of a database ğ· and a function ğœ‹ mapping
each fact ğ‘“ âˆˆ ğ· to a probability value ğœ‹ (ğ‘“ ) âˆˆ [0, 1]. The TID
(ğ·, ğœ‹) defines a probability distribution Prğœ‹ on ğ· â€² âŠ† ğ·, where
def
= (cid:206)ğ‘“ âˆˆğ·â€² ğœ‹ (ğ‘“ ) Ã— (cid:206)ğ‘“ âˆˆğ·\ğ·â€² (1 âˆ’ ğœ‹ (ğ‘“ )). Given a Boolean
Prğœ‹ (ğ· â€²)
def
query ğ‘, the probability that ğ‘ is satisfied by (ğ·, ğœ‹) is Pr(ğ‘, (ğ·, ğœ‹))
=
(cid:205)ğ·â€² âŠ†ğ· s.t. ğ‘ (ğ·â€²)=1 Prğœ‹ (ğ· â€²). The probabilistic query evaluation prob-
lem for ğ‘, PQE(ğ‘) for short, is then defined as follows.

PROBLEM:

PQE(ğ‘)

INPUT: A tuple-independent database (ğ·, ğœ‹).

OUTPUT: The value Pr(ğ‘, (ğ·, ğœ‹)).

For two computational problems ğ´ and ğµ, we write ğ´ â‰¤

p
T ğµ to
assert the existence of a polynomial-time Turing reduction from ğ´
to ğµ. We are ready to state the main result of this section.

Proposition 3.1. For every Boolean query ğ‘, we have that

Shapley(ğ‘) â‰¤

p
T PQE(ğ‘).

This result implies that for any query ğ‘ for which PQE(ğ‘) is
tractable then so is Shapley(ğ‘). Dalvi and Suciu [8] showed a di-
chotomy for unions of conjunctive queries: for every such query ğ‘,
either PQE(ğ‘) is solvable in polynomial time, in which case ğ‘ is

4

called safe1, or PQE(ğ‘) is FP#P-hard (and ğ‘ is called unsafe). There-
fore, we obtain as a direct corollary of Proposition 3.1 that Shapley(ğ‘)
can be solved in polynomial time for all safe queries.

Corollary 3.2. If ğ‘ is a safe UCQ then Shapley(ğ‘) can be solved

in polynomial time.

In particular, this corollary generalizes the tractability result
obtained in [20], to account for CQs with self-joins and even unions
of such queries. We now prove Proposition 3.1.

Proof of Proposition 3.1. For a Boolean query ğ‘, database ğ· =

ğ·x âˆª ğ·n, and integer ğ‘˜ âˆˆ {0, . . . , |ğ·n|}, define
def
= |{ğ¸ âŠ† ğ·n | |ğ¸| = ğ‘˜ and ğ‘(ğ·x âˆª ğ¸) = 1}|.
#Slices(ğ‘, ğ·x, ğ·n, ğ‘˜)
Then, by grouping by size the terms ğ¸ from Equation 1 we obtain

Shapley(ğ‘, ğ·n, ğ·x, ğ‘“ ) =

|ğ·n |âˆ’1
âˆ‘ï¸

ğ‘˜=0

ğ‘˜!(|ğ·n| âˆ’ ğ‘˜ âˆ’ 1)
|ğ·n|

(cid:18)

(2)

#Slices(ğ‘, ğ·x âˆª {ğ‘“ }, ğ·n \ {ğ‘“ }, ğ‘˜)

âˆ’ #Slices(ğ‘, ğ·x, ğ·n \ {ğ‘“ }, ğ‘˜)

(cid:19)

.

All arithmetical terms (such as ğ‘˜! or |ğ·n|!) can be computed in
p
T PQE(ğ‘),
polynomial time. Therefore, to prove that Shapley(ğ‘) â‰¤
it is enough to show that, given an oracle to the problem PQE(ğ‘), we
can compute in polynomial time the quantities #Slices(ğ‘, ğ·x, ğ·n, ğ‘˜),
for some arbitrary ğ· = ğ·x âˆª ğ·n and ğ‘˜ âˆˆ {0, . . . , |ğ·n|}. This is what
we do next.

The proof is similar to that of [36, Theorem 2] in the context of
SHAP-score for machine learning (but, as explained in the Intro-
duction, the two results seem to be incomparable).

We wish to compute #Slices(ğ‘, ğ·x, ğ·n, ğ‘˜), for some database ğ· =
ğ·x âˆªğ·n and integer ğ‘˜ âˆˆ {0, . . . |ğ·n|}. Let ğ‘› = |ğ·n| be the number of
endogenous facts of ğ·. For ğ‘§ âˆˆ Q, we define a TID database (ğ·ğ‘§, ğœ‹ğ‘§)

1This notion of safety is distinct from the â€œusualâ€ notion of query safety [1] that
ensures domain independence.

as follows: ğ·ğ‘§ contains all the facts of ğ·, and for an exogenous
def
fact ğ‘“ of ğ· we define ğœ‹ğ‘§ (ğ‘“ )
= 1 while for an endogenous fact ğ‘“
def
= ğ‘§
of ğ· we define ğœ‹ğ‘§ (ğ‘“ )
1+ğ‘§ . It is then routine to show that the
following relation holds:

(1 + ğ‘§)ğ‘› Pr(ğ‘, (ğ·ğ‘§, ğœ‹ğ‘§)) =

ğ‘›
âˆ‘ï¸

ğ‘–=0

ğ‘§ğ‘– #Slices(ğ‘, ğ·x, ğ·n, ğ‘–).

This suffices to conclude the proof. Indeed, we now call an ora-
cle to PQE(ğ‘) on ğ‘› + 1 databases ğ·ğ‘§0, . . . , ğ·ğ‘§ğ‘› for ğ‘› + 1 arbitrary
distinct values ğ‘§0, . . . , ğ‘§ğ‘›, forming a system of linear equations as
given by the relation above. Since the corresponding matrix is a
Vandermonde with distinct coefficients, it is invertible, so we can
â–¡
compute in polynomial time the value #Slices(ğ‘, ğ·x, ğ·n, ğ‘˜).

A intriguing natural question is whether the converse of Proposi-
p
tion 3.1 is true, that is, whether we also have PQE(ğ‘) â‰¤
T Shapley(ğ‘).
This is true when ğ‘ is a self-joinâ€“free conjunctive query: indeed,
by the results of [20], either ğ‘ is hierarchical and then both PQE(ğ‘)
p
and Shapley(ğ‘) can be solved in polynomial time (hence PQE(ğ‘) â‰¤
T
Shapley(ğ‘)), or ğ‘ is not hierarchical and then Shapley(ğ‘) is FP#P-
hard (which, together with the fact that PQE(ğ‘) âˆˆ FP#P, implies
p
T Shapley(ğ‘)). However, to the best of our knowledge,
PQE(ğ‘) â‰¤
the existence of such a reduction in the general case is unknown.

Open problem 1. Do we have PQE(ğ‘) â‰¤

p
T Shapley(ğ‘) for every

Boolean query ğ‘?

Interestingly, we note that this direction is trivial in the setting of
SHAP-scores. Indeed, this is directly implied by the efficiency axiom
of the Shapley value; see, e.g., [2, Lemma 4.2] or [36, Equation 5].
In our case, this axiom only gives us the following equality:

âˆ‘ï¸

ğ‘“ âˆˆğ·n

Shapley(ğ‘, ğ·n, ğ·x, ğ‘“ ) = ğ‘(ğ·n âˆª ğ·x) âˆ’ ğ‘(ğ·x),

which does not seem to help in showing PQE(ğ‘) â‰¤

p
T Shapley(ğ‘).

4 EXACT COMPUTATION THROUGH

KNOWLEDGE COMPILATION

Motivated by the connection to PQE that we have seen in Sec-
tion 3, we now investigate whether an approach using knowledge
compilation can be used for computing Shapley values. Indeed,
a common method to compute the probability that a probabilis-
tic database (ğ·, ğœ‹) satisfies a Boolean query ğ‘ is to first compute
the lineage of ğ‘ on ğ· in a formalism from knowledge compilation,
and then to use the good properties of said formalism to com-
pute Pr(ğ‘, (ğ·, ğœ‹)) in linear time [18, 26, 33]. Recently, Arenas and
others [2, 3] showed that this approach is also viable for the notion
of SHAP-score used in machine learning, by proving that SHAP-
scores can be computed in polynomial time when the models are
given as circuits from knowledge compilation. By reusing some of
these techniques, we can show that this method can also be used in
our setting for computing Shapley values of database facts. Again,
to the best of our knowledge, the two results are incomparable, i.e.,
we are not aware of a reduction in either direction between the two
problems. We start by formally defining the notions of lineage and
the relevant circuit classes from knowledge compilation.

5

âˆ¨

âˆ§

Â¬

âˆ¨

ğ‘1

Â¬

âˆ§

ğ‘6

ğ‘7

âˆ§

Â¬

âˆ§

Â¬

âˆ§

Â¬

ğ‘3

Â¬

ğ‘2

Â¬

ğ‘4

Â¬

ğ‘5

2: Deterministic

Figure
for ELin(ğ‘, ğ·x, ğ·n) from the running example.

and

decomposable

circuit

Boolean functions and query lineages. Let ğ‘‹ be a finite set of
variables. An assignment ğœˆ of ğ‘‹ is a subset ğœˆ âŠ† ğ‘‹ of ğ‘‹ . We denote
by 2ğ‘‹ the set of all assignments of ğ‘‹ . A Boolean function ğœ‘ over ğ‘‹
is a function ğœ‘ : 2ğ‘‹ â†’ {0, 1}. An assignment ğœˆ âŠ† ğ‘‹ is satisfying
if ğœ‘ (ğœˆ) = 1. We denote by SAT(ğœ‘) âŠ† 2ğ‘‹ the set of all satisfying
assignments of ğœ‘, and by #SAT(ğœ‘) the size of this set. For ğ‘˜ âˆˆ N, we
def
define SATğ‘˜ (ğœ‘)
= SAT(ğœ‘) âˆ© {ğœˆ âŠ† ğ‘‹ | |ğœˆ | = ğ‘˜ }, that is, the set of
satisfying assignments of ğœ‘ of Hamming weight ğ‘˜, and let #SATğ‘˜ (ğœ‘)
be the size of this set.

Let ğ‘ be a Boolean query and ğ· be a database. The lineage
Lin(ğ‘, ğ·) is the (unique) Boolean function whose variables are the
facts of ğ·, and that maps each sub-database ğ· â€² âŠ† ğ· to ğ‘(ğ· â€²). This
definition extends straightforwardly to queries with free variables
as follows: if ğ‘( Â¯ğ‘¥) is a query with free variables Â¯ğ‘¥ and Â¯ğ‘¡ is a tuple of
constants of the appropriate size, then Lin(ğ‘[ Â¯ğ‘¥/Â¯ğ‘¡]), ğ·) is the lineage
for the tuple Â¯ğ‘¡.

Example 4.1. Consider again the database ğ· and the Boolean
query ğ‘ from Figures 1a and 1c. In Figure 1d, the lineage Lin(ğ‘, ğ·)
is represented as a formula in disjunctive normal form (DNF).

For our purposes, we will use a refinement of this lineage that
accounts for the nature of exogenous tuples; specifically, these tu-
ples should be considered as always being part of the database.
Let ğ· = ğ·n âˆª ğ·x be a database with endogenous tuples ğ·n and
exogenous tuples ğ·x, and let ğ‘ be a Boolean query. Then the en-
dogenous lineage ELin(ğ‘, ğ·x, ğ·n) is the (unique) Boolean function
whose variables are ğ·n and that maps every set ğ¸ of endogenous
facts to ğ‘(ğ·x âˆª ğ¸). In other words, ELin(ğ‘, ğ·x, ğ·n) can be obtained
from Lin(ğ‘, ğ·) by fixing all variables in ğ·x to the value 1. Again,
we extend this definition to queries with free variables by using the
function ELin(ğ‘[ Â¯ğ‘¥/Â¯ğ‘¡], ğ·x, ğ·n).

Example 4.2. Continuing the previous example, the endogenous

lineage ELin(ğ‘, ğ·n, ğ·x) can be represented as a DNF by
ğ‘1 âˆ¨ (ğ‘2 âˆ§ ğ‘4) âˆ¨ (ğ‘2 âˆ§ ğ‘5) âˆ¨ (ğ‘3 âˆ§ ğ‘4) âˆ¨ (ğ‘3 âˆ§ ğ‘5) âˆ¨ (ğ‘6 âˆ§ ğ‘7) .

In the last two examples, lineages were represented with Boolean
formulas in DNF. Since a lineage is a Boolean function, it can be
represented with any formalism that allows to represent Boolean
functions. We next review some classes of circuits from the field of
knowledge compilation that will be relevant for our work.

Knowledge compilation classes. Let ğ¶ be a Boolean circuit, fea-
turing âˆ§, âˆ¨, Â¬, and variable gates, with the usual semantics.2 For a
gate ğ‘” of ğ¶, we denote by Vars(ğ‘”) the set of variables that have a
directed path to ğ‘”. An âˆ§-gate ğ‘” of ğ¶ is decomposable if for every two
input gates ğ‘”1 â‰  ğ‘”2 of ğ‘” we have Vars(ğ‘”1) âˆ© Vars(ğ‘”2) = âˆ…. We call ğ¶
decomposable if all âˆ§-gates are. An âˆ¨-gate ğ‘” of ğ¶ is deterministic
if the Boolean functions captured by each pair of distinct input
gates of ğ‘” are pairwise disjoint; i.e., no assignment satisfies both.
We call ğ¶ deterministic if all âˆ¨-gates in it are. A deterministic and
decomposable (d-D [26]) Boolean circuit is a Boolean circuit that is
both deterministic and decomposable. If ğ¶ is a Boolean circuit we
write Vars(ğ¶) to denote the set of variables that appear in it.

Example 4.3. Recall ELin(ğ‘, ğ·n, ğ·x) from Example (4.2) repre-
sented as a DNF. Figure 2 depicts a d-D circuit for ELin(ğ‘, ğ·n, ğ·x).
The output gate, for instance, is a deterministic âˆ¨-gate: indeed, its
left child requires ğ‘1 to be 1, whereas its right child requires ğ‘1 to
be 0. The right child of the output gate is a decomposable âˆ§-gate:
indeed, for its left child ğ‘”1 we have Vars(ğ‘”1) = {ğ‘1}, whereas for its
right child ğ‘”2 we have Vars(ğ‘”2) = {ğ‘2, ğ‘3, ğ‘4, ğ‘5, ğ‘6, ğ‘7}, and these
are indeed disjoint. The reader can easily check that all other âˆ¨-
gates are deterministic, and that all other âˆ§-gates are decomposable.

4.1 Algorithm
The main result of this section is then the following.

Proposition 4.4. Given as input a deterministic and decompos-
able circuit ğ¶ representing ELin(ğ‘, ğ·n, ğ·x) for a database ğ· = ğ·x âˆª
ğ·n and Boolean query ğ‘, and an endogenous fact ğ‘“ âˆˆ ğ·n, we can
compute in polynomial time (in |ğ¶ |) the value Shapley(ğ‘, ğ·n, ğ·x, ğ‘“ ).

Next, we prove Proposition 4.4 and present the algorithm, and
then explain in Section 4.2 the architecture of the implementation.

Proof of Proposition 4.4. Let ğ¶ be a deterministic and decom-
posable circuit representing ELin(ğ‘, ğ·n, ğ·x), and let ğ‘“ âˆˆ ğ·n. First,
we complete the circuit ğ¶ so that all variables of ğ·n appear in ğ¶.
Indeed, it could be the case that Vars(ğ¶) âŠŠ ğ·n: this happens for in-
stance with the deterministic and decomposable circuit in Figure 2,
where the endogenous fact ğ‘8 does not appear in the circuit. To do
this, we conjunct ğ¶ with the conjunction (cid:211)ğ‘“ â€² âˆˆğ·n\Vars(ğ¶) (ğ‘“ â€² âˆ¨ Â¬ğ‘“ â€²).
Note that this does not change the semantics of the circuit (as this
conjunction always evaluates to 1) and that the resulting circuit
is still deterministic and decomposable. Now, let ğ¶1 (resp., ğ¶2) be
the Boolean circuit obtained from ğ¶ by replacing all variable gates
corresponding to the fact ğ‘“ by a constant 1-gate (resp., by a con-
stant 0-gate). Observe then that the variables of ğ¶1 and ğ¶2 are
exactly ğ·n \ {ğ‘“ }, and moreover that ğ¶1 and ğ¶2 are still determinis-
tic and decomposable. By definition of the endogenous lineage, we
can rewrite Equation (2) into the following.

2We allow unbounded-fanin âˆ§- and âˆ¨-gates, and also allow constant 1-gates and
constant 0-gates as, respectively, âˆ§-gates with no input and âˆ¨-gates with no inputs.

6

Shapley(ğ‘, ğ·n, ğ·x, ğ‘“ ) =

|ğ·n |âˆ’1
âˆ‘ï¸

ğ‘˜=0

ğ‘˜!(|ğ·n| âˆ’ ğ‘˜ âˆ’ 1)
|ğ·n|

(cid:0)

#SATğ‘˜ (ğ¶1) âˆ’ #SATğ‘˜ (ğ¶2)(cid:1).

(3)

Proposition 4.4 will thus directly follow from the next lemma.

Lemma 4.5. Given as input a deterministic and decomposable
Boolean circuit ğ¶ and an integer ğ‘˜ âˆˆ {0, . . . , |Vars(ğ¶)|}, we can
compute in polynomial time the quantity #SATğ‘˜ (ğ¶).

Proof. Our proof is similar to that of [3, Section 3.2]. Let ğ‘‹

def
=
def
= |ğ‘‹ |. First of all, we preprocess ğ¶ so that the
Vars(ğ¶) and ğ‘›
fanin of every âˆ¨- and âˆ§-gate is exactly 0 or 2; this can simply be
done by rewriting every âˆ§-gate of fanin ğ‘š > 2 with ğ‘š âˆ’ 1 âˆ§-
gates of fanin 2 (same for âˆ¨-gates), and adding a constant gate of
the appropriate type to every âˆ¨- and âˆ§-gate of fan-in 1. We then
compute, for every gate ğ‘” of ğ¶, the set of variables Vars(ğ‘”) upon
which the value of ğ‘” depends. For a gate ğ‘” of ğ¶, let us denote by ğœ‘ğ‘”
the Boolean function over the variables Vars(ğ‘”) that is represented
by this gate. For a gate ğ‘” and an integer â„“ âˆˆ {0, . . . , |Vars(ğ‘”)|},
def
we define ğ›¼ â„“
= #SATâ„“ (ğœ‘ğ‘”), i.e. the number of assignments of
ğ‘”
size â„“ to Vars(ğ‘”) that satisfy ğœ‘ğ‘”. We will show how to compute all
the values ğ›¼ â„“
ğ‘” for every gate ğ‘” of ğ¶ and â„“ âˆˆ {0, . . . , |Vars(ğ‘”)|} in
polynomial time. This will conclude the proof since, for the output
gate ğ‘”output of ğ¶, we have that ğ›¼ğ‘˜
= #SATğ‘˜ (ğ‘“ ). We will need
the following notation: for two disjoint sets of variables ğ‘‹1, ğ‘‹2 and
two subsets ğ‘†1 âŠ† 2ğ‘‹1 , ğ‘†2 âŠ† 2ğ‘‹2 of assignments to ğ‘‹1 and ğ‘‹2, we
denote by ğ‘†1 âŠ— ğ‘†2 âŠ† 2ğ‘‹1âˆªğ‘‹2 the set of assignments of ğ‘‹1 âˆª ğ‘‹2
def
defined by ğ‘†1 âŠ— ğ‘†2
= {ğœˆ1 âˆª ğœˆ2 | ğœˆ1 âˆˆ ğ‘†2, ğœˆ2 âˆˆ ğ‘†2}. We next show
how to compute the values ğ›¼ â„“

ğ‘” by bottom-up induction on ğ¶.

ğ‘”output

Variable gate. If ğ‘” is a variable gate corresponding to some

variable ğ‘¦, then Vars(ğ‘”) = {ğ‘¦}. Then, ğ›¼ 0
Â¬-gate. If ğ‘” is a Â¬-gate with input gate ğ‘”â€², then ğ›¼ â„“

ğ‘” is 0 and ğ›¼ 1

ğ‘” is 1.
ğ‘” = (cid:0) |Vars(ğ‘”) |

(cid:1)âˆ’

ğ‘™

ğ›¼ â„“
ğ‘”â€² for every â„“ âˆˆ {0, . . . , |Vars(ğ‘”)|}.

Deterministic âˆ¨-gate. If ğ‘” is a deterministic âˆ¨-gate with no
input then ğœ‘ğ‘” is the Boolean function on variables Vars(ğ‘”) =
âˆ… that is always false, hence ğ›¼ 0
ğ‘” = 0. Otherwise ğ‘” has ex-
actly two input gates; let us denote them ğ‘”1 and ğ‘”2. Ob-
serve that Vars(ğ‘”) = Vars(ğ‘”1) âˆª Vars(ğ‘”2) by definition. De-
def
fine ğ‘†1
= Vars(ğ‘”1) \
Vars(ğ‘”2). Since ğ‘” is deterministic, we have:

def
= Vars(ğ‘”2) \Vars(ğ‘”1) and similarly ğ‘†2

SAT(ğœ‘ğ‘”) = (SAT(ğœ‘ğ‘”1 ) âŠ— 2ğ‘†1 ) âˆª (SAT(ğœ‘ğ‘”2 ) âŠ— 2ğ‘†2 )

with the union being disjoint. By intersecting with the as-
signments of Vars(ğ‘”) of size â„“, we obtain:

SATâ„“ (ğœ‘ğ‘”) =(cid:2)(SAT(ğœ‘ğ‘”1 ) âŠ— 2ğ‘†1 ) âˆ© {ğœˆ âŠ† Vars(ğ‘”) | |ğœˆ | = â„“ }(cid:3)

âˆª (cid:2)(SAT(ğœ‘ğ‘”2 ) âŠ— 2ğ‘†2 ) âˆ© {ğœˆ âŠ† Vars(ğ‘”) | |ğœˆ | = â„“ }(cid:3)
with again the middle union being disjoint, therefore:
#SATâ„“ (ğœ‘ğ‘”) =|(SAT(ğœ‘ğ‘”1 ) âŠ— 2ğ‘†1 ) âˆ© {ğœˆ âŠ† Vars(ğ‘”) | |ğœˆ | = â„“ }|

+ |(SAT(ğœ‘ğ‘”2 ) âŠ— 2ğ‘†2 ) âˆ© {ğœˆ âŠ† Vars(ğ‘”) | |ğœˆ | = â„“ }|

We now explain how to compute the first term, that is,
|(SAT(ğœ‘ğ‘”1 ) âŠ— 2ğ‘†1 ) âˆ© {ğœˆ âŠ† Vars(ğ‘”) |
|ğœˆ | = â„“ }|; the second
term is similar. This is equal3 to

min(â„“, |Vars(ğ‘”1) |)
âˆ‘ï¸

ğ‘– = max(0, â„“âˆ’|ğ‘†1 |)

ğ›¼ğ‘–
ğ‘”1 Ã—

(cid:19)

.

(cid:18) |ğ‘†1|
â„“ âˆ’ ğ‘–

Decomposable âˆ§-gate. If ğ‘” is a decomposable âˆ§-gate with no
input
then ğœ‘ğ‘” is the Boolean function on variables
Vars(ğ‘”) = âˆ… that is always true, hence ğ›¼ 0
ğ‘” = 1. Otherwise,
let ğ‘”1 and ğ‘”2 be the two input gates of ğ‘”. Since ğ‘” is decompos-
able we have Vars(ğ‘”) = Vars(ğ‘”1) âˆª Vars(ğ‘”2) with the union
being disjoint. But then we have:

SAT(ğœ‘ğ‘”) = SAT(ğœ‘ğ‘”1 ) âŠ— SAT(ğœ‘ğ‘”2 )

We now intersect with the set of assignments of Vars(ğ‘”) of
size ğ‘™ to obtain

ğ›¼ â„“
ğ‘” = #SATâ„“ (ğœ‘ğ‘”) =

min(â„“, |Vars(ğ‘”1) |)
âˆ‘ï¸

ğ‘”1 Ã— ğ›¼ â„“âˆ’ğ‘–
ğ›¼ğ‘–
ğ‘”2

ğ‘– = max(0, â„“âˆ’|Vars(ğ‘”2) |)
This concludes the proof of the lemma, as well as the proof
â–¡
of Proposition 4.4.

Algorithm. Algorithm 1 depicts the solution underlying Propo-
sition 4.4. The subroutine ComputeAll#SATğ‘˜ takes as input a d-D
circuit ğ¶ and outputs all the values #SAT0 (ğ¶), . . . , #SAT|Vars(ğ¶) | (ğ¶).
This function computes values ğ›¼ â„“
ğ‘” by bottom-up induction on ğ¶ just
as in the proof of Lemma 4.5, by using the appropriate equations de-
pending on the type of each gate. Then, Lines 1â€“5 in the algorithm
simply follow the part of the proof that starts at the beginning of
this section until Lemma 4.5. For instance, the returned value on
Line 5 corresponds to Equation (3). A quick inspection of Algo-
rithm 1 reveals that, if one ignores the complexity of performing
arithmetic operations (i.e., considering that additions and multipli-
cations take constant time), the running time is ğ‘‚ (|ğ¶ | Â· |ğ·n|2). If
one wishes to compute the Shapley value of every endogenous fact
(as will be done in the experiments), then the overall complexity
is ğ‘‚ (|ğ¶ | Â· |ğ·n|3). Last, we point out that, in the case of non-Boolean
queries, this cost is incurred for each potential output tuple that
one wants to analyze.

4.2 Implementation Architecture
In this section, we present our architecture for implementing the
knowledge compilation approach over realistic datasets. The rele-
vant parts, for now, are the middle and top part of Figure 3, which
we next explain. Given a database ğ· = ğ·x âˆª ğ·n, a query ğ‘( Â¯ğ‘¥), a
tuple Â¯ğ‘¡ of the same arity as Â¯ğ‘¥, and an endogenous fact ğ‘“ âˆˆ ğ·n, we
want to compute Shapley(ğ‘[ Â¯ğ‘¥/Â¯ğ‘¡], ğ·x, ğ·n, ğ‘“ ). We use two existing
tools to help us with this task: ProvSQL [31] and the knowledge
compiler c2d [10]. ProvSQL is a tool integrated into PostgreSQL that
can perform provenance (lineage) computation in various semir-
ings. For our purposes, a knowledge compiler is a tool that takes as
input a Boolean function in CNF and outputs an equivalent Boolean
function into another formalism. The target formalism that we will
use is the so-called â€œd-DNNF". A d-DNNF is simply a deterministic

3This comes from the fact that, for disjoint ğ‘‹1, ğ‘‹2 and assignments ğœˆ1 of ğ‘‹1 and ğœˆ2
of ğ‘‹2 we have |ğœˆ1 âˆª ğœˆ2 | = |ğœˆ1 | + |ğœˆ2 |, and Vars(ğœ‘ğ‘”1 ) and ğ‘†1 are disjoint.

Algorithm 1: Shapley values from deterministic and
decomposable Boolean circuits

Input

: Deterministic and decomposable Boolean
circuit ğ¶ with output gate ğ‘”output
representing ELin(ğ‘, ğ·x, ğ·n) and an
endogenous fact ğ‘“ âˆˆ ğ·n.
Output : The value Shapley(ğ‘, ğ·x, ğ·n, ğ‘“ ).

1 Complete ğ¶ so that Vars(ğ‘”output) = ğ·n;
2 Compute ğ¶1 = ğ¶ [ğ‘“ â†’ 1] and ğ¶2 = ğ¶ [ğ‘“ â†’ 0];

// Partial evaluations of ğ¶ by setting ğ‘“ to 1
and to 0

3 Î“ = ComputeAll#SATğ‘˜ (ğ¶1);
4 Î” = ComputeAll#SATğ‘˜ (ğ¶2);

5 return

|ğ·n |âˆ’1
âˆ‘ï¸

ğ‘˜=0

ğ‘˜! (|ğ·n| âˆ’ ğ‘˜ âˆ’ 1)!
|ğ·n|!

// As an array
// As an array

Â· (Î“ [ğ‘˜] âˆ’ Î”[ğ‘˜]);

6 Def ComputeAll#SATğ‘˜ (C):
7

Preprocess ğ¶ so that each âˆ¨-gate and âˆ§-gate has
fan-in exactly 0 or 2;

8

9

10

Compute the set Vars(ğ‘”) for every gate ğ‘” in ğ¶;
Compute values ğ›¼ â„“
and â„“ âˆˆ {0, . . . , |Vars(ğ‘”)|} by bottom-up induction
on ğ¶ using the inductive relations from the proof of
Lemma 4.5;

ğ‘” for every gate ğ‘” in ğ¶

return [ğ›¼ 0

ğ‘”output

, . . . , ğ›¼ |Vars(ğ¶) |
ğ‘”output

];

and decomposable Boolean circuit such that negation gates are only
applied to variables (NNF stands for negation normal form).4

In our case, we use ProvSQL as follows: we feed it the database ğ·,
query ğ‘( Â¯ğ‘¥) and tuple Â¯ğ‘¡, and ProvSQL computes Lin(ğ‘ [ Â¯ğ‘¥/Â¯ğ‘¡], ğ·) as
a Boolean circuit, called ğ¶ in Figure 3. We note here that for SPJU
queries, ğ¶ can be computed in polynomial-time data complexity. We
then set to 1 all the exogenous facts to obtain a Boolean circuit ğ¶ â€²
for ELin(ğ‘[ Â¯ğ‘¥/Â¯ğ‘¡], ğ·x, ğ·n). Then, ideally, we would like to use the
knowledge compiler to transform ğ¶ â€² into an equivalent d-DNNF, in
order to be able to apply Algorithm 1. Unfortunately, every knowl-
edge compiler that we are aware of takes as input Boolean formulas
in conjunctive normal form (CNF), and not arbitrary Boolean cir-
cuits. To circumvent it, we use the Tseytin transformation [35] to
def
transform the circuit ğ¶ â€² into a CNF ğœ‘
= Tseytin(ğ¶ â€²), whose size
is linear in that of ğ¶ â€². This CNF ğœ‘ has the following properties:
(1) its variables are the variables of ğ¶ â€² plus a set ğ‘ of additional
variables; (2) for every valuation ğœˆ âŠ† Vars(ğ¶ â€²) that satisfies ğ¶ â€²,
there exists exactly one valuation ğœˆ â€² âŠ† ğ‘ such that ğœ‘ (ğœˆ âˆª ğœˆ â€²) = 1;
and (3) for every valuation ğœˆ âŠ† Vars(ğ¶ â€²) that does not satisfy ğ¶ â€²,
there is no valuation ğœˆ â€² âŠ† ğ‘ such that ğœ‘ (ğœˆ âˆª ğœˆ â€²) = 1. We then

4This additional NNF restriction is not important here, but, as far as we know, no
knowledge compiler has the more general â€œdeterministic and decomposable circuitsâ€
(without NNF) as a target. It is currently unknown whether d-DNNFs and deterministic
and decomposable circuits are exponentially separated or not [11, Table 7].

7

feed ğœ‘ to the knowledge compiler, which produces a d-DNNF ğ¶ â€²â€²
equivalent to ğœ‘ (the variables of ğ¶ â€²â€² are again Vars(ğ¶ â€²) âˆª ğ‘ ). We
note here that there is no theoretical guarantee that this step is
efficient; indeed, the task of transforming a CNF into an equivalent
d-D circuit is FP#P-hard in general; see Section 6 for an experimen-
tal analysis of its tractability in practice. Next, we need to eliminate
the additional variables ğ‘ in order to be able to apply Algorithm 1.
To this end, we use the following Lemma.

Lemma 4.6. Given as input a d-DNNF ğ¶ â€²â€² that is equivalent to
Tseytin(ğ¶ â€²) for a Boolean circuit ğ¶ â€², we can compute in time ğ‘‚ (|ğ¶ â€²â€²|)
a d-DNNF ğ¶ â€²â€²â€² that is equivalent to ğ¶ â€² (in particular, the variables
of ğ¶ â€²â€²â€² are the same as the variables of ğ¶ â€²; in our case, they consist
only of endogenous facts).

Proof sketch. Let ğ‘ be the additional variables coming from
the Tseytin transformation. First, we remove all the gates of ğ¶ â€²â€²
that are not satisfiable, and then we remove all the gates that are
not connected to the output gate. Now, let ğ¶ â€²â€²â€² be the circuit that is
obtained from this intermediate circuit by replacing every literal ğ‘§
or Â¬ğ‘§ for ğ‘§ âˆˆ ğ‘ by a constant 1-gate. We return ğ¶ â€²â€²â€². The proof that
this algorithm is correct uses the properties (1â€“3) of the Tseytin
â–¡
transformation, and is omitted due to lack of space.

Using this lemma, we obtain a d-DNNF ğ¶ â€²â€²â€² for the endogenous
lineage ELin(ğ‘[ Â¯ğ‘¥/Â¯ğ‘¡], ğ·x, ğ·n), to which we can finally apply Algo-
rithm 1 to obtain the value Shapley(ğ‘[ Â¯ğ‘¥/Â¯ğ‘¡], ğ·x, ğ·n, ğ‘“ ).

5 INEXACT COMPUTATION
As we will show in the experimental section, the exact computation
algorithm that we have proposed performs well in most cases but
is too costly in others. In the latter cases, we may wish to resort
to methods that do not necessarily compute exact Shapley values,
if their results still typically suffice to determine the order of facts
according to their Shapley contribution. In this section we propose
CNF Proxy, a heuristic solution that is very efficient, and we will
experimentally show that the ranking of facts based on CNF Proxy
tends to match the ranking based on the exact Shapley values.

At a high level, CNF Proxy is based on the observation that hav-
ing a high Shapley score is correlated (albeit in a complex manner)
with (1) appearing many times in the provenance and (2) having
few â€œalternatives,â€ that is, facts that could compensate for the ab-
sence of the given fact. The first factor (number of occurrences)
may be directly read from the CNF obtained by applying the Tseytin
transformation to the provenance circuit (ğ¶ â€² in Figure 3). It is also
easy to read from the CNF partial information about the second
factor (number of alternatives), namely the number of alternatives
in each clause (ignoring intricate dependencies between clauses).
Next, we present the details of CNF Proxy.

We will start with an auxiliary definition, denoting the Shapley

value of a general function â„ : 2ğ‘‹ â†’ R and a variable ğ‘¥ âˆˆ ğ‘‹ as

Shapley(â„, ğ‘¥)

def
=

âˆ‘ï¸

ğ‘† âŠ†ğ‘‹ \{ğ‘¥ }

|ğ‘† |!(|ğ‘‹ | âˆ’ |ğ‘† | âˆ’ 1)!
|ğ‘‹ |!

(cid:0)â„(ğ‘† âˆª {ğ‘¥ }) âˆ’ â„(ğ‘†)(cid:1).

Naturally, if â„ = ELin(ğ‘, ğ·n, ğ·x), i.e., the endogenous lineage, and ğ‘¥
is a fact in ğ·n, then Shapley(â„, ğ‘¥) = Shapley(ğ‘, ğ·n, ğ·x, ğ‘¥).

8

Algorithm 2: CNF Proxy

Input
Output : The value Shapley((cid:101)

: CNF ğœ‘ and a set of endogenous facts ğ·n.
ğœ‘, ğ‘¥) for each ğ‘¥ âˆˆ ğ·n.

1 ğ‘› â† |ğœ‘.ğ‘ğ‘™ğ‘ğ‘¢ğ‘ ğ‘’ğ‘  ()|;
2 ğ‘£ â† 0 |ğ·n |;
3 for ğœ“ âˆˆ ğœ‘.ğ‘ğ‘™ğ‘ğ‘¢ğ‘ ğ‘’ğ‘  () do
L â† ğœ“ .ğ‘™ğ‘–ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘™ğ‘  ();
4
ğ‘š â† |L|;
ğ‘ğ‘œğ‘  â† {â„“ âˆˆ L | â„“ is positive};
ğ‘›ğ‘’ğ‘” â† {â„“ âˆˆ L | â„“ is negative};
for â„“ âˆˆ ğ‘ğ‘œğ‘  âˆ© ğ·n do

6

7

5

8

9

10

11

12

ğ‘£ [â„“.ğ‘£ğ‘ğ‘Ÿ ()] â† ğ‘£ [â„“.ğ‘£ğ‘ğ‘Ÿ ()] +

end
for â„“ âˆˆ ğ‘›ğ‘’ğ‘” âˆ© ğ·n do

ğ‘£ [â„“.ğ‘£ğ‘ğ‘Ÿ ()] â† ğ‘£ [â„“.ğ‘£ğ‘ğ‘Ÿ ()] âˆ’

end

13
14 end
15 return ğ‘£

// As an array

1
ğ‘›ğ‘š Â·( ğ‘šâˆ’1
|ğ‘›ğ‘’ğ‘”|)

;

1
ğ‘›ğ‘š Â·( ğ‘šâˆ’1
|ğ‘ğ‘œğ‘  |)

;

Now, note that for a CNF formula ğœ‘ = (cid:211)ğ‘›

ğ‘–=1 ğœ“ğ‘– (where each ğœ“ğ‘– is
a disjunction of literals) and an assignment ğœˆ it holds that ğœ‘ (ğœˆ) =
(cid:206)ğ‘›
ğ‘–=1 ğœ“ğ‘– (ğœˆ). Instead of calculating the Shapley values of a CNF
formula ğœ‘ (which may be a hard problem), CNF Proxy computes
Shapley values with respect to a proxy function, denoted
ğœ‘. The
(cid:101)
proxy function of ğœ‘ is defined as the sum (instead of the product)
def
1
= (cid:205)ğ‘›
ğ‘›ğœ“ğ‘– (ğœˆ). Intuitively, a fact that
of the clauses of ğœ‘, i.e.,
appears in many clauses of the CNF ğœ‘ will occur in many summands
of
ğœ‘, the
(cid:101)
number of alternatives in each clause will be reflected in decreased
value of the respective summands.

ğœ‘, and when we compute Shapley values with respect to
(cid:101)

ğœ‘ (ğœˆ)
(cid:101)

ğ‘–=1

12 , 3

12 , 1

12 , 1

Example 5.1. Consider the CNF formula ğœ‘ = (ğ‘¥1 âˆ¨ ğ‘¥2) âˆ§ (ğ‘¥1 âˆ¨
ğ‘¥3 âˆ¨ ğ‘¥4). The Shapley values of ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¥4 are 7
12 re-
spectively. Note that ğ‘¥1 has the highest influence, which intuitively
may be attributed to its appearance in two clauses whereas each
other variable appears only in a single clause. The variable ğ‘¥2 has
more influence than ğ‘¥3 and ğ‘¥4, intuitively since it has less alterna-
tives. These comparative features are preserved in
ğœ‘ = (ğ‘¥1 âˆ¨ ğ‘¥2) +
(cid:101)
(ğ‘¥1 âˆ¨ ğ‘¥3 âˆ¨ ğ‘¥4), and indeed the Shapley values of ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¥4 with
3 respectively. Observe that although the
respect to
values assigned to the variables are very different from their actual
Shapley values, their order remains intact in this case.

ğœ‘ are 5
(cid:101)

6 , 1

3 , 1

2 , 1

Due to the linearity of Shapley values, their computation with
ğœ‘ is much more efficient, as implied by the following
(cid:101)

respect to
lemma (whose proof we omit for space reasons):

ğ‘–=1

Lemma 5.2. Let â„ = (cid:205)ğ‘›

1
ğ‘›ğœ“ğ‘– , where each ğœ“ğ‘– is a Boolean function
representing a disjunction of literals. Without loss of generality let
us assume that for each ğœ“ğ‘– there is no variable that appears in more
than one literal of ğœ“ğ‘– . Denote by ğ‘ğ‘– and ğ‘ğ‘– the number of positive and
negative literals in ğœ“ğ‘– respectively. Then, for every variable ğ‘¥, it holds

The Shapley value
Shapley(ğ‘[ Â¯ğ‘¥/Â¯ğ‘¡], ğ·x, ğ·n, ğ‘“ )

Database ğ· = ğ·x âˆª ğ·n
Fact ğ‘“ âˆˆ ğ·n
Query ğ‘( Â¯ğ‘¥)
Answer Â¯ğ‘¡

Input

Algorithm 1

d-DNNF ğ¶ â€²â€²â€² for ELin(ğ‘[ Â¯ğ‘¥/Â¯ğ‘¡], ğ·x, ğ·n)
(w/o extra vars)

Lemma 4.6

d-DNNF ğ¶ â€²â€² (w/ extra vars)
equivalent to ğœ‘

ProvSQL

Boolean circuit ğ¶
for Lin(ğ‘[ Â¯ğ‘¥/Â¯ğ‘¡], ğ·)

Partial eval: set
exo vars to 1

Bool. circuit ğ¶ â€² for
ELin(ğ‘[ Â¯ğ‘¥/Â¯ğ‘¡], ğ·x, ğ·n)

Tseytin
transform

CNF formula ğœ‘
(w/ extra vars)

Knowledge
compiler

Proxy
formula

The value Shapley((cid:101)
(heuristics)

ğœ‘, ğ‘“ )

Algorithm 2

ğœ‘ (w/ extra vars)
(cid:101)

Figure 3: Our implementation architecture.

(cid:205)ğ‘›

that Shapley(â„, ğ‘¥) = 1
ğ‘›
1
ï£±ï£´ï£´ï£´ï£´ï£´ï£²
(ğ‘ğ‘– +ğ‘ğ‘– ) Â·(ğ‘ğ‘– +ğ‘ğ‘– âˆ’1
âˆ’1
(ğ‘ğ‘– +ğ‘ğ‘– ) Â·(ğ‘ğ‘– +ğ‘ğ‘– âˆ’1
ï£´ï£´ï£´ï£´ï£´
0

Î¦(ğœ“ğ‘–, ğ‘¥) =

ğ‘ğ‘–

ğ‘ğ‘–

ï£³

ğ‘–=1 Î¦(ğœ“ğ‘–, ğ‘¥), where

if ğ‘¥ appears in ğœ“ğ‘– in positive form;

if ğ‘¥ appears in ğœ“ğ‘– in negative form;

)

)

otherwise.

Algorithm 2 then describes the operation of CNF Proxy, com-
puting Shapley values of
ğœ‘ according to Lemma 5.2. The input to
(cid:101)
the algorithm is a CNF formula ğœ‘ and the set of endogenous facts
ğ·n. In Line 1, CNF Proxy counts the number ğ‘› of clauses of ğœ‘, and
in Line 2 it initializes the contribution of every variable of ğœ‘ to zero.
Then, it iterates over the clauses (Line 3). For each clause, it counts
the number ğ‘š of literals, and identifies the set of positive and nega-
tive literals, ğ‘ğ‘œğ‘  and ğ‘›ğ‘’ğ‘” respectively (Lines 4â€“7). In Lines 9 and 12
we add (resp., subtract) quantities for each variable in a positive
(resp., negative) literal according to Lemma 5.2 (note that ğ‘š corre-
sponds to ğ‘ğ‘– + ğ‘ğ‘– as denoted in the lemma). Finally, in Line 15 the
algorithm returns the contribution value of each fact ğ‘¥ âˆˆ ğ·n based
ğœ‘, ğ‘¥). Observe that CNF Proxy runs in linear time in
on Shapley((cid:101)
the size of ğœ‘ (which itself, being the Tseytin transformation of the
provenance circuit ğ¶ â€² from Figure 3, is linear in ğ¶ â€²).

Example 5.3. Recall the queries ğ‘1, ğ‘2, ğ‘ and their lineages de-

picted in Figure 1. Using endogenous lineages, we have:

â€¢ ELin(ğ‘1, ğ·n, ğ·x) = ğ‘1
â€¢ ELin(ğ‘2, ğ·n, ğ·x) = (ğ‘2 âˆ§ ğ‘4) âˆ¨ (ğ‘2 âˆ§ ğ‘5) âˆ¨ (ğ‘3 âˆ§ ğ‘4) âˆ¨

(ğ‘3 âˆ§ ğ‘5) âˆ¨ (ğ‘6 âˆ§ ğ‘7)

â€¢ ELin(ğ‘, ğ·n, ğ·x) =ğ‘1 âˆ¨ (ğ‘2 âˆ§ ğ‘4) âˆ¨ (ğ‘2 âˆ§ ğ‘5) âˆ¨ (ğ‘3 âˆ§ ğ‘4) âˆ¨
(ğ‘3 âˆ§ ğ‘5) âˆ¨ (ğ‘6 âˆ§ ğ‘7)

The lineage of ğ‘1 is a CNF with a single variable, thus the contribu-
tion of ğ‘1 to ğ‘1 as computed by Algorithm 2 is 1, which is indeed
equal to Shapley(ğ‘1, ğ·n, ğ·x, ğ‘1). Applying the Tseytin transforma-
tion to the lineage of ğ‘2 introduces 6 new variables ({ğ‘§ğ‘– }6
ğ‘–=1) and
results in the following equisatisfiable CNF:

(ğ‘§1) âˆ§ (ğ‘§1 âˆ¨ ğ‘§2) âˆ§ (ğ‘§1 âˆ¨ ğ‘§3) âˆ§ (ğ‘§1 âˆ¨ ğ‘§4) âˆ§ (ğ‘§1 âˆ¨ ğ‘§5) âˆ§ (ğ‘§1 âˆ¨ ğ‘§6) âˆ§
(ğ‘§1 âˆ¨ ğ‘§2 âˆ¨ ğ‘§3 âˆ¨ ğ‘§4 âˆ¨ ğ‘§5 âˆ¨ ğ‘§6) âˆ§
(ğ‘§2 âˆ¨ ğ‘2) âˆ§ (ğ‘§2 âˆ¨ ğ‘4) âˆ§ (ğ‘§2 âˆ¨ ğ‘2 âˆ¨ ğ‘4) âˆ§ (ğ‘§3 âˆ¨ ğ‘2) âˆ§ (ğ‘§3 âˆ¨ ğ‘5) âˆ§ (ğ‘§3 âˆ¨ ğ‘2 âˆ¨ ğ‘5) âˆ§
(ğ‘§4 âˆ¨ ğ‘3) âˆ§ (ğ‘§4 âˆ¨ ğ‘4) âˆ§ (ğ‘§4 âˆ¨ ğ‘3 âˆ¨ ğ‘4) âˆ§ (ğ‘§5 âˆ¨ ğ‘3) âˆ§ (ğ‘¥5 âˆ¨ ğ‘5) âˆ§ (ğ‘§5 âˆ¨ ğ‘3 âˆ¨ ğ‘5) âˆ§
(ğ‘§6 âˆ¨ ğ‘6) âˆ§ (ğ‘§6 âˆ¨ ğ‘7) âˆ§ (ğ‘§6 âˆ¨ ğ‘6 âˆ¨ ğ‘7)

Algorithm 2 iterates over the above clauses, and computes the
contribution of the endogenous facts ğ·n over the proxy function.
Note that the facts ğ·n appear in clauses of two forms. The first form

9

= âˆ’1

1
22Â·2Â·(1
1)

âˆ’1
22Â·3Â·(2
1)

= 1
is (cid:0)ğ‘§ ğ‘— âˆ¨ ğ‘ğ‘– (cid:1); appearance in this type of clause adds
44
to the contribution of ğ‘ğ‘– . The second form is (cid:0)ğ‘§ ğ‘— âˆ¨ ğ‘ğ‘– âˆ¨ ğ‘â„ (cid:1), which
132 . Note that each of ğ‘2, ğ‘3, ğ‘4, ğ‘5 has two appear-
adds
ances in clauses of the first form, and one appearance in clauses
of the second form. Thus, according to Algorithm 2 the contri-
5
bution of ğ‘2, ğ‘3, ğ‘4, ğ‘5 is
132 â‰ˆ 0.038. In contrast, ğ‘6 and ğ‘7 each
have a single appearance in a clause of the first form and a sin-
gle appearance in a clause of the second form, thus their contri-
bution is 1
66 â‰ˆ 0.015. We note that the values calculated by Al-
gorithm 2 are very different from the actual Shapley values, as
Shapley(ğ‘2, ğ·n, ğ·x, ğ‘ğ‘– ) = 11
60 â‰ˆ 0.183 for ğ‘ğ‘– âˆˆ {ğ‘2, ğ‘3, ğ‘4, ğ‘5} and
Shapley(ğ‘2, ğ·n, ğ·x, ğ‘ğ‘– ) = 2
15 â‰ˆ 0.133 for ğ‘ğ‘– âˆˆ {ğ‘6, ğ‘7}. However,
the facts ğ‘2, ğ‘3, ğ‘4, ğ‘5, are correctly determined to be more influen-
tial than ğ‘6 and ğ‘7.

Our experimental evaluation indicates that in most cases, the
ordering of facts according to the values assigned to them by Algo-
rithm 2 agrees with the order obtained by using the actual Shapley
values. There is however no theoretical guarantee that this will
always be the case, as shown by the following example.

Example 5.4. Applying the Tseytin transformation over the lin-
eage of ğ‘ will result in a CNF similar to that obtained for ğ‘2, with
a new variable (ğ‘§7) and the new clauses (ğ‘§1 âˆ¨ ğ‘§7) âˆ§ (ğ‘§7 âˆ¨ ğ‘1) âˆ§
(ğ‘§7 âˆ¨ ğ‘1). In addition, the disjunct âˆ¨ğ‘§7 will be added to the clause
(ğ‘§1 âˆ¨ ğ‘§2 âˆ¨ ğ‘§3 âˆ¨ ğ‘§4 âˆ¨ ğ‘§5 âˆ¨ ğ‘§6). Similarly to the case of ğ‘2, the contri-
butions of ğ‘2, ğ‘3, ğ‘4, and ğ‘5 are correctly determined to be larger
than those of ğ‘6 and ğ‘7. As for ğ‘1, its contribution according to
Algorithm 2 is 0 while in fact it is the most influential fact.

6 EXPERIMENTS
Our system is implemented in Python 3.6 and using the PostgreSQL
11.10 database engine, and the experiments were performed on
a Linux Debian 14.04 machine with 1TB of RAM and an Intel(R)
Xeon(R) Gold 6252 CPU @ 2.10GHz processor. ProvSQL [31] was
used to capture the provenance. For knowledge compilation we
have used the c2d compiler [9, 10]. The source code of our imple-
mentation is available in [13].

Since no standard benchmark for our problem exists, we have
created such a benchmark of 40 queries over the TPC-H (1.4GB)
and IMDB (1.2GB) databases.The TPC-H queries are based on the
ones in [34], where we have only removed nested queries (which
ProvSQL does not handle) and aggregation operations (for which

provenance is not Boolean). The queries for the IMDB database are
based on the join queries in [19], where for each query we have
added a (last) projection operation over one of the join attributes to
make provenance more complex and thus more challenging for our
algorithms. The resulting queries are quite complex: in particular,
only 4 out of the 40 are hierarchical. See [13] for details of the
obtained queries.

6.1 Exact computation
We have evaluated our solution for exact Shapley computation,
presented in Section 4.2, on each of the 40 queries. In total, we
have obtained 95,803 output tuples along with their provenance
expressions (computed with ProvSQL). We have then transformed
each provenance expression into a d-DNNF structure using the
c2d [9, 10] knowledge compiler. In this experiment, for both the
knowledge compilation (KC) and Shapley evaluation steps we have
set a timeout of one hour (in fact, as we show below, a much shorter
timeout of 2.5 seconds typically suffices). In case the compilation
completed successfully within this timeframe, we have computed
the Shapley values using Algorithm 1. Table 1 presents the execu-
tion times of our solution for 16 representative queries; next we
will overview different aspects of the results.

Success rate. We report here the rate of successful executions.
The IMDB queries resulted in 95,636 output tuples; the KC step
completed successfully for 95,599 out of them, where all 37 failures
were the result of insufficient memory. For each of the IMDB output
tuples that were successfully compiled into a d-DNNF, we have
executed Algorithm 1; only a single execution failed in this step (due
to a timeout of one hour). Overall, the exact computation of Shapley
values was successful for 95,598 out of the 95,636 IMDB output
tuples (i.e., 99.96% success rate). The TPC-H queries resulted in 167
output tuples; the KC step has completed successfully for 141 out
of them, again all 26 failures were the result of insufficient memory.
For all TPC-H outputs that compiled successfully Algorithm 1 was
successful, yielding an overall 84.43% success rate.

Execution time. For each of the 40 queries, we have measured
the execution time of each step of the computation. First, we have
measured (in the column â€œExecution timeâ€) the execution time in
PostgreSQL, which includes provenance generation for every out-
put tuple using ProvSQL. Then, for each tuple Â¯ğ‘¡ in the output of the
query, we have measured the KC execution time and the execution
time of Algorithm 1 to compute the contribution of all input facts
with respect to the output tuple Â¯ğ‘¡. For the latter two algorithms, the
execution times varied significantly for the different output tuples,
and thus we report the execution times for different percentiles
(mean, p25, p50, p75 and p99). Observe that the computation is
typically efficient; outliers include q11d for which the execution
time of Algorithm 1 was over 96 seconds in average.

Figure 4 depicts the running time of the KC step and of the
computation of Shapley values from the d-DNNF as a function of
different features of the provenance, such as the number of facts
appearing in the provenance, the number of clauses in its CNF rep-
resentation, and the number of gates in its d-DNNF representation.

(a) KC time per #facts

(b) Alg. 1 time per #facts

(c) KC time per #CNF clauses

(d) Alg. 1 time per #CNF clauses

(e) KC time per d-DNNF size

(f) Alg. 1 time per d-DNNF size

Figure 4: Running times of knowledge compilation and com-
putation of Shapley values from the d-DNNF as function of
the number of distinct facts, CNF clauses, and d-DNNF size

depicts the running time of Algorithm 1 over 8 representative query
outputs as a function of the number of facts in the lineitem table.
Figure 5a depicts 4 representative query outputs, for which the
running time takes a few milliseconds, e.g., for â€œQ3 23426â€ (result
23426 of query ğ‘„3) and the full TPC-H dataset, the computation
of Shapley values for all relevant input facts complete in 4.3ms.
In contrast, Figure 5b depicts 4 query outputs for which the exact
computation failed to complete over the full TPC-H database. We
observe that the algorithm does succeed in these cases if we execute
the queries over subsets of the input database, though its execution
time may still be high: e.g., if we take a â€œsliceâ€ of the lineitem table
consisting of 480,097 facts, then computation of the contribution of
all input facts w.r.t. â€œQ9 ALGERIAâ€ takes 556sec.

(a) Representative outputs

(b) â€œDifficult" outputs

Scalability. To evaluate the scalability of Algorithm 1 we have
further looked at different scales of the TPC-H database. Figure 5

Figure 5: Alg. 1 running time for various TPC-H query out-
puts as function of table lineitem size

10

Table 1: Statistics on the exact computation of Shapley values for 16 representative queries

Dataset

Query

H
-
C
P
T

B
D
M
I

3
5
7
10
11
16
18
19
1a
6b
7c
8d
11a
11d
13c
15d
16a

#Joined
tables
3
6
6
4
6
3
4
2
5
5
8
7
8
8
9
9
8

#Filter
conditions
5
9
8
6
7
5
3
21
10
8
21
10
18
16
19
18
15

Execution
time [sec]
20980.71
48.67
30.57
4.72
0.13
1.25
37.31
2.04
0.25
2.61
77.33
145.10
3.20
56.99
2.44
24.25
5.56

#Output
tuples
100
5
4
10
10
10
10
1
35
1
2415
44517
10
210
14
207
173

Success
rate
100%
0%
0%
100%
100%
100%
100%
100%
100%
100%
99%
99.9%
100%
98.1%
100%
97.6%
100%

6.2 Inexact computation
As observed above, computing exact Shapley values using our solu-
tion for a given output tuple is typically fast, but may be costly or
even fail in some cases. In this section we evaluate inexact compu-
tation alternatives.

Algorithms. We compare three algorithms: CNF Proxy (Section 5)

and two existing baselines: Monte Carlo, and Kernel SHAP.

Monte Carlo. This is a well-known sampling algorithm [23]
for approximating Shapley values in general. To employ the Monte
Carlo algorithm in our setting, we feed it a provenance expression â„
containing ğ‘› distinct input facts, and a budget of ğ‘Ÿ Â· ğ‘› samples, for
some ğ‘Ÿ âˆˆ Z+. The Shapley value of each fact ğ‘“ is approximated by
sampling ğ‘Ÿ permutations (ğœ‹1, . . . , ğœ‹ğ‘Ÿ ) of the input facts, and then
(cid:17), where ğ‘†ğœ‹ğ‘–,<ğ‘“ is
(cid:16)
outputting 1
â„(ğ‘†ğœ‹ğ‘–,<ğ‘“ âˆª {ğ‘“ }) âˆ’ â„(ğ‘†ğœ‹ğ‘–,<ğ‘“ )
the coalition of all facts preceding ğ‘“ in the permutation ğœ‹ğ‘– .

ğ‘Ÿ Â· (cid:205)ğ‘Ÿ

ğ‘–=1

Kernel SHAP. Lundberg and Lee [22] have defined the notion
of SHAP values in the context of ML explainability. Given a function
â„ : Rğ‘‘ â†’ R (the model whose decisions we want to explain), a
probability distribution D on Rğ‘‘ (the inputs), and an input vector
Â¯ğ‘’ âˆˆ Rğ‘‘ , SHAP values were defined to measure the contribution of
Â¯ğ‘’â€™s features to the outcome â„( Â¯ğ‘’). To overcome the issue that â„ does
not operate over subsets of features, the notion of SHAP-score has
been defined as follows

SHAP(â„, Â¯ğ‘’, ğ‘¥)

def
=

âˆ‘ï¸

ğ‘† âŠ†ğ‘‹ \{ğ‘¥ }

|ğ‘† |!(|ğ‘‹ | âˆ’ |ğ‘† | âˆ’ 1)!
|ğ‘‹ |!

(â„ Â¯ğ‘’ (ğ‘† âˆª{ğ‘¥ })âˆ’â„ Â¯ğ‘’ (ğ‘†)),

where ğ‘‹ is the set of ğ‘‘ features, ğ‘¥ is a specific feature whose
contribution we wish to assess, and â„ Â¯ğ‘’ : 2ğ‘‹ â†’ R is defined by
def
= EÂ¯ğ‘§âˆ¼D [â„( Â¯ğ‘§) | Â¯ğ‘’ğ‘† = Â¯ğ‘§ğ‘† ], where Â¯ğ‘’ğ‘† and Â¯ğ‘§ğ‘† denote the
â„ Â¯ğ‘’ (ğ‘†)
vectors Â¯ğ‘’ and Â¯ğ‘§ restricted to the features in ğ‘†.

In [22] the authors have proposed a method for approximating
SHAP values, called Kernel SHAP. Kernel SHAP assumes feature
independence and estimates the probability by multiplying the mar-
ginal distributions (cid:206)ğ‘–âˆ‰ğ‘† Pr(ğ‘§ğ‘– ). The marginal probabilities Pr(ğ‘§ğ‘– )
in turn are estimated from a background data ğ‘‡ . To approximate
SHAP values, Kernel SHAP then samples ğ‘š coalitions ğ‘†1, . . . , ğ‘†ğ‘š of
features, and trains a linear model ğ‘” : 2ğ‘‹ â†’ R by minimizing the

11

KC execution times [sec]
p50
0.07
-
-
0.13
0.08
0.18
0.08
1.20
0.08
0.44
0.28
0.18
0.48
0.32
0.18
0.48
0.14

p75
0.08
-
-
0.14
0.08
0.33
0.18
1.20
0.13
0.44
0.63
0.29
1.14
0.78
0.28
1.28
0.20

p25
0.04
-
-
0.13
0.08
0.13
0.08
1.20
0.08
0.44
0.18
0.13
0.18
0.19
0.13
0.24
0.13

Mean
0.06
-
-
0.14
0.09
0.26
0.13
1.20
0.17
0.44
0.82
0.26
0.75
0.99
0.22
1.89
0.18

p99 Mean
0.00
0.13
-
-
-
-
0.01
0.19
0.01
0.13
0.18
0.57
0.01
0.23
156.06
1.20
5.92
2.10
0.08
0.44
24.28
9.44
0.36
1.09
23.34
2.15
96.19
5.97
0.02
0.53
70.05
10.01
0.02
0.53

Alg. 1 execution times [sec]
p50
0.00
-
-
0.01
0.00
0.03
0.01
156.06
0.01
0.08
0.05
0.02
0.13
0.06
0.01
0.30
0.01

p25
0.00
-
-
0.01
0.00
0.01
0.00
156.06
0.01
0.08
0.02
0.01
0.01
0.02
0.01
0.06
0.01

p75
0.00
-
-
0.01
0.01
0.29
0.01
156.06
0.01
0.08
0.39
0.05
1.27
0.48
0.01
3.52
0.02

p99
0.01
-
-
0.01
0.03
0.88
0.03
156.06
206.72
0.08
787.12
2.28
151.99
1650.72
0.06
1821.13
0.12

Table 2: Median (resp., mean) performance. Monte Carlo and
Kernel SHAP use 50 Â· #facts samples

Execution time
L1
L2
nDCG
Precision@5
Precision@10

Monte Carlo
0.079 (1.875)
0.439 (0.448)
0.03 (0.034)
1.0 (0.992)
1.0 (0.955)
1.0 (0.953)

Kernel SHAP
0.127 (1.978)
0.110 (0.109)
0.001 (0.002)
1.0 (0.998)
1.0 (0.961)
1.0 (0.961)

CNF Proxy
7e-4 (0.002)
0.317 (0.315)
0.010 (0.014)
1.0 (0.999)
1.0 (0.989)
1.0 (0.968)

ğ‘–=1 ğ‘¤ğ‘– Â· (ğ‘”(ğ‘†ğ‘– ) âˆ’ Ë†â„ Â¯ğ‘’ (ğ‘†ğ‘– ))2, where ğ‘¤ğ‘– is proportional
weighted loss (cid:205)ğ‘š
to the size of ğ‘†ğ‘– and Ë†â„ Â¯ğ‘’ is the estimation of â„ Â¯ğ‘’ using the feature
independence assumption. The coefficient associated with a feature
ğ‘¥ in the trained model ğ‘” is the approximated SHAP value of ğ‘¥.

We adapt Kernel SHAP to our setting and use it to approximate
the Shapley values of facts, as follows. The features are the input
facts of the database, we set â„ to be the Boolean function represent-
ing the (endogenous) provenance; the vector of interest, Â¯ğ‘’, has the
value 1 for all facts, and the background data ğ‘‡ contains a single
example with value 0 in all its entries. Note that for such Â¯ğ‘’ and ğ‘‡ ,
Kernel SHAP estimates â„ Â¯ğ‘’ (ğ‘†) as the result of applying â„ to a vector
with ones in ğ‘† features and zeros in the remaining entries.

The input of both Monte Carlo and Kernel SHAP includes a
budget of ğ‘š samples; for provenance expressions containing ğ‘› dis-
tinct facts we have experimented with ğ‘š âˆˆ {10ğ‘›, 20ğ‘›, 30ğ‘›, 40ğ‘›, 50ğ‘›}.

Accuracy metrics To evaluate the performance of the above meth-
ods we have used various metrics, specified below. All metrics were
computed with respect to the ground truth values obtained by the
knowledge compilation approach, and thus these experiments are
confined to the cases where the exact computation succeeded.

â€¢ nDCG is the normalized discounted cumulative gain score
[38], used to compare the ordering based on the inexact
solution to the ordering based on the ground truth.

â€¢ Precision@k is the number of facts that appears in the top-
ğ‘˜ of both the inexact and exact solutions, divided by ğ‘˜. This
was evaluated for ğ‘˜ âˆˆ {1, 3, 5, 10}.

â€¢ L1 and L2 are the mean absolute error and squared error,
respectively, of the results of an inexact computation method

(a) Execution time

(b) nDCG
Figure 6: Comparing various metrics for inexact methods as a function of the sampling budget; CNF Proxy does not rely on
sampling, thus remains constant for all budgets.

(c) Precision@10

(a) Running time distribution

(b) Worst case running time

(c) nDCG Distribution

(d) Worst case nDCG

(e) Precision@10 distribution

(f) Worst case Precision@10

Figure 7: Comparing inexact methods as a function the num-
ber of distinct facts in the provenance (ğ‘›). Sampling methods
(Kernel SHAP and Monte Carlo) presented with sampling
budget of ğ‘š = 20ğ‘›; different budgets led to similar trends.

with respect to the ground truth (i.e. how different the results
are from the actual Shapley values).

Execution time. Figure 6a depicts the execution time of the above
methods as a function of the sampling budget. The execution times
of Monte Carlo and Kernel SHAP are rather similar, while the CNF
Proxy method (which does not rely on sampling) is substantially
faster, with a median of 0.72 milliseconds and a mean of 2.06 mil-
liseconds for a single query output. Figures 7a and 7b depict the
distribution and worst-case running times of the three methods (for
Monte Carlo and Kernel SHAP we used a budget of 20 samples per
fact) as function of the number of distinct facts in the provenance ex-
pressions. CNF Proxy is substantially faster than its competitors: in
most cases CNF Proxy completes in few milliseconds and 4 seconds
in the worst case, whereas Monte Carlo and Kernel SHAP median
execution time for circuits with 101â€“200 distinct input facts are 59
and 62 seconds respectively and may take up to 1,539 seconds.

12

Recall our analysis of the execution time of the exact solution
in Section 6.1. The computation of Shapley values (KC plus Algo-
rithm 1) takes 27,600 seconds if performed for all output tuples of
query q8d in the IMDB dataset; for comparison, the CNF Proxy
method has (inexactly, see quality analysis below) computed the
Shapley value of the proxy functions for all query outputs within 95
seconds, which is 0.3% of the exact computation time. Surprisingly,
the execution time of the exact computation was comparable to
Monte Carlo and Kernel SHAP, and even faster for large sampling
budgets; For example, Kernel SHAP with ğ‘š = 10ğ‘› (where ğ‘› is the
number of distinct facts in the provenance) has completed computa-
tion for all output tuples of the query q8d in 16,447 seconds, which
is 59% of the exact computation time. For ğ‘š = 50ğ‘›, Kernel SHAP
required 58,161 seconds for completion of this computation, which
is 210% of the exact computation time. This means that using such
a sampling budget for Kernel SHAP is impractical in this setting,
and we will use it to obtain upper bounds on Kernel SHAPâ€™s quality.
Quality analysis. Figures 6b and 6c depict the ranking quality of
the above methods as a function of the sampling budget. The rank-
ings were compared for output tuples where the exact computation
succeeded, so we have the ground truth. Recall that CNF Proxy does
not rely on sampling, and so it remains constant throughout the
different budgets. Naturally, as the sampling budget grows, Monte
Carlo and Kernel SHAP quality improves. Most notable is Monte
Carlo improvement in terms of nDCG (Figure 6b), where its median
(resp., mean) nDCG with budget of 10 samples per fact is 0.9669
(resp., 0.9435), while with budget of 50 samples per fact it is 1.0
(resp., 0.9923). Comparison between the two sampling methods
(Monte Carlo and Kernel SHAP) reveals that Kernel SHAP is supe-
rior w.r.t all metrics, across the entire budgets range. In terms of
nDCG, all methods achieve rather high scores, but CNF Proxy per-
forms best. Indeed, the median (resp., mean) nDCG of CNF Proxy is
1.0 (resp., 0.9989), while Kernel SHAP requires 50 samples per fact
to get a median (resp., mean) nDCG of 1.0 (resp., 0.9986); as previ-
ously noted such budget leads to slower execution than that of the
exact computation. It is worth mentioning that also when looking
at nDCG@k for ğ‘˜ âˆˆ {1, 3, 5, 10}, CNF Proxy performs better than
Monte Carlo and Kernel SHAP, even when allowing a budget of 50
samples per fact. In terms of identifying the top influential facts
(i.e., Precision@k), CNF Proxy also outperforms the other methods.
For example, CNF Proxyâ€™s median (resp., mean) Precision@10 are
1.0 (resp., 0.9688), while Kernel SHAP reaches 1.0 (resp., 0.9611)
with 50 samples per fact. Similarly, CNF Proxy outperforms Monte
Carlo and Kernel SHAP in terms of Precision@k for ğ‘˜ âˆˆ {1, 3, 5}.
Finally, Table 2 zooms in on the results when fixing the budgets
of Monte Carlo and Kernel SHAP to 50 samples per fact, which

is the highest budget tested (already for this budget, computation
of Monte Carlo and Kernel SHAP is slower than for the exact
algorithm). Note that CNF Proxy is much faster then the other
methods, while still being superior in terms of ranking (nDCG and
Precision@k). As explained above, ranking of facts is indeed the use
case we recommend for CNF Proxy. Unsurprisingly, Kernel SHAP
achieves better distance from the exact Shapley values (L1 and L2),
at the cost of being slower by several orders of magnitude.

Dependency on the provenance size. Figure 7 depicts the perfor-
mance of Monte Carlo, Kernel SHAP, and CNF Proxy as a function
of the number of facts in the provenance expression. The results are
aggregated over all output tuples of all queries. Figure 7c presents
the quality of facts ranking (nDCG) as a function of the number
of distinct provenance facts. CNF Proxy performs the best, and
its quality remains steady regardless of the number of facts in the
provenance expression. For example, with 1â€“10 facts the CNF Proxy
median (resp., mean) nDCG is 1.0 (resp., 0.9999), and with 201â€“400
facts it is 0.9977 (resp., 0.9924). Kernel SHAP has a minor deteriora-
tion, where it drops from median (resp., mean) nDCG of 1.0 (resp.,
0.9998) with 1â€“10 facts to 0.9906 (resp., 0.9888) with 201â€“400 facts.
Figure 7d zooms in on the aggregated results over the worst case ex-
pressions of all output tuples for all queries, and shows that even the
worst case CNF Proxy is superior to the alternatives, and that the
error in terms of nDCG is small (0.92 in the worst case evaluated).
Figure 7e depicts the dependency of Precision@10 on the number of
provenance facts. Both Monte Carlo and Kernel SHAP suffer from
a massive drop of median (resp., mean) Precision@10, down to 0.4
(resp., 0.476) and 0.6 (resp., 0.6253) respectively with 201â€“400 facts,
while CNF Proxy remains at 0.8 (resp., 0.8293) median and mean
Precision@10. A similar trend is observed for Precision@5, whereas
for Precision@3 and Precision@1 the drop is less significant. Here
again, Figure 7f zooms in on the worst case and again shows the
superiority of CNF Proxy.

6.3 Hybrid computation
Recall that in Section 6.1 we have measured the success rate of the
exact computation: for IMDB the success rate was 99.96% and for
TPC-H the success rate was 84.43%. We saw in Section 6.2 that the
inexact method CNF Proxy is very efficient and that the ranking of
tuples based on CNF Proxy is typically close to the ranking obtained
based on the real Shapley values.

In this section we consider a hybrid approach that works as
follows. First, we start by running the exact computation, that is,
the knowledge compilation step and Algorithm 1. If the exact com-
putation completes successfully within less than ğ‘¡ seconds (where
ğ‘¡ is configurable) we return its result. Otherwise, we terminate the
exact computation and execute the inexact method CNF Proxy,
returning only a ranking of input tuples rather than their Shapley
values. Figure 8a depicts the success rate of the exact computation
given different timeouts. Note that given a timeout of 2.5 seconds,
the exact computation succeed for 98.67% of the IMDB output tu-
ples, and 83.83% for TPCH-H. Increasing the timeout has a rather
minor impact on the success rate: having a 15 seconds timeout
increases the success rate for IMDB to 99.52%, while the success
rate for TPC-H remains unchanged. Recall that having a timeout of
one hour results in success rates of 99.96% and 84.43% for IMDB and

(a) Exact computation success rate as
function of the computation timeout

(b) Mean execution time of the hy-
brid approach as a function of the
computation timeout

Figure 8: Hybrid approach performance

TPC-H respectively. Figure 8b depicts the mean execution time of
the hybrid approach as a function of the chosen timeout ğ‘¡. Observe
that given a timeout of 2.5 seconds the mean hybrid execution time
is 0.31 seconds and 0.67 seconds for IMDB and TPC-H respectively.
The mean execution time of the hybrid approach grows very mod-
erately w.r.t. the timeout for IMDB (since most cases do not reach
the timeout); it grows faster for TPC-H, where the difficult cases
for which timeout is reached have a more significant effect on the
overall mean execution time.

Main conclusions. Our experimental results indicate that for
most output tuples (98.67% for IMDB and 83.83% for TPC-H) exact
computation of Shapley values terminates within 2.5 seconds. When
it does not, we propose an alternative of ranking facts according to
CNF Proxy values, which typically only takes several milliseconds
(up to 4 seconds for an outlier case). Experimental evidence shows
that the ranking obtained via CNF Proxy is both much faster to
compute and more accurate (in terms of nDCG and Precision@k,
measured in cases where exact computation does succeed, and
so we have the ground truth) than the alternative of only using
sampling to approximate the actual Shapley values.

7 CONCLUSION
We have proposed in this paper a first practical framework for
computing the contributions of database facts in query answering,
quantified through Shapley values. The framework includes an ex-
act algorithm that computes the contribution of input facts, and
a faster algorithm that is practically effective in ranking contribu-
tions of input facts, while producing inexact Shapley values. Our
practical implementation is currently designed for SPJU queries
(that is, to the class of queries supported by ProvSQL). In addition to
these practical contributions, we have also established a theoretical
connection between the problem of computing Shapley values and
that of probabilistic query evaluation, by showing that, for every
query, the former can be reduced in polynomial time to the latter.
We leave it open to determine whether there is also a reduction
in the other direction (Open Problem 1). Other interesting direc-
tions would be to study further constructs such as aggregates and
negation, or to extend the framework to bag semantics. Concerning
bag semantics we observe that, by differentiating each copy of a
same tuple in a bag database (for instance, adding an identifier
attribute), our framework can be used as-is. Nevertheless, it would
be interesting to see how one could adapt the definitions in order
to consider fact multiplicities in a more elaborate way.

13

[22] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting
model predictions. In Advances in neural information processing systems. 4765â€“
4774.
http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-
model-predictions.pdf

[23] Irwin Mann and LS Shapley. 1960. Values for large games, IV: Evaluating the
Electoral College by Monte Carlo Techniques. The Rand Corporation. Research
Memorandum 2651 (1960). https://www.rand.org/pubs/research_memoranda/
RM2651.html

[24] Alexandra Meliou, Wolfgang Gatterbauer, Katherine F. Moore, and Dan Suciu.
2010. The complexity of causality and responsibility for query answers and
non-answers. PVLDB 4, 1 (2010), 34â€“45. https://www.vldb.org/pvldb/vol4/p34-
meliou.pdf

[25] Alexandra Meliou, Sudeepa Roy, and Dan Suciu. 2014. Causality and explanations
in databases. Proceedings of the VLDB Endowment (PVLDB) 7, 13 (2014), 1715â€“1716.
http://www.vldb.org/pvldb/vol7/p1715-meliou.pdf

[26] MikaÃ«l Monet. 2020. Solving a Special Case of the Intensional vs Extensional
Conjecture in Probabilistic Databases. In Proceedings of PODS. 149â€“163. https:
//arxiv.org/abs/1912.11864

[27] Alon Reshef, Benny Kimelfeld, and Ester Livshits. 2020. The impact of negation
on the complexity of the Shapley value in conjunctive queries. In Proceedings of
PODS. 285â€“297. https://arxiv.org/abs/1912.12610

[28] Alvin E Roth. 1988. The Shapley Value: Essays in Honor of Lloyd S. Shapley.

Cambridge University Press. http://www.library.fa.ru/files/Roth2.pdf

[29] Sudeepa Roy, Laurel J. Orr, and Dan Suciu. 2015. Explaining query answers with
explanation-ready databases. Proceedings of the VLDB Endowment (PVLDB) 9, 4
(2015), 348â€“359. http://www.vldb.org/pvldb/vol9/p348-roy.pdf

[30] Babak Salimi, Leopoldo E. Bertossi, Dan Suciu, and Guy Van den Broeck. 2016.
Quantifying causal effects on query answering in databases. In TaPP. USENIX
Association. http://web.cs.ucla.edu/~guyvdb/papers/SalimiTaPP16.pdf

[31] Pierre Senellart, Louis Jachiet, Silviu Maniu, and Yann Ramusat. 2018. Provsql:
Provenance and probability management in postgresql. Proceedings of the VLDB
Endowment (PVLDB) 11, 12 (2018), 2034â€“2037. https://hal.inria.fr/hal-01851538/
file/p976-senellart.pdf

[32] Lloyd S Shapley. 1953. A value for n-person games. Contributions to the Theory of
Games 2, 28 (1953), 307â€“317. http://www.library.fa.ru/files/Roth2.pdf#page=39
[33] Dan Suciu, Dan Olteanu, Christopher RÃ©, and Christoph Koch. 2011. Probabilistic
Databases. Morgan & Claypool. https://www.morganclaypool.com/doi/abs/10.
2200/S00362ED1V01Y201105DTM016

[34] Transaction Processing Performance Council (TPC). 2017. TPC-H benchmark.
http://www.tpc.org/tpc_documents_current_versions/pdf/tpc-h_v2.17.2.pdf
[35] Grigori S Tseitin. 1983. On the complexity of derivation in propositional calculus.
In Automation of reasoning. Springer, 466â€“483. https://link.springer.com/chapter/
10.1007/978-3-642-81955-1_28

[36] Guy Van den Broeck, Anton Lykov, Maximilian Schleich, and Dan Suciu. 2021.
On the tractability of shap explanations. In Proceedings of AAAI. https://arxiv.
org/abs/2009.08634

[37] Moshe Y Vardi. 1982. The complexity of relational query languages. In STOC.
ACM, 137â€“146. http://www.dis.uniroma1.it/~degiacom/didattica/semingsoft/
SIS05-06/materiale/1-query-congiuntive/riferimenti/vardi-1982.pdf

[38] Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei Chen, and Tie-Yan Liu. 2013.
A theoretical analysis of NDCG ranking measures. In Proceedings of COLT,
Vol. 8. 6. https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.680.490&
rep=rep1&type=pdf

[39] Bruno Yun, Srdjan Vesic, Madalina Croitoru, and Pierre Bisquert. 2018. Incon-
sistency Measures for Repair Semantics in OBDA. In IJCAI. ijcai.org, 1977â€“1983.
https://www.ijcai.org/proceedings/2018/0273.pdf

ACKNOWLEDGMENTS
This research has been partially funded by the European Research
Council (ERC) under the European Unionâ€™s Horizon 2020 research
and innovation programme (Grant agreement No. 804302). The
work of Benny Kimelfeld was supported by the Israel Science Foun-
dation (ISF), Grant 768/19, and the German Research Foundation
(DFG) Project 412400621 (DIP program).

REFERENCES
[1] Serge Abiteboul, Richard Hull, and Victor Vianu. 1995. Foundations of Databases.

Vol. 8. Addison-Wesley Reading. http://webdam.inria.fr/Alice/

[2] Marcelo Arenas, Pablo BarcelÃ³, Leopoldo Bertossi, and MikaÃ«l Monet. 2021. On
the complexity of SHAP-score-based explanations: Tractability via knowledge
https:
compilation and non-approximability results. arXiv preprint (2021).
//arxiv.org/abs/2104.08015

[3] Marcelo Arenas, Pablo BarcelÃ³, Leopoldo Bertossi, and MikaÃ«l Monet. 2021. The
tractability of SHAP-score-based explanations over deterministic and decompos-
able Boolean circuits. In Proceedings of AAAI. https://arxiv.org/abs/2007.14045
[4] Peter Buneman, James Cheney, Wang-Chiew Tan, and Stijn Vansummeren. 2008.
Curated databases. In Proceedings of PODS. 1â€“12. https://homepages.inf.ed.ac.
uk/opb/papers/inv.pdf

[5] Peter Buneman, Sanjeev Khanna, and Tan Wang-Chiew. 2001. Why and where:
A characterization of data provenance. In ICDT. Springer, 316â€“330. https://
repository.upenn.edu/cgi/viewcontent.cgi?article=1209&context=cis_papers
[6] Yingwei Cui, Jennifer Widom, and Janet L Wiener. 2000. Tracing the lineage of
view data in a warehousing environment. ACM Transactions on Database Systems
(TODS) 25, 2 (2000), 179â€“227. http://ilpubs.stanford.edu:8090/252/1/1997-3.pdf
[7] Nilesh Dalvi and Dan Suciu. 2007. Efficient query evaluation on probabilistic
databases. VLDB J. 16, 4 (2007), 523â€“544. https://homes.cs.washington.edu/
~suciu/vldbj-probdb.pdf

[8] Nilesh Dalvi and Dan Suciu. 2013. The dichotomy of probabilistic inference for
unions of conjunctive queries. Journal of the ACM (JACM) 59, 6 (2013), 1â€“87.
https://homes.cs.washington.edu/~suciu/jacm-dichotomy.pdf

[9] Adnan Darwiche. 2001. On the tractable counting of theory models and its
application to truth maintenance and belief revision. J. Applied Non-Classical
Logics 11, 1-2 (2001). https://arxiv.org/abs/cs/0003044

[10] Adnan Darwiche. 2004. New advances in compiling CNF to decomposable
negation normal form. In Proceedings of ECAI. Citeseer, 328â€“332. http://citeseerx.
ist.psu.edu/viewdoc/summary?doi=10.1.1.178.2262

[11] Adnan Darwiche and Pierre Marquis. 2002. A knowledge compilation map.
Journal of Artificial Intelligence Research 17 (2002), 229â€“264. https://arxiv.org/
abs/1106.1819

[12] Daniel Deutch, Nave Frost, and Amir Gilad. 2020. Explaining natural language
query results. The VLDB Journal 29, 1 (2020), 485â€“508. https://arxiv.org/abs/
2007.04454

[13] Daniel Deutch, Nave Frost, Benny Kimelfeld, and MikaÃ«l Monet. 2021. Shapley

for database facts source code. https://github.com/navefr/ShapleyForDbFacts.

[14] Todd J Green, Grigoris Karvounarakis, and Val Tannen. 2007. Provenance
semirings. In Proceedings of PODS. 31â€“40. https://repository.upenn.edu/cgi/
viewcontent.cgi?article=1022&context=db_research

[15] Todd J Green and Val Tannen. 2017. The semiring framework for database
https://dl.acm.org/doi/10.1145/

provenance. In Proceedings of PODS. 93â€“99.
3034786.3056125

[16] Anthony Hunter and SÃ©bastien Konieczny. 2010. On the measure of conflicts:
Shapley inconsistency values. Artificial Intelligence 174, 14 (2010), 1007â€“1026.
http://www.cril.univ-artois.fr/~konieczny/papers/aij10a.pdf

[17] Tomasz Imielinski and Witold Lipski Jr. 1984. Incomplete Information in Rela-
tional Databases. J. ACM 31, 4 (1984), 761â€“791. https://doi.org/10.1145/1634.1886
[18] Abhay Jha and Dan Suciu. 2013. Knowledge compilation meets database theory:
compiling queries to decision diagrams. Theory of Computing Systems 52, 3 (2013),
403â€“440. https://link.springer.com/article/10.1007/s00224-012-9392-5

[19] Viktor Leis, Andrey Gubichev, Atanas Mirchev, Peter Boncz, Alfons Kemper, and
Thomas Neumann. 2015. How good are query optimizers, really? Proceedings
of the VLDB Endowment 9, 3 (2015), 204â€“215. https://www.vldb.org/pvldb/vol9/
p204-leis.pdf

[20] Ester Livshits, Leopoldo E. Bertossi, Benny Kimelfeld, and Moshe Sebag. 2020. The
Shapley value of tuples in query answering. In ICDT, Vol. 155. Schloss Dagstuhl,
20:1â€“20:19. https://arxiv.org/abs/1904.08679

[21] Scott M Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M Prutkin,
Bala Nair, Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. 2020.
From local explanations to global understanding with explainable AI for trees.
Nature machine intelligence 2, 1 (2020), 2522â€“5839. https://arxiv.org/pdf/1905.
04610.pdf

14

