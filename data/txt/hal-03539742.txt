Privacy attacks for automatic speech recognition
acoustic models in a federated learning framework
Natalia Tomashenko, Salima Mdhaffar, Marc Tommasi, Yannick EstÃ¨ve,

Jean-FranÃ§ois Bonastre

To cite this version:

Natalia Tomashenko, Salima Mdhaffar, Marc Tommasi, Yannick EstÃ¨ve, Jean-FranÃ§ois Bonastre. Pri-
vacy attacks for automatic speech recognition acoustic models in a federated learning framework.
ICASSP 2022, 2022, Singapour, Singapore. ï¿¿hal-03539742v2ï¿¿

HAL Id: hal-03539742

https://hal.science/hal-03539742v2

Submitted on 24 Jan 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

Lâ€™archive ouverte pluridisciplinaire HAL, est
destinÃ©e au dÃ©pÃ´t et Ã  la diffusion de documents
scientifiques de niveau recherche, publiÃ©s ou non,
Ã©manant des Ã©tablissements dâ€™enseignement et de
recherche franÃ§ais ou Ã©trangers, des laboratoires
publics ou privÃ©s.

PRIVACY ATTACKS FOR AUTOMATIC SPEECH RECOGNITION ACOUSTIC MODELS IN A
FEDERATED LEARNING FRAMEWORK

Natalia Tomashenko1, Salima Mdhaffar1, Marc Tommasi2, Yannick Est`eve1, Jean-FrancÂ¸ois Bonastre1

1 LIA, Avignon UniversitÂ´e, France
2 UniversitÂ´e de Lille, CNRS, Inria, Centrale Lille, UMR 9189 - CRIStAL, Lille, France

ABSTRACT

This paper investigates methods to effectively retrieve speaker
information from the personalized speaker adapted neural network
acoustic models (AMs) in automatic speech recognition (ASR). This
problem is especially important in the context of federated learning
of ASR acoustic models where a global model is learnt on the server
based on the updates received from multiple clients. We propose an
approach to analyze information in neural network AMs based on a
neural network footprint on the so-called Indicator dataset. Using
this method, we develop two attack models that aim to infer speaker
identity from the updated personalized models without access to the
actual usersâ€™ speech data. Experiments on the TED-LIUM 3 corpus
demonstrate that the proposed approaches are very effective and can
provide equal error rate (EER) of 1â€“2%.

Index Termsâ€” Privacy, federated learning, acoustic models, at-

tack models, speech recognition, speaker verification

1. INTRODUCTION

Federated learning (FL) for automatic speech recognition (ASR) has
recently become an active area of research [1â€“6]. To preserve the
privacy of the usersâ€™ data in the FL framework, the model is updated
in a distributed fashion instead of communicating the data directly
from clients to a server.

Privacy is one of the major challenges in FL [7, 8]. Sharing
model updates, i.e. gradient information, instead of raw user data
aims to protect user personal data that are processed locally on de-
vices. However, these updates may still reveal some sensitive infor-
mation to a server or to a third party [9, 10]. According to recent
research, FL has various privacy risks and may be vulnerable to dif-
ferent types of attacks, i.e. membership inference attacks [11] or
generative adversarial network (GAN) inference attacks [12]. Tech-
niques to enhance the privacy in a FL framework are mainly based
on two categories [8]: secure multiparty computation [13] and dif-
ferential privacy [14]. Encryption methods [15, 16] such as fully
homomorphic encryption [16] and secure multiparty computation
perform computation in the encrypted domain. These methods in-
crease computational complexity. In a FL framework, this increase
is not so significant compared to standard centralized training, since
only the transmitted parameters need to be encrypted instead of large
amounts of data, however with an increased number of participants,
computational complexity becomes a critical issue. Differential pri-
vacy methods preserve privacy by adding noise to usersâ€™ parame-
ters [14, 17], however such solutions may degrade learning perfor-
mance due to the uncertainty they introduce into the parameters.

This work was funded by VoicePersonae and DEEP-PRIVACY (ANR-
18-CE23-0018) projects. This work was performed using HPC resources
from GENCI-IDRIS (Grant 2021-AD011013331).

Alternative methods to privacy protection for speech include dele-
tion methods [18] that are meant for ambient sound analysis, and
anonymization [19, 20] that aims to suppress personally identifiable
information in the speech signal keeping unchanged all other at-
tributes. These privacy preservation methods can be combined and
integrated in a hybrid fashion into a FL framework.

Despite the recent interest in FL for ASR and other speech-
related tasks such as keyword spotting [21, 22], emotion recogni-
tion [23], and speaker verification [24], there is a lack of research
on vulnerability of ASR acoustic models (AMs) to privacy attacks
in a FL framework. In this work, we make a step towards this direc-
tion by analyzing speaker information that can be retrieved from the
personalized AM locally updated on the userâ€™s data. To achieve this
goal, we developed two privacy attack models that operate directly
on the updated model parameters without access to the actual userâ€™s
data. Parameters of neural network (NN) personalized AMs contain
a wealth of information about the speakers [25]. In this paper, we
propose novel methods to efficiently and easily retrieve speaker in-
formation from the adapted AMs. The main idea of the proposed
methods is to use an external Indicator dataset to analyze the foot-
print of AMs on this data. Another important contribution of this
work is understanding how the speaker information is distributed in
the adapted NN AMs.

This paper is structured as follows. Section 2 briefly introduces
a considered FL framework for AM training. Section 3 describes
the privacy preservation scenario and proposes two attack models.
Experimental evaluation is presented in Section 4. We conclude in
Section 5.

2. FEDERATED LEARNING FOR ASR ACOUSTIC
MODELS

We consider a classical FL scenario where a global NN AM is
trained on a server from the data stored locally on multiple remote
devices [7]. The training of the global model is performed under
the constraint that the training speech data are stored and processed
locally on the user devices (clients), while only model updates are
transmitted to the server from each client. The global model is learnt
on the server based on the updates received from multiple clients.
The FL in a distributed network of clients is illustrated in Figure 1.
First, an initial global speech recognition AM Wg is distributed to
the group of devices of N users (speakers). Then, the initial global
model is run on every user si (i âˆˆ 1..N ) device and updated lo-
cally on the private user data. The updated models Wsi are then
transmitted to the server where they are aggregated to obtain a new
global model W âˆ—
g . Typically, the personalized updated models are
aggregated using federated averaging and its variations [13, 26].
Then, the updated global model W âˆ—
g is shared with the clients. The

process restarts and loops until convergence or after a fixed number
of rounds. The utility and training efficiency of the FL AMs have
been successfully studied in recent works [1â€“6], and these topics are
beyond the scope of the current paper. Alternatively, we focus on
the privacy aspect of this framework.

from hidden layers h on some speech data. We will refer to this
speech data as Indicator data. Note, that the Indicator data is not
related to any test or AM training data and can be chosen arbitrarily
from any speakers.

3.2.1. Attack model A1

The ASV task with the proposed attack model is performed in sev-
eral steps as illustrated in Figure 2. Let denote a set of utterances
in the Indicator dataset I as u1, . . . , uJ âˆˆ I; a sequence of vectors
in utterance uj as {u(1)
}; a set of personalized models
as Ws1 , . . . , WsN âˆˆ W; and an identifier of a hidden layer in the
global or personalized AM as h.

, . . . , u(Tj )

j

j

1. âˆ€ Wsi âˆˆ W, âˆ€ uj âˆˆ I we compute activation values from
si (uj) = {wh,t
t=1 and
t=1, and per-frame differences between

the layer h for model pairs: W h
g,j }Tj
g (uj) = {wh,t
W h
corresponding outputs:

si,j}Tj

âˆ†h

si (uj) = {âˆ†h,t

si,j}Tj

t=1,

(1)

where âˆ†h,t

si,j = wh,t

si,j âˆ’ wh,t

g,j , t âˆˆ 1..Tj.

2. For each personalized model, we compute mean and standard
si,j over all speech frames in the In-

deviation vectors for âˆ†h,t
dicator dataset I:

Âµh
si

=

(cid:80)I

j=1

(cid:80)Tj

t=1 âˆ†h,t
si,j

(cid:80)I

j=1 Tj

,

(cid:32) (cid:80)I

j=1

Ïƒh

si =

si,j âˆ’ Âµh
si

(cid:80)Tj

t=1(âˆ†h,t
(cid:80)I
j=1 Tj

(2)

(3)

(cid:33) 1

2

)2

3. For a pair of personalized models Wsi and Wsk , we compute
a similarity score Ï at hidden level h on the Indicator dataset
based on the L2-normalised Euclidean distance between the
corresponding vector pairs for means and standard deviations:

Ï(W h

si , W h

sk ) = Î±Âµ

âˆ¥Âµh
si
âˆ¥Âµh
si

âˆ’ Âµh
sk
âˆ¥2âˆ¥Âµh
sk

âˆ¥2
âˆ¥2

+ Î±Ïƒ

âˆ¥Ïƒh
âˆ¥Ïƒh

si âˆ’ Ïƒh
si âˆ¥2âˆ¥Ïƒh

sk âˆ¥2
sk âˆ¥2

,

(4)

where Î±Âµ, Î±Ïƒ are fixed parameters in all experiments.

4. Given similarity scores for all matrix pairs, we can complete

a speaker verification task based on these scores.

Fig. 2. Statistic computation for the attack model A1.

Fig. 1. Federated learning in a distributed network of clients:
1) Download of the global model Wg by clients. 2) Speaker adapta-
tion of Wg on the local devices using user private data. 3) Collection
and aggregation of multiple personalized models Ws1 ,...,WsN on
the sever. 4) Sharing the resulted model W âˆ—

g with the clients.

3. ATTACK MODELS

In this section, we describe the privacy preservation scenario and
present two attack models.

3.1. Privacy preservation scenario

Privacy preservation is formulated as a game between users who
share some data and attackers who access this data or data derived
from it and aim to infer information about the users [19]. To pre-
serve the user data, in FL, there is no speech data exchange between
a server and clients, only model updates are transmitted between the
clients and server (or between some clients). Attackers aim to attack
users using information owned by the server. They can get access to
some updated personalized models.

In this work, we assume that an attacker has access to the fol-

lowing data:

â€¢ An initial global NN AM Wg;
â€¢ A personalized model Ws of the target speaker s who is
enrolled in the FL system. The corresponding personalized
model was obtained from the global model Wg by fine-tuning
Wg using speaker data. We consider this model as enrollment
data for an attacker.

â€¢ Other personalized models of non-target and target speakers:
Ws1 ,...,WsN . We will refer to these models as test trial data.
The attackerâ€™s objective is to conduct an automatic speaker ver-
ification (ASV) task by using the enrollment data model in the form
of Ws and test trial data in the form of models Ws1 ,...,WsN .

3.2. Attack models

The motivation of the proposed approaches is based on the hypoth-
esis that we can capture information about the identity of speaker s
from the corresponding speaker-adapted model Ws and the global
model Wg by comparing the outputs of these two neural AMs taken

ServerClient 1Client 2Client Nğ‘Šğ‘”ğ‘Šğ‘”âˆ—ğ‘Šğ‘ 1ğ‘Šğ‘ 2ğ‘Šğ‘ ğ‘ğ‘Šğ‘”ğ‘Šğ‘”ğ‘Šğ‘”111222444333Pool of personalized modelsIndicator speech datağ‘Šğ‘ 1â„(ğ‘¢)ğ‘Šğ‘”â„(ğ‘¢)ğ‘Šğ‘ ğ‘ğ‘Šğ‘ 1ğ‘Šğ‘ 2ğ‘Šğ‘”ğ‘¢ğ‘¢ğ‘Šğ‘ ğ‘â„(ğ‘¢)Î”ğ‘ 1â„(ğ‘¢)Î”ğ‘ ğ‘â„(ğ‘¢)[Î¼ğ‘ 1â„, ğœğ‘ 1â„][Î¼ğ‘ ğ‘â„, ğœğ‘ ğ‘â„]â€¦3.2.2. Attack model A2

Train-G Part-1 Part-2 Indicator

For the second attack model, we train a NN model as shown in Fig-
ure 3. This NN model uses personalized and global models and the
speech Indicator dataset for training. It is trained to predict a speaker
identity provided the corresponding personalized model. When the
model is trained, we use it in evaluation time to extract speaker em-
beddings similarly to x-vectors and apply probabilistic linear dis-
criminant analysis (PLDA) [27, 28].

As shown in Figure 3, the model consists of two parts (frozen
and trained). The outputs of the frozen part are âˆ†h
si sequences of
vectors computed per utterance of the Indicator data as defined in
Formula (1). For every personalized model Wsi , we compute âˆ†h
si
for all the utterances u of the Indicator corpus; then âˆ†h
si (u) is used
as input to the second (trained) part of the NN which comprises sev-
eral time delay neural network (TDNN) layers [29] and one statisti-
cal pooling layer.

Fig. 3. Training a speaker embedding extractor for the attack
model A2.

4. EXPERIMENTS

4.1. Data

The experiments were conducted on the speaker adaptation partition
of the TED-LIUM 3 corpus [30]. This publicly available data set
contains TED talks that amount to 452 hours speech data in English
from about 2K speakers, 16kHz. Similarly to [3], we selected from
the TED-LIUM 3 training dataset three datasets: Train-G, Part-1,
Part-2 with disjoint speaker subsets as shown in Table 1. The Indi-
cator dataset was used to train an attack model. It is comprised of
320 utterances selected from all 32 speakers of test and development
datasets of the TED-LIUM 3 corpus. The speakers in the Indica-
tor dataset are disjoint from speakers in Train-G, Part-1, and Part-2.
For each speaker in the Indicator dataset we select 10 utterances.
The size of the Indicator dataset is 32 minutes. The Train-G dataset
was used to train an initial global AM Wg. Part-1 and Part-2 were
used to obtain two sets of personalized models.1

4.2. ASR acoustic models

The ASR AMs have a TDNN model architecture [29] and were
trained using the Kaldi speech recognition toolkit [31]. 40-dimen-
sional Mel-frequency cepstral coefficients (MFCCs) without cepstral

200
Duration, hours
Number of speakers
880
Number of personalized models â€”

86
736
1300

0.5
73
634
32
1079 â€”

Table 1. Data sets statistics

truncation appended with 100-dimensional i-vectors were used as
the input into the NNs. Each model has thirteen 512-dimensional
hidden layers followed by a softmax layer where 3664 triphone
states were used as targets2. The initial global model Wg was
trained using the lattice-free maximum mutual information (LF-
MMI) criterion with a 3-fold reduced frame rate as in [32]. The two
types of speech data augmentation strategies were applied for the
training and adaptation data: speed perturbation (with factors 0.9,
1.0, 1.1) and volume perturbation, as in [29]. Each model has about
13.8M parameters. The initial global model Wg was trained on the
Train-G. Personalized models Wsi were obtained by fine-tuning all
the parameters of Wg on the speakersâ€™ data from Part-1 and Part-2
as described in [3]. For all personalized speaker models, we use ap-
proximately the same amount of speech data to perform fine-tuning
(speaker adaptation) â€“ about 4 minutes per model. For most of the
speakers (564 in Part-1, 463 in Part-2) we obtained two different
personalized models (per speaker) on disjoint adaptation subsets, for
the rest speakers we have adaptation data only for one model.

4.3. Attack models

We investigate two approaches for attack models: A1 â€“ a simple ap-
proach based on the comparative statistical analysis of the NN out-
puts and the associated similarity score between personalized mod-
els, and A2 â€“ a NN based approach. For the test target trials, we
use comparisons between different personalized models of the same
speakers (564 in Part-2 and 1027 in the Part-1+Part-2), and for the
non-target trials we randomly selected 10K pairs of models from dif-
ferent speakers in a corresponding dataset.

4.3.1. Attack model A1

The first attack model was applied as described in Section 3.2.1. The
parameters Î±Âµ, Î±Ïƒ in Formula (4) equal to 1 and 10 respectively.
This model was evaluated on two datasets of personalized models
corresponding to Part-2 and combined Part-1+Part-2 datasets. The
Indicator dataset is the same in all experiments.

4.3.2. Attack model A2

For training the attack model A2, we use 1300 personalized speaker
models corresponding to 736 unique speakers from Part-1. When we
applied the frozen part of the architecture shown in Figure 3 to the
32-minute Indicator dataset for each speaker model in Part-1, we
obtained the training data with the amount corresponding to about
693h (32Ã—1300). The trained part of the NN model, illustrated in
Figure 3, has a similar topology to a conventional x-vector extrac-
tor [27]. However, unlike the standard NN x-vector extractor, that
is trained to predict speaker id-s by the input speech segment, our
proposed model learns to predict a speaker identity from the W h
s
part of a speaker personalized model. We trained 2 attack models
corresponding to the two values of parameter h âˆˆ {1, 5} â€“ a hidden

1Data partitions and scripts will be available online: https://

2Following the notation from [29], the model configuration can be de-

github.com/Natalia-T/privacy-attacks-asr

scribed as follows: {-1,0,1} Ã— 6 layers; {-3,0,3} Ã— 7 layers.

Indicator speech dataSpeaker ğ‘ embeddingTDNN layerStat poolingFrozen layersTDNNlayerğ‘Šğ‘ â„ğ‘Šğ‘”â„âˆ†ğ‘ Softmax: speaker idsPool of speaker personalized models for different speakersğ‘Šğ‘ layer in the ASR neural AMs at which we compute the activations.
Values h were chosen based on the results for the attack model A1.
The output dimension of the frozen part is 512. The frozen part is
followed by the trained part that consists of seven hidden TDNN lay-
ers and one statistical pooling layer introduced after the fifth TDNN
layer. The output is a softmax layer with the targets corresponding
to speakers in the pool of speaker personalized models (number of
unique speakers in Part-1).

4.4. Results

The attack models were evaluated in terms of equal error rate (EER).
Denoting by Pfa(Î¸) and Pmiss(Î¸) the false alarm and miss rates at
threshold Î¸, the EER corresponds to the threshold Î¸EER at which
the two detection error rates are equal, i.e., EER = Pfa(Î¸EER) =
Pmiss(Î¸EER).

Results for the attack model A1 are shown in Figure 4 for Part-2
and combined Part-1 and Part-2 datasets. Speaker information can
be captured for all values h with varying success: EER ranges from
0.86% (for the first hidden layer) up to 20.51% (for the top hidden
layer) on Part-2. For the Part-1+Part-2 we observe similar results.

To analyze the impact of each component in Formula (4) on the
ASV performance, we separately compute similarity score Ï either
using only means (Î±Ïƒ = 0) or only standard deviations (Î±Âµ = 0).
Results on the Part-2 dataset are shown in Figure 5. Black bars corre-
spond to Î±Ïƒ = 0 when only means were used to compute similarity
scores Ï between personalized models. Blue bars represent results
for Î±Âµ = 0 when only standard deviations were used to compute Ï.
Orange bars correspond to the combined usage of means and stan-
dard deviations as in Figure 4 (Î±Âµ = 1, Î±Ïƒ = 10). The impact
of each component in the sum changes for different hidden layers.
When we use only standard deviations, we observe the lowest EER
on the first layer. In case of using only means, the first layer is, on the
contrary, one of the least informative for speaker verification. For all
other layers, combination of means and standard deviations provided
superior results over the cases when only one of these components
were used. These surprising results for the first hidden layer could
possibly be explained by the fact that the personalized models in-
corporate i-vectors in their inputs, thus the speaker information can
be easily learnt at this level of the NN. We plan to investigate this
phenomena in detail in our future research.

We choose two values h âˆˆ {1, 5} which demonstrate promising
results for the model A1, and use the corresponding outputs to train
two attack models with the configuration A2. The comparative re-
sults for the two attack models are presented in Table 2. For h = 5,
the second attack model provides significant improvement in perfor-
mance over the first one and reduces EER from 7% down to 2%.
For h = 1, we could not obtain any improvement by training a NN
based attack model: the results for A2 in this case are worse than
for the simple approach A1. One explanation for this phenomenon
could be the following. The first layers of the AMs provide highly
informative features for speaker classification, however, training the
proposed NN model on these features results in overfitting because
training criterion of the NN is speaker accuracy, but not the target
EER metric, and the number of targets is relatively small, hence, the
NN overfits to classify the seen speakers in the training dataset.

Fig. 4. EER, % for the attack model A1 depending on the hidden
layer h (in Wg and Wsi ) which was used to compute outputs, eval-
uated on Part-2 and on the combined Part-1+Part-2 dataset.

Attack model

h=1

A1
A2

0.86
12.31

h=5

7.11
1.94

Table 2. EER, % evaluated on Part-2, h - indicator of a hidden layer

5. CONCLUSIONS

In this work, we focused on the privacy protection problem for ASR
AMs trained in a FL framework. We explored to what extent ASR
AMs are vulnerable to privacy attacks. We developed two attack
models that aim to infer speaker identity from the locally updated
personalized models without access to any speech data of the tar-
get speakers. One attack model is based on the proposed similarity
score between personalized AMs computed on some external Indi-
cator dataset, and another one is a NN model. We demonstrated on
the TED-LIUM 3 corpus that both attack models are very effective
and can provide EER of about 1% for the simple attack model A1
and 2% for the NN attack model A2. Another important contribu-
tion of this work is the finding that the first layer of personalized
AMs contains a large amount of speaker information that is mainly
contained in the standard deviation values computed on Indicator
data. This interesting property of NN adapted AMs opens new per-
spectives also for ASV, and in future work, we plan to use it for
developing an efficient ASV system.

Fig. 5. EER, % for the attack model A1 depending on the hidden
layer h, evaluated on Part-2 dataset. Âµ+Ïƒ â€“ both means and standard
deviations were used to compute similarity score Ï; Âµ â€“ only means;
and Ïƒ â€“ only standard deviations were used.

12345678910111213Hidden layer0.02.55.07.510.012.515.017.520.0EER,%0.868.4112.977.947.117.988.8611.9813.1714.1516.0818.9620.511.098.8413.368.027.237.668.7510.9612.4614.0315.3517.9319.28Part-2Part-1+Part-212345678910111213Hidden layer051015202530EER,%+6. REFERENCES

[1] Xiaodong Cui, Songtao Lu, and Brian Kingsbury,

â€œFeder-
ated acoustic modeling for automatic speech recognition,â€ in
ICASSP, 2021, pp. 6748â€“6752.

[2] Dimitrios Dimitriadis, Kenichi Kumatani, Robert Gmyr, et al.,
â€œA federated approach in training acoustic models.,â€ in Inter-
speech, 2020, pp. 981â€“985.

[3] Salima Mdhaffar, Marc Tommasi, and Yannick Est`eve, â€œStudy
on acoustic model personalization in a context of collaborative
learning constrained by privacy preservation,â€ in Speech and
Computer, 2021, pp. 426â€“436.

[4] Dhruv Guliani, FrancÂ¸oise Beaufays, and Giovanni Motta,
â€œTraining speech recognition models with federated learning:
A quality/cost framework,â€ in ICASSP, 2021, pp. 3080â€“3084.
[5] Dimitrios Dimitriadis, Kenichi Kumatani, Robert Gmyr, et al.,
â€œFederated transfer learning with dynamic gradient aggrega-
tion,â€ arXiv preprint arXiv:2008.02452, 2020.

[6] Wentao Yu, Jan Freiwald, SÂ¨oren Tewes, Fabien Huennemeyer,
and Dorothea Kolossa, â€œFederated learning in ASR: Not as
easy as you think,â€ arXiv preprint arXiv:2109.15108, 2021.
[7] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia
Smith, â€œFederated learning: Challenges, methods, and future
directions,â€ IEEE Signal Processing Magazine, vol. 37, no. 3,
pp. 50â€“60, 2020.

[8] Viraaji Mothukuri, Reza M Parizi, Seyedamin Pouriyeh, Yan
Huang, Ali Dehghantanha, and Gautam Srivastava, â€œA survey
on security and privacy of federated learning,â€ Future Gener-
ation Computer Systems, vol. 115, pp. 619â€“640, 2021.

[9] Jonas Geiping, Hartmut Bauermeister, Hannah DrÂ¨oge, and
â€œInverting gradientsâ€“how easy is it
arXiv preprint

Michael Moeller,
to break privacy in federated learning?,â€
arXiv:2003.14053, 2020.

[10] Nicholas Carlini, Chang Liu, Â´Ulfar Erlingsson, Jernej Kos, and
Dawn Song, â€œThe secret sharer: Evaluating and testing un-
intended memorization in neural networks,â€ in 28th Security
Symposium, 2019, pp. 267â€“284.

[11] Stacey Truex, Ling Liu, Mehmet Emre Gursoy, Lei Yu, and
Wenqi Wei, â€œDemystifying membership inference attacks in
machine learning as a service,â€ IEEE Transactions on Services
Computing, 2019.

[12] Zhibo Wang, Mengkai Song, Zhifei Zhang, Yang Song, Qian
Wang, and Hairong Qi, â€œBeyond inferring class representa-
tives: User-level privacy leakage from federated learning,â€ in
IEEE INFOCOM. IEEE, 2019, pp. 2512â€“2520.

[13] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, et al., â€œPrac-
tical secure aggregation for federated learning on user-held
data,â€ arXiv preprint arXiv:1611.04482, 2016.

[14] Cynthia Dwork, â€œDifferential privacy,â€ in International Collo-
quium on Automata, Languages, and Programming, 2006.
[15] Manas A Pathak, Bhiksha Raj, Shantanu D Rane, and Paris
Smaragdis,
â€œPrivacy-preserving speech processing: crypto-
graphic and string-matching frameworks show promise,â€ IEEE
signal processing magazine, vol. 30, no. 2, pp. 62â€“74, 2013.

[16] Paris Smaragdis and Madhusudana Shashanka, â€œA framework
for secure speech recognition,â€ IEEE Transactions on Audio,
Speech, and Language Processing, vol. 15, no. 4, pp. 1404â€“
1413, 2007.

[17] Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, and Jiayu
Zhou, â€œDifferentially private generative adversarial network,â€
arXiv preprint arXiv:1802.06739, 2018.

[18] Alice Cohen-Hadria, Mark Cartwright, Brian McFee, and
Juan Pablo Bello,
â€œVoice anonymization in urban sound
recordings,â€ in IEEE 29th International Workshop on Machine
Learning for Signal Processing (MLSP), 2019, pp. 1â€“6.
[19] Natalia Tomashenko, Brij Mohan Lal Srivastava, Xin Wang,
Emmanuel Vincent, Andreas Nautsch, Junichi Yamagishi,
Nicholas Evans, et al., â€œIntroducing the VoicePrivacy initia-
tive,â€ in Interspeech, 2020, pp. 1693â€“1697.

[20] Natalia Tomashenko, Xin Wang, Emmanuel Vincent, Jose
Patino, et al., â€œThe VoicePrivacy 2020 Challenge: Results and
findings,â€ Submitted to Computer Speech and Language, 2021.
[21] David Leroy, Alice Coucke, Thibaut Lavril, Thibault Gissel-
brecht, and Joseph Dureau, â€œFederated learning for keyword
spotting,â€ in ICASSP. IEEE, 2019, pp. 6341â€“6345.

[22] Andrew Hard, Kurt Partridge, Cameron Nguyen, Niran-
jan Subrahmanya, Aishanee Shah, Pai Zhu, Ignacio Lopez
Moreno, and Rajiv Mathews, â€œTraining keyword spotting mod-
els on non-iid data with federated learning,â€ arXiv preprint
arXiv:2005.10406, 2020.

[23] Siddique Latif, Sara Khalifa, Rajib Rana, and Raja Jurdak,
â€œFederated learning for speech emotion recognition applica-
in 2020 19th ACM/IEEE International Conference
tions,â€
on Information Processing in Sensor Networks (IPSN). IEEE,
2020, pp. 341â€“342.

[24] Filip Granqvist, Matt Seigel, Rogier van Dalen, Â´Aine Cahill,
Stephen Shum, and Matthias Paulik, â€œImproving on-device
speaker verification using federated learning with privacy,â€
arXiv preprint arXiv:2008.02651, 2020.

[25] Salima Mdhaffar et al., â€œRetrieving speaker information from
personalized acoustic models for speech recognition,â€ in Sub-
mitted to ICASSP, 2021.

[26] Brendan McMahan, Eider Moore, Daniel Ramage, et al.,
â€œCommunication-efficient learning of deep networks from de-
in Artificial intelligence and statistics.
centralized data,â€
PMLR, 2017, pp. 1273â€“1282.

[27] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel
Povey, and Sanjeev Khudanpur, â€œX-vectors: Robust DNN em-
beddings for speaker recognition,â€ in ICASSP. IEEE, 2018, pp.
5329â€“5333.

[28] Sergey Ioffe, â€œProbabilistic linear discriminant analysis,â€ in
European Conference on Computer Vision, 2006, pp. 531â€“542.
[29] Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur,
â€œA time delay neural network architecture for efficient model-
ing of long temporal contexts,â€ in Sixteenth annual conference
of the international speech communication association, 2015.
[30] FrancÂ¸ois Hernandez, Vincent Nguyen, Sahar Ghannay, Natalia
Tomashenko, and Yannick Est`eve, â€œTED-LIUM 3: twice as
much data and corpus repartition for experiments on speaker
adaptation,â€ in Speech and Computer, 2018, pp. 198â€“208.
[31] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget,
et al., â€œThe Kaldi speech recognition toolkit,â€ in ASRU. IEEE
Signal Processing Society, 2011.

[32] Daniel Povey, Vijayaditya Peddinti, Daniel Galvez, Pegah
Ghahremani, Vimal Manohar, Xingyu Na, et al.,
â€œPurely
sequence-trained neural networks for ASR based on lattice-
free MMI,â€ in Interspeech, 2016, pp. 2751â€“2755.

