Abstra: Toward Generic Abstractions for Data of Any
Model
Nelly Barret, Ioana Manolescu, Prajna Upadhyay

To cite this version:

Nelly Barret, Ioana Manolescu, Prajna Upadhyay. Abstra: Toward Generic Abstractions for Data
ï¿¿hal-
of Any Model. BDA 2022 - informal publication only, Oct 2022, Clermont-Ferrand, France.
03774599ï¿¿

HAL Id: hal-03774599

https://inria.hal.science/hal-03774599

Submitted on 11 Sep 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

Lâ€™archive ouverte pluridisciplinaire HAL, est
destinÃ©e au dÃ©pÃ´t et Ã  la diffusion de documents
scientifiques de niveau recherche, publiÃ©s ou non,
Ã©manant des Ã©tablissements dâ€™enseignement et de
recherche franÃ§ais ou Ã©trangers, des laboratoires
publics ou privÃ©s.

Abstra: Toward Generic Abstractions for Data of Any Model

Nelly Barret, Ioana Manolescu, Prajna Upadhyay
Inria & Institut Polytechnique de Paris
nelly.barret@inria.fr,ioana.manolescu@inria.fr,prajna-devi.upadhyay@inria.fr

ABSTRACT

Digital data sharing leads to unprecedented opportunities to de-
velop data-driven systems for supporting economic activities (e.g.,
e-commerce or maps for tourism), the social and political life, and
science. Many open-access datasets are RDF graphs, but others are
CSV files, Neo4J property graphs, JSON or XML documents, etc.

Potential users need to understand a dataset in order to decide if
it is useful for their goal. While some datasets come with a schema
and/or documentation, this is not always the case. Data summaries
or schema can be derived from the data, but their technical features
may be hard to understand for non-IT specialist users, or they may
overwhelm users with information.

We propose to demonstrate Abstra, a dataset abstraction system,
which (ğ‘–) applies on a large variety of data models; (ğ‘–ğ‘–) computes a
description meant for humans (as opposed to a schema meant for
a parser), akin to an Entity-Relationship diagram; (ğ‘–ğ‘–ğ‘–) integrates
Information Extraction data profiling to classify dataset content
among a set of categories of interest to the user.

1 INTRODUCTION
Open-access data being shared over the Internet has enormous
positive impact. It enables the development of new businesses, eco-
nomic opportunities and applications; it also leads to circulating
knowledge on a variety of topics, from health to education, envi-
ronment, the arts, science, news, etc.

The World Wide Web Consortiumâ€™s recommended data shar-
ing format is RDF graphs, and many datasets are shared this way.
However, in practice, other formats are also widely used. For
instance, CSV files are shared on portals such as Kaggle or the
French public portal data.gouv.fr; hundreds of millions of biblio-
graphic notices on PubMed, a leading medical scientific site, are
available in XML; JSON is increasingly used, e.g., to document the
activity of the French parliament on the websites NosDeputes.fr and
NosSenateurs.fr, on Twitter, etc. Relational databases are sometimes
shared as dumps, including schema constraints such as primary
and foreign keys, etc., or as CSV files; property graphs (PGs, in
short, such as pioneered by Neo4J) are used to share Offshore leaks,
a journalistic database of offshore companies.

Users who must decide whether to use a dataset in an application
need a basic understanding of its content and the suitability
to their need.

Towards this goal, schemas may be available to describe the
data structure, yet they have some limitations: (ğ‘–) schemas are
often unavailable for semistructured datasets (XML, JSON, RDF,
PGs). Even when a schema is supplied with or extracted from the
data, e.g., [4, 9, 16, 19]: (ğ‘–ğ‘–) schema syntactic details, such as regular
expressions, etc., are hard to interpret for non-expert users; (ğ‘–ğ‘–ğ‘–) a
schema focuses primarily on the dataset structure, not on its content.
It does not exploit the linguistic information encoded in node names,
in the string values the dataset may contain, etc.; (ğ‘–ğ‘£) schemas

employ the data producerâ€™s terminology, not the categories of interest
to users; (ğ‘£) schemas do not quantitatively reflect the dataset, whereas
knowing â€œwhat is the main content of a datasetâ€ can be very helpful
for a first acquaintance with it. Data summarization has been studied
for semistructured data models, e.g., [7, 11]. In the particular case
when the dataset is RDF, it may come with an ontology describing
the semantics of the dataset, which is a step toward lifting limitation
(ğ‘–ğ‘–ğ‘–) above; however, all the others still apply. Mining for patterns
in the dataset [13] allows to find popular motifs, e.g., items often
purchased together, or small groups of strongly connected nodes
in a graph, etc. This avoids shortcomings (ğ‘–) and (ğ‘£), but not the
others. Dataset documentation, when well-written, is most helpful
for users. However, it still suffers from limitations (ğ‘–) and (ğ‘–ğ‘£) above:
it is often lacking, and it reflects the producerâ€™s view.

We propose to demonstrate Abstra, a all-in-one system for
abstracting any relational, CSV, XML, JSON, RDF or PG dataset.
Abstra is based on the idea that any dataset comprises some records,
typically grouped in collections (which we view as sets). Records
describe entities or relationships in the classical conceptual data-
base design sense [17]; Abstra entities can have deeply nested
structure. When several collection of entities co-exist in a dataset,
relationships typically connect them. To identify the entities and
relationships, Abstra proceeds as follows.
(1). Given any dataset, Abstra models it as a graph, and identi-
fies collections of equivalent nodes, leveraging graph structural
summarization, as we describe in Section 2.
(2). Among the collections, Abstra detects the main ones, that is:
a small number of collections, each comprising records that may
be simple or very complex (i.e., with deeply nested structure), and
such that these collections, together, hold a large part of the dataset
contents. The challenge here is to detect, in the data graph, the nodes
and edges that are â€œpart ofâ€ each main collection record, and to do
so efficiently even if the graph has complex, cyclic structure. This
is addressed by introducing a notion of data weight and exploiting
it as we describe in Section 3.
(3). Abstra attempts to classify each collection of entities into
a given semantic category, such as Person, Product, Geographi-
calPosition, etc., using a set of semantic properties, some of which
we collect from well-known knowledge bases, while others can be
elicited from users. The classification leverages Information Extrac-
tion to detect the presence of entities in the text fields of the data,
then exploit them in our semantic properties (Section 4). It also uses
language models to detect proximity between the dataset and the
target categories.

Abstra outputs a natural-language, compact description of
the main, classified collections of entities, together with the possible
relationships in which they participate; this description is free of
any data model-specific details. For instance, given an XMark [18]
XML document describing an online auctions site, containing 2.3M
nodes, which, together, have 80 different labels, and are organized

{authors: [{name: "Alice", cities: [â€Parisâ€]}]}

authors

ğœ–

ğœ–

ğœ–

ğœ–

name

"Alice"

ğœ–

ğœ–

authors

ğœ–

authors.

cities
ğœ–

ğœ–

"Paris"

name

"Alice"

cities
cities.
ğœ–

"Paris"

Figure 1: JSON fragment (top), its direct tree model (bottom
left), and the model used in Abstra (bottom right).

on 124 labeled paths, Abstra returns: â€œA collection of Person en-
tities, a collection of Product, and a collection of categoryâ€ (the
latter are used to describe the items for sale). Users can explore
them and inspect their internal structure through an interactive
GUI (see Section 5).

Below, we describe the abstraction steps and outline the demon-
stration scenarios, before concluding. Abstra examples and a video
can be found at: https://team.inria.fr/cedar/projects/abstra/.
2 BUILDING A COLLECTION GRAPH
We first explain how any dataset is converted in a graph representa-
tion (Section 2.1), before partitioning it and constructing the central
tool of our method, the collection graph (Section 2.2).

2.1 Graph representation of any dataset

The graph representation we start from has been introduced in
ConnectionLens [2, 8], a graph-based heterogeneous data integra-
tion system, to which we bring some modifications. Any relational,
XML, JSON, RDF, or PG dataset is turned into a directed graph
ğº0 = (ğ‘0, ğ¸0, ğœ†0) where ğ¸0 âŠ† ğ‘0 Ã— ğ‘0 is a set of directed edges,
and ğœ†0 is a function labeling each node and edge with a string label,
that could in particular be ğœ– (the empty label).

XML trees and RDF graphs naturally map into this modeling.
JSON documents are modeled as trees. A common model, also
used in ConnectionLens, turns maps and arrays into unlabeled
nodes. Figure 1 shows a sample JSON fragment and, at left, this
tree model: the ğœ–-labeled nodes, from the top down, correspond
respectively to the outermost map, the outermost array, the inner-
most map, and the innermost array. In Abstra, we are interested
in recognizing groups of nodes that play similar roles in the dataset
(see Section 2.2), and we facilitate that by attaching them more
meaningful node names. As illustrated in Figure 1 at the bottom
right, we (ğ‘–) move the labels of edges which connect a map parent
to its children, on the child nodes; (ğ‘–ğ‘–) label the children of an array
node with label of their parent, to which we concatenate . (a dot).
In general, a nodeâ€™s label is computed from the closest non-empty
label among its ancestors nodes, and concatenating a dot whenever
going from an array to one of its children. This process ensures
that every node but the root has a non-empty label.

A CSV file leads to a tree, whose root has an edge going to a
node for each tuple; in turn, such a node has edges (labeled with the
possible CSV attribute names) going toward each attribute value.

Nelly Barret, Ioana Manolescu, Prajna Upadhyay

Figure 2: Sample normalized graph.

A relational database is similarly modeled; in the presence of a
primary key-foreign key constraint of the form â€œğ‘….ğ‘ is a foreign
key referencing ğ‘†.ğ‘â€, each node ğ‘›ğ‘Ÿ corresponding to an ğ‘… tuple has
an outgoing edge labeled ğ‘ pointing to the respective ğ‘† tuple node.
From a PG, we create a node for each PG node and for each of
its attributes, with labeled edges connecting them. We also create
a node for each PG edge, having one child node for each edge
attribute (if any). Whenever the PG contains an edge ğ‘’ from ğ‘›ğ‘1 to
ğ‘›ğ‘2, our graph has an edge from ğ‘›ğ‘1 to the node ğ‘›ğ‘’ representing ğ‘’,
and one from ğ‘›ğ‘’ to ğ‘›ğ‘2.

In ğº0, some edges have empty (ğœ–) labels, e.g., parent-child edges
in XML, while other edges are labeled. For uniformity, Abstra
transforms ğº0 into a normalized graph ğº, copying all the nodes
of ğº0 and all its ğœ–-label edges, and replacing each ğº0 edge of the form
ğ‘™
ğ‘›1
âˆ’â†’ ğ‘›2 where ğ‘™ â‰  ğœ– by two unlabeled edges ğ‘›1 â†’ ğ‘¥ğ‘™ , ğ‘¥ğ‘™ â†’ ğ‘›2
where ğ‘¥ğ‘™ is a new intermediary node labeled ğ‘™. All subsequent
Abstra steps apply on the normalized graph ğº.

Figure 2 shows a sample bibliographic data graph ğº. It depicts
three papers (one partially shown), which are published in (pIn)
conferences. The papers are written by (wb) authors, described by
their name and email. Note the inverse â€œhas writtenâ€ (hW) edges
going from authors to their papers. Author 21 is invited (inv) by
the conference organizers. As Figure 2 shows, the graph may con-
tain: (ğ‘–) nodes such as papers, whose information content is deeply
nested, and (ğ‘–ğ‘–) several cycles (in-cycle edges are shown in red).
Within each leaf (value) node, ConnectionLens extracts named
entities such as: persons (highlighted in yellow), dates (pink high-
light), emails (light blue), etc. Abstra leverages these in order to
classify the main entity collections (Section 4).
2.2 Partitioning nodes into collections
To leverage structural information present in ğº, we build a partition
P = {ğ¶ğ‘– }ğ‘– of the graph nodes ğ‘ , such that (cid:208)ğ‘– ğ¶ğ‘– = ğ‘ and the ğ¶ğ‘–
are pairwise disjoint. We say the nodes from a given set ğ¶ğ‘– are
equivalent, and call ğ¶ğ‘– an equivalence class. Many node partitioning
schemes, a.k.a. quotient summaries, exist [7]. We need a method
that is robust to heterogeneity, i.e., it can recognize the various
papers in Figure 2 even though they have heterogeneous structure,
and efficiently computed (ideally in linear time in the size of ğ¸).
For RDF graphs, we use Type Strong summarization [10], which
satisfies these requirements; it leverages RDF types when available,
but can also identify interesting equivalence classes without them.
We extend it also to PGs and graphs derived from CSV files and

Abstra: Toward Generic Abstractions for Data of Any Model

(c) In a greedy fashion, we select the main entities by repeating

the following steps:
(i) Select the collection node ğ¶ğ¸ currently having the high-

est score, as a root of a main entity;

(ii) Determine the boundary of the entity ğ¶ğ¸ : this is a con-
nected subgraph of the collection graph, containing ğ¶ğ¸ .
We consider all this subgraph as part of ğ¶ğ¸ , which will
be reported to users including all its boundary;
(iii) Update the collection graph to reflect the selection of ğ¶ğ¸
and its boundaries, and recompute the collection scores;
until a certain maximum number ğ¸ğ‘šğ‘ğ‘¥ of entities have
been selected, or these entities together cover a sufficient
fraction ğ‘ğ‘œğ‘£ğ‘šğ‘–ğ‘› of the data.

(2) Selecting relationships between the main entity col-
lections. These relationships will also be reported as part
of the abstraction (Section 3.3).

3.2 Main entity selection
We assign to each leaf node in ğº an own data weight (ğ‘œğ‘¤) equal to
the number of edges incoming that node. In tree data formats,
ğ‘œğ‘¤ is 1; in RDF, for instance, a literal that is the value of many
triples may have ğ‘œğ‘¤ > 1. We leverage this to define the ğ‘œğ‘¤ of a
leaf collection as the sum of the ğ‘œğ‘¤ of its nodes, e.g., in Figure 3,
ğ‘œğ‘¤ (title#) = 2, ğ‘œğ‘¤ (name#) = 5 etc.

For each edge ğ¶ğ‘– â†’ ğ¶ ğ‘— in the collection graph, we define the
edge transfer factor ğ‘“ğ‘—,ğ‘– as the fraction of nodes in ğ¶ ğ‘— having a
parent node in ğ¶ğ‘– ; 0 < ğ‘“ğ‘—,ğ‘– â‰¤ 1. Intuitively, ğ‘“ğ‘–,ğ‘— of ğ¶ ğ‘— â€™s weight can
also be seen as belonging to its parent ğ¶ğ‘– . For instance, there are 5
name nodes, but two belong to conferences, thus the transfer factor
from name to conf is ğ‘“ğ‘›ğ‘ğ‘šğ‘’,ğ‘ğ‘œğ‘›ğ‘“ = 2/5.

We experiment with two weight propagation methods.

â€¢ We run the PageRank [6] algorithm on the collection graph
with the edge direction inverted, so that each child node trans-
fers ğ‘“ğ‘–,ğ‘— of its weight to the parent. Initially, only leaf collec-
tions have non-zero ğ‘œğ‘¤, but successive PageRank iterations
spread their weights across the graph. We denote this method
PRğ‘œğ‘¤.

â€¢ Our second method propagates weights still backwards, but
only outside of the collection graph cycles. Specifically, we
assign to each collection a data weight ğ‘‘ğ‘¤, which on leaf
collection is initialized to ğ‘œğ‘¤, and on others, to 0. Then,
for each non-leaf ğ¶ğ‘– , and non cyclic path from ğ¶ğ‘– to a leaf
collection ğ¶ğ‘˜ , we increase ğ‘‘ğ‘¤ (ğ¶ğ‘– ) by ğ‘“ğ‘˜,ğ‘– Â·ğ‘œğ‘¤ (ğ¶ğ‘˜ ). We denote
this method propğ‘‘ğ‘¤.

For instance, using the second method, the collection author
in Figure 3 obtains ğ‘‘ğ‘¤ = 6, corresponding to 3 transferred from
mail, and 3 transferred from name. The intuition behind propğ‘‘ğ‘¤ is
that edges that are part of cycles may have a meaning closer to
â€œsymmetric relationships between entitiesâ€, than to â€œincluding a
collection in another collectionâ€™s boundaryâ€.

To determine entity boundaries, we proceed as follows:

â€¢ When using propğ‘‘ğ‘¤, we consider part of the boundary of an
entity ğ¶ğ‘– , any entity ğ¶ğ‘˜ that transferred some weight to ğ¶ğ‘– ,
and all the edges along which such transfers took place. For
instance, in Figure 3, mail and name are within the boundary
of author.

Figure 3: Sample collection graph corresponding to Figure 2.

relational databases. For graphs derived from XML or JSON as
discussed in Section 2.1, we simply partition the nodes by their
labels. On the graph in Figure 2, the equivalence classes are: {1, 40};
{6, 8, 38}; {20, 21, 23}, etc.; there is one class for each distinct label
of non-leaf nodes, and one class for each set of leaf nodes whose
parents are equivalent.

We call collection graph the graph whose nodes are the collections
ğ¶ğ‘– , and having an edge ğ¶ğ‘– â†’ ğ¶ğ‘˜ if and only if for some nodes
ğ‘›ğ‘– âˆˆ ğ¶ğ‘–, ğ‘› ğ‘— âˆˆ ğ¶ ğ‘— , ğ‘›ğ‘– â†’ ğ‘› ğ‘— âˆˆ ğ¸. Figure 3 shows the collection graph
corresponding to the graph in Figure 2. Here, in each collection, all
nodes have the same label, shown in the collection; this does not
hold in general, e.g., in a collection of RDF nodes, each node has a
different label. The label year# is used to denote the collection of
text children of the nodes from the collection with the label year
and similarly for the others whose label end in #. The dw attributes
will be discussed in Section 3.

3 IDENTIFYING THE MAIN ENTITIES TO
REPORT AND THEIR RELATIONSHIPS
Among the collections C, some are clearly better abstractions of the
dataset than others, e.g., in Figure 3, paper seems a better candidate
than its child collection year. However, one cannot simply return
â€œthe parent (or root) collection(s)â€: the collection graph may have no
root at all, if it is cyclic as in Figure 3 (red edges are part of cycles).
Even if a root collection exists, it may not be the best choice. For
instance, consider XHTML search results grouped in pages, of the
form âŸ¨topâŸ© âŸ¨pageâŸ© âŸ¨resultâŸ©...âŸ¨/resultâŸ© âŸ¨resultâŸ©... âŸ¨/resultâŸ© âŸ¨/pageâŸ© âŸ¨pageâŸ©...
âŸ¨/pageâŸ© ... âŸ¨/topâŸ©. Here, the top collection is that of pages, but the
actual data is in the results, thus, "a collection of results" is a better
abstraction.

3.1 Overview of the method

A high-level view of our method is the following (concrete details
will be provided below):

(1) Selecting the main entities (Section 3.2):

(a) We assign to each collection a weight, and to each edge in

the collection graph, a transfer factor.

(b) We propagate weights in the collection graph, based on the
weights and transfer factors, to assign to each collection
a score that reflects not only its own weight, but also its
position in the graph.

RDF type ğœ (if
typed RDF nodes)

ğ¶

ğ¶ properties

ğ‘‘ğ‘ğ‘–

P

K

ğ‘
ğ‘‘ğ‘œğ‘šğ‘ğ‘–ğ‘›(ğ‘)
ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ (ğ‘)

l

compare

ğ‘˜

ğ¶ node
labels

profile
ğœğ¶,ğ‘‘ğ‘ğ‘–

vote to
classify ğ¶
as ğ‘˜

compare

Figure 4: Entity classification outline.

Nelly Barret, Ioana Manolescu, Prajna Upadhyay

classify ğ¶
as ğœ

classify ğ¶
as winner
class ğ‘˜âˆ—

set P of semantic properties, which have known domain and range
constraints connecting them to the classes in K.

We bootstrapped our K and P by selecting six common classes
(Person, Place, Organization, Creative Work, Event and Product),
and associating them, based on WikiData and YAGO, properties
they are likely to have, or to be values of. For instance, a Person
has property birthPlace, while the value of birthPlace is a Place).
Next, we rely on GitTables [14], a repository of 1.5M tables extracted
from Github. For each attribute name encountered in a table, it pro-
vides candidate properties from DBPedia [3] and/or schema.org1; it
also provides the domain and range triples corresponding to these
properties. For instance, GitTableâ€™s entry for gender is:

â€¢ When using PRğ‘œğ‘¤, to determine the boundary of ğ¶ğ‘– , we
traverse the graph edges starting from ğ¶ğ‘– and include its
neighbor node ğ¶ ğ‘— if and only if (ğ‘–) the edge ğ¶ğ‘– â†’ ğ¶ ğ‘— has a
transfer factor of at least ğ‘“ğ‘šğ‘–ğ‘›, or (ğ‘–ğ‘–) each node from ğ¶ğ‘– has
at most one child in ğ¶ ğ‘— . The intuition for (ğ‘–ğ‘–) is that such
a child ğ¶ ğ‘— â€œcan be assimilated to an attribute of ğ¶ğ‘– â€, rather
than being â€œindependent of itâ€.

Finally, to update the graph after selecting one main entity ğ¶ğ¸ ,
each leaf collection in the boundary of ğ¶ğ¸ subtracts from its own
weight ğ‘œğ‘¤ the fraction (at most 1.0) that it propagated to ğ¶ğ¸ . For
instance, once author is selected with name in its boundary, the ğ‘œğ‘¤
of name decreases to 2. Then, the scores of all graph collections
(ğ‘‘ğ‘¤, respectively, PageRank score based on ğ‘œğ‘¤) are recomputed.

ğ¸, . . . , ğ¶ğ‘šğ‘ğ‘¥ğ¸

3.3 Relationship selection
Having selected the main entities {ğ¶1
} and their bound-
aries, every oriented path in the collection graph that goes from a
given ğ¶ğ‘–
ğ¸ is reported as a relationship. For instance,
in Figure 3, if the main entities are author (with mail and name in its
boundary) and paper (with year, title and abstract in its boundary),
hW
âˆ’âˆ’âˆ’â†’ paper,
the relationships reported are: paper

wB
âˆ’âˆ’âˆ’â†’ author, author

ğ¸ to another ğ¶ ğ‘—

ğ¸

and paper

pIn.conf.inv
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ author.

If the scores lead to reporting three main entities, the two above
entities and also conf (with name and inv in its boundary), the rela-

tionships are: paper
inv
âˆ’âˆ’â†’ author.
and conf

wB
âˆ’âˆ’âˆ’â†’ author, author

hW
âˆ’âˆ’âˆ’â†’ paper, paper

3.4 Discussion

pIn
âˆ’âˆ’âˆ’â†’ conf,

Abstra may return different results on a given dataset, depending
on the scoring method used (propğ‘‘ğ‘¤ or PRğ‘œğ‘¤), as well as the pa-
rameters: ğ¸ğ‘šğ‘ğ‘¥ and ğ‘ğ‘œğ‘£ğ‘šğ‘–ğ‘› (Section 3.1), and ğ‘“ğ‘šğ‘–ğ‘› (Section 3.2). Em-
pirically, we have used ğ¸ğ‘šğ‘ğ‘¥ âˆˆ {3, 5}, ğ‘ğ‘œğ‘£ğ‘šğ‘–ğ‘› = 0.8 and ğ‘“ğ‘šğ‘–ğ‘› = 0.3.
More generally, classical Entity-Relationship (E-R) modeling is
known to include a subjective factor, and for a given database,
several E-R models may be correct. Our focus is on not missing
any essential component of the dataset, while allowing users to limit
the amount of information through ğ¸ğ‘šğ‘ğ‘¥ , and classifying the main
entities into semantic categories, to make them as informative as
possible, as we explain below.

4 MAIN ENTITY CLASSIFICATION
To each main entity ğ¶ thus identified, we want to associate a cate-
gory from a predefined set K of semantic classes, and using also a

"id":"schema:gender", "label":"gender",
"description":"Gender of something, typically a Person,
"domain":["schema:Person","schema:SportsTeam"],
"range": ["schema:GenderType","schema:Text"]

GitTables have been populated using SHERLOCK [15], a state-of-
the-art deep learning semantic annotation technique. From GitTa-
bles, we derive 4.187 P properties; 3.687 among them have domain
information, and 3.898 have range statements.

The overall classification process is outlined in Figure 4; solid
arrows connect associated data items and trace the classification
process, while dotted arrows go from a set to one of its elements.
At the top of the figure, if the collection contains RDF resources
considered equivalent by RDFQuotient due to a common type ğœ,
we return that type, considering it is the most precise.

Otherwise, we exploit two kinds of information attached to ğ¶:
(1) We consider each data property ğ‘‘ğ‘ğ‘– that ğ¶ has, such as mail
for the author collection in Figure 3. Out of all the values that
ğ‘‘ğ‘ğ‘– takes on a node from ğ¶, we compute an entity profile
ğœğ¶,ğ‘‘ğ‘ğ‘– , reflecting the entities extracted from these values. For
instance, ğœauthor, mail states that each value of the property
mail contains an email (blue highlight in Figure 2), and that
overall, 100% of the length of these values is part of an email
entity. In general, a profile may reflect the presence of entities
of several types, which may span over only a small part of
the property values.
We compare ğ‘‘ğ‘ğ‘– and ğœğ¶,ğ‘‘ğ‘ğ‘– to each property ğ‘ âˆˆ P and
the classes in the range of ğ‘. If the property name ğ‘‘ğ‘ğ‘– is
sufficiently similar (through word embeddings) with some
property ğ‘ âˆˆ P, and that ğœğ¶,ğ‘‘ğ‘ğ‘– is similarly sufficiently simi-
lar to a class ğ‘˜ğ‘– âˆˆ K, e.g., EmailAddress, that is in the range of
ğ‘, this leads to a vote of ğ‘‘ğ‘ğ‘– for classifying ğ¶, in every classes
ğ‘˜ âˆˆ K such that ğ‘˜ is in the domain of ğ‘. Each property ğ‘‘ğ‘ğ‘–
may â€œvoteâ€ in favor of several classes, via different domain
constraints; the higher the similarity between ğ‘‘ğ‘ğ‘– and ğ‘, the
more frequent ğ‘‘ğ‘ğ‘– is on ğ¶ nodes, the fewer domain and range
constraints ğ‘ has, the stronger the vote is.

(2) The labels of ğ¶ nodes may also â€œvoteâ€ toward classifying
collection ğ¶. All ğ¶ nodes may have the same label, e.g., author,
which may resemble the name of a class, e.g., Author. Or,
ğ¶ nodes may all have different names, e.g., RDF URIs of
the form http://ns.com/Author123, from which we extract the
component after the last /, eliminate all but alphabet letters,

1https://schema.org/

Abstra: Toward Generic Abstractions for Data of Any Model

Figure 5: Sample screenshots of the Abstra GUI.

and use the result(s), weighted by their support among ğ¶
nodes, in a similar fashion.

Finally, ğ¶ is classified with the class ğ‘˜âˆ— âˆˆ K having received
the highest sum of votes. The process resembles domain inference
using RDF Schema ontology constraints, with the difference that
our â€œvotesâ€ are quantified by similarity and support, and, to keep
things simple for the users, we select a single class, the one having
the strongest support.
5 SYSTEM AND SCENARIOS
Abstra is implemented in Java, leveraging the graph creation (in-
cluding entity extraction) and Postgres-based store of Connection-
Lens [2]. All Abstra steps scale up linearly in the data size, which
we experimentally verified on datasets of up to tens of millions
of edges. The main memory needs are in TypedStrong partition-
ing, namely ğ‘‚ (|ğ‘ |); other operations are implemented in SQL and
benefit from Postgresâ€™ optimizations.

We have computed abstractions of dozens of synthetic bench-
mark datasets used in the data management literature, such as
XMark [18], BSBM [5], LUBM [12], and real-life datasets about
cooking, NASA flights, clean energy production, Nobel prizes, CSV

datasets from Kaggle, etc. varying the data model, the number of
collections and entity complexity, the presence of relations, etc.
During the demonstration, users will be able to: (ğ‘–) change the sys-
tem parameters, and see the impact on the classification results;
(ğ‘–ğ‘–) edit the semantic information K and P to influence the entity
classification; (ğ‘–ğ‘–ğ‘–) edit a set of small RDF, JSON and XML examples,
then abstract them to see the impact.

Abstractions are shown both as HTML text and as a light-
weight E-R diagram of the main entity collections and their re-
lationships. Clicking on a collection launches a node-link GUI
(JavaScript) enabling users to see records from the respective col-
lection, navigate to neighbors, etc. (see Figure 5).

6 CONCLUSION
Abstra extracts the main entities and relationships from heterogeneous-
structure datasets, leveraging Information Extraction, language
models, knowledge bases, and user input to classify the main col-
lections, so as to get close to the userâ€™s interest. The goal of our
tool is to generate, as automatically as possible, compact dataset de-
scriptions. Abstra complements schema extraction [4, 9, 16, 19] or
data profiling [1], aimed at more technical uses and users; Abstra
aims to help novice, first-time users discover and start interacting
with the data, through compact, graphical abstractions.

REFERENCES
[1] Z. Abedjan, L. Golab, F. Naumann, and T. Papenbrock. Data Profiling. Synthesis

Lectures on Data Management. Morgan & Claypool Publishers, 2018.

[2] A. C. Anadiotis, O. Balalau, C. Conceicao, H. Galhardas, M. Y. Haddad,
I. Manolescu, T. Merabti, and J. You. Graph integration of structured, semistruc-
tured and unstructured data for data journalism. Information Systems, July 2021.
[3] S. Auer et al. Dbpedia: A nucleus for a web of open data. In The Semantic Web,

pages 722â€“735, Berlin, Heidelberg, 2007. Springer Berlin Heidelberg.

[4] M. A. Baazizi, C. Berti, D. Colazzo, G. Ghelli, and C. Sartiani. Human-in-the-loop

schema inference for massive JSON datasets. In EDBT, 2020.

[5] C. Bizer and A. Schultz. The Berlin SPARQL benchmark. IJSWIS, 5(2):1â€“24, 2009.
[6] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine.

Comput. Networks, 30(1-7):107â€“117, 1998.

[7] S. Cebiric, F. GoasdouÃ©, H. Kondylakis, D. Kotzinos, I. Manolescu, G. Troullinou,
and M. Zneika. Summarizing Semantic Graphs: A Survey. The VLDB Journal,
28(3), June 2019.

[8] C. Chanial, R. Dziri, H. Galhardas, J. Leblay, M. L. Nguyen, and I. Manolescu.
ConnectionLens: Finding connections across heterogeneous data sources (demon-
stration). PVLDB, 11(12), 2018.

[9] D. Colazzo, G. Ghelli, and C. Sartiani. Schemas for safe and efficient XML

processing. In ICDE. IEEE Computer Society, 2011.

[10] F. GoasdouÃ©, P. Guzewicz, and I. Manolescu. RDF graph summarization for

first-sight structure discovery. The VLDB Journal, 29(5), Apr. 2020.

[11] R. Goldman and J. Widom. Dataguides: Enabling query formulation and opti-

mization in semistructured databases. In VLDB, 1997.

[12] Y. Guo, Z. Pan, and J. Heflin. Lubm: A benchmark for owl knowledge base

systems. Journal of Web Semantics, 3(2-3):158â€“182, 2005.

[13] J. Han, M. Kamber, and J. Pei. Data mining concepts and techniques, third edition.

Morgan Kaufmann Publishers, 2012.

[14] M. Hulsebos, Ã‡. Demiralp, and P. Groth. Gittables: A large-scale corpus of

relational tables. CoRR, abs/2106.07258, 2021.

[15] M. Hulsebos, K. Hu, M. Bakker, E. Zgraggen, A. Satyanarayan, T. Kraska, Ã‡. Demi-
ralp, and C. A. Hidalgo. Sherlock: A Deep Learning Approach to Semantic Data
Type Detection. SIGKDD explorations : newsletter of the Special Interest Group
(SIG) on Knowledge Discovery & Data Mining, May 2019.

[16] H. Lbath, A. Bonifati, and R. Harmer. Schema inference for property graphs. In

EDBT. OpenProceedings.org, 2021.

[17] R. Ramakhrishnan and J. Gehrke. Database Management Systems (3rd edition).

McGraw-Hill, 2003.

[18] A. Schmidt, F. Waas, M. L. Kersten, M. J. Carey, I. Manolescu, and R. Busse. Xmark:

A benchmark for XML data management. In PVLDB, 2002.

[19] W. Spoth, O. A. Kennedy, Y. Lu, B. Hammerschmidt, and Z. H. Liu. Reducing

ambiguity in JSON schema discovery. In SIGMOD, 2021.

