Facilitating Heterogeneous Dataset Understanding
Nelly Barret

To cite this version:

Nelly Barret. Facilitating Heterogeneous Dataset Understanding. BDA 2021 - informal publication
only, Oct 2021, Paris, France. ï¿¿hal-03344102ï¿¿

HAL Id: hal-03344102

https://hal.science/hal-03344102

Submitted on 14 Sep 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

Lâ€™archive ouverte pluridisciplinaire HAL, est
destinÃ©e au dÃ©pÃ´t et Ã  la diffusion de documents
scientifiques de niveau recherche, publiÃ©s ou non,
Ã©manant des Ã©tablissements dâ€™enseignement et de
recherche franÃ§ais ou Ã©trangers, des laboratoires
publics ou privÃ©s.

Facilitating Heterogeneous Dataset Understanding

Nelly Barret
Inria & Institut Polytechnique de Paris
nelly.barret@inria.fr

ABSTRACT
The era of Big Data and data sharing has lead to very large volumes
of data becoming available to users across the world. This data
is heterogeneous in its modelling, format and quality. Taking full
advantage of such data raises many challenges, in particular related
to the integration and the understanding of such data. My PhD
thesis, started in January 2021, seeks to develop novel methods to
help users without advanced IT skills discover a new dataset, by
(ğ‘–) building an abstract understanding of the data, as consisting
of records and collections, (ğ‘–ğ‘–) interpreting or classifying the data
based on usersâ€™ interests, and leveraging Information Extraction
and Natural Language tools.

1 INTRODUCTION
Since the last decades, the Open Data Initiative has led to an in-
creasing number of publicly-spread datasets. Such datasets are often
quite large and heterogeneous (depending on the source provider,
the field, the kind of data, etc). Many such datasets are large; further,
they are extremely heterogeneous, in particular for what concerns
their data model (RDF, JSON, XML, CSV, property graphs, relational
databases, etc.), their schema (if a schema exists), etc. The scale
and heterogeneity make it challenging for human users to identify,
among the many available datasets, those that could be used for a
given application they have in mind.

This thesis is part of the ConnectionLens project [1], which aims
at integrating heterogeneous data into a graph. Our goal is to create
small expressive descriptions of what a dataset is about, using the
power of integration of ConnectionLens. In this paper, we present
the challenges (Section 2), then the approach (Section 3) and finally
some preliminary results (Section 4) before concluding (Section 5).
2 CHALLENGES
Finding the right dataset is complicated, especially because they
are often not well-documented and it can be difficult to appreciate
how it can be useful. Our approach, which aims at helping users to
choose a dataset, should satisfy the following requirements:

â€¢ R1: The approach should be applicable to any kind of
data. There are various data formats, such as RDF (as in the
Open Data Cloud), but also XML (as in the PubMed database),
JSON (most of French open data), relational databases, and
so on. This requirement is handled by Section 3.1.

â€¢ R2: The data descriptions we build for users should be
sufficiently expressive, but also compact. Users need
to understand what is inside a dataset, but when a full de-
scription is complex, we need to bring them only the most
important facts about it. We discuss how to fulfil this require-
ment in Sections 3.2 and 3.3.

3 APPROACH
ConnectionLens is a system capable to produce a graph ğº from any
dataset of any format, where each node is a piece of data and edges

link these nodes to reflect the content of the original source. More-
over, an entity extraction process is applied on text nodes, to extract
from them named entities, such as Person, Location, Organization,
Date, etc. In my thesis, to be able to produce compact descriptions of
any data format, we leverage ConnectionLens to start our summa-
rization method from the graph ğº. Our approach is the following:
(1) Build a structural summary of ğº. The structural sum-
mary ğº â€² is a graph computed out of ğº, potentially much
smaller than ğº, and which gives us a first idea of groups
of nodes that may contain similar information: each such
group of ğº nodes is represented by a single node in ğº â€².
(2) Find collections and records. Starting from the summary,
we seek to identify the nodes that represent records, that is,
objects of a certain â€œkindâ€ with some internal structure, and
collections, that is, containers of potentially many records of
the same â€œkindâ€.

(3) Categorize collections. Finally, we aim at classifying collec-
tions among a set of categories K, containing (ğ‘–) the kind(s)
of data that the user is looking for, if the user can formu-
late such a request, e.g., â€œBooksâ€, or â€œPlaces to visitâ€, and/or
(ğ‘–ğ‘–) a set of generic categories we pre-define, such as Person,
Organization, Location, Event and Creative work. The cate-
gorization adds a limited form of semantics (we keep things
simple on purpose since we assume non-technical users),
and enable adapting to the usersâ€™ interest.

3.1 Summarization
We explain now how we compute the summary ğº â€² of ğº. For effi-
ciency, we distinguish two cases: rooted, acyclic data source graphs,
vs. the general case where graphs may have cycles and/or may not
have a root.

Rooted acyclic graphs. These graphs are obtained for instance
from XML or JSON datasets. On such graphs, we apply the strong
DataGuide summarization method [4] to create ğº â€² from ğº. A Dataguide
is a concise summary of the structure of a database. This method
builds a set of paths, such that each path of the DAG appears exactly
once in the summary. Such summarization method works only on
acyclic graphs because the recursion should not encounter a cycle.
General graphs. Such graphs can originate in RDF, property
graphs, or relational database datasets (where primary-foreign keys
can lead to cyclic connections between the tuples). For such graphs,
we need a graph summarization method that (ğ‘–) reflects all the
graph, (ğ‘–ğ‘–) groups nodes into equivalence classes and (ğ‘–ğ‘–ğ‘–) can be
computed efficiently even from large graphs. RDFQuotient [3], orig-
inally introduced for RDF but easy to adapt to arbitrary graphs,
meets these criteria, thus we rely on it to compute the summary ğº â€²
of ğº for non-acyclic graphs. RDFQuotient gives a set of equivalence
classes between nodes based on their types and their properties.

3.2 Records and Collections
We seek to understand ğº â€² based on two key concepts:

â€¢ A Record is basically a thing; in data modelling terms, it
describes either an entity or a relationship. It has some prop-
erties (e.g. a title and a DOI for a paper) and can handle
nested collections (e.g. the authors list of a paper).

â€¢ A Collection is a set of similar records (e.g. a bibliography is
a collection of books). They are explicit when a node handles
similar records; or implicit when some records refer to the
same purpose without being handled by a node.

Other nodes in ğº â€² are called Sub-Records and are mainly the
properties of the records (i.e. the set of outgoing properties of a
record ğ‘Ÿ , referred as ğ‘Ÿ .P). Furthermore, we compute the signature
of each sub-record ğ‘ , where the signature is compound of a domain
(â€œto which categories ğ‘  belongs to?") and a range (â€œto which categories
ğ‘  points to?"). For example, the sub-record settledDownIn has for
domain {Person, Organization} and for range {Location}.

To find them, we first determine collections and then, in a top-
down fashion, the direct children of collections are identified as
records. To compute collections, we rely on a clustering algorithm
we devised, based on the support of a set of properties among
a set of potential records (how many of these records have this
set of properties). Our clustering algorithm identifies both explicit
collections, where a ğº â€² node is actually the parent of all the nodes
representing the records in the collection, and implicit collections,
where such a common parent/collection node does not exist in ğº â€².

3.3 Analysis and Categorization of Collections
Given a set of hints H and a set of user-defined categories K, we
aim at categorizing a collection ğ‘ among K, i.e. give a category
ğ‘˜ âˆˆ K to ğ‘ using H , as illustrated by Algorithm 1. A hint â„ is a
triple âŸ¨ğ´, ğ‘™, ğµâŸ© where A is the domain âŠ† K, ğ‘™ is the label and ğµ is the
range âŠ† K. For instance, the hint âŸ¨Organization, hasCEO, PersonâŸ©
states that a collection having a record holding the property hasCEO,
whose signatureâ€™s range matches Person, should be categorized as
an Organization.

For each record ğ‘Ÿ âˆˆ ğ‘, we initialize Kğ‘Ÿ (set of candidate categories
in which ğ‘Ÿ may belong) and scores (score of ğ‘›ğ‘ for each hint in H ).
Then, if ğ‘Ÿ has a label semantically close to one of the category in K,
this category is stored as a candidate category in Kğ‘Ÿ . For each child
ğ‘›ğ‘ âˆˆ ğ‘Ÿ , we create a pair ğœ‹ containing the label and the signature
of ğ‘›ğ‘. Then, we compute the similarity of ğœ‹ with each hint â„ in H ,
where the similarity is based on the label and the signature of both
elements. We choose the hint â„ leading to the highest similarity
score for each ğœ‹. Each category indicated by the domain of â„ gets
a vote. Then, we classify ğ‘Ÿ in the category that gets the highest
number of votes or Other if no category is frequent enough. Finally,
we classify ğ‘ in the most represented category in its records. We
also determine if a collection describes entities or relationships, by
looking at the connections between the collections.
4 STATUS
We have fully implemented our approach in a prototype, which
leverages the graph creation and storage of ConnectionLens [1],
and includes the novel algorithms described in Section 3. More
details can be found in a short paper [2].

Figure 1 shows an example of our approach applied on a set of
PubMed articles. The set of articles is considered as a collection of
Creative Work. Moreover, the authors are identified as a collection
of Persons.

Nelly Barret

Algorithm 1: Classifying a collection ğ‘
Input: a collection ğ‘, hints H, categories K
Output: a category ğ‘˜ âˆˆ K or Other

1 foreach ğ‘Ÿ âˆˆ C do
2

Kğ‘Ÿ â† âˆ…
scores â† âˆ…
foreach ğ‘˜ âˆˆ K do

if the similarity between ğ‘˜ and the label of ğ‘Ÿ is higher than a threshold then

Kğ‘Ÿ â† Kğ‘Ÿ âˆª {ğ‘˜ }

3

4
5
6

7
8

9
10

11

12

foreach ğ‘›ğ‘ âˆˆ ğ‘Ÿ .children do

ğœ‹ â† (ğ‘›ğ‘.ğ‘™ğ‘ğ‘ğ‘’ğ‘™, ğ‘›ğ‘.ğ‘ ğ‘–ğ‘”ğ‘›ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’)
foreach â„ âˆˆ H do

scores â† scores (cid:208)(â„, ğ‘ ğ‘–ğ‘š (ğœ‹, â„))

bestHint â† argmax(scores)
Kğ‘Ÿ â† Kğ‘Ÿ âˆª { bestHint.domain }

Classify ğ‘Ÿ in the most frequent ğ‘˜ âˆˆ Kğ‘Ÿ , or Other
13
14 Classify ğ‘ with the most frequent category of its records

Figure 1: Example of ğº â€², an abstract graph with collections
and categorized records.

5 CONCLUSION AND PERSPECTIVES
My PhD thesis aims to create expressive descriptions of big het-
erogeneous datasets by using summarization methods and catego-
rization of expressive structures (records and collections). Beyond
finalizing the implementation of our platform for all the data models
we consider (notably, beyond XML and RDF, also JSON and prop-
erty graphs), we will experiment to analyse its scalability as well as
the expressiveness and precision of the record categorization. Next,
we will investigate the adoption of sampling-based approaches, to
try to construct such dataset descriptions without traversing the
dataset entirely, in order to further improve performance.
Thesis context My PhD is funded by DIM RFSI and is a collab-
oration between Inria and WeDoData, a SME specialized in data
visualization and interactive data-driven Web content. My PhD ad-
visers are Ioana Manolescu (Inria) and Karen Bastien (WeDoData).
Acknowledgments. This work is funded by DIM RFSI PHD 2020-
01 and AI Chair SourcesSay project (ANR-20-CHIA-0015-01) grants.

REFERENCES
[1] A. C. Anadiotis, O. Balalau, C. Conceicao, H. Galhardas, M. Y. Haddad, I. Manolescu,
T. Merabti, and J. You. Graph integration of structured, semistructured and un-
structured data for data journalism. Information Systems, July 2021.

[2] N. Barret, I. Manolescu, and P. Upadhyay. Toward generic abstractions for data of

any model. 2021. Short paper, accepted for publication at BDA 2021.

[3] F. GoasdouÃ©, P. Guzewicz, and I. Manolescu. RDF graph summarization for first-

sight structure discovery. The VLDB Journal, 29(5), Apr. 2020.

[4] R. Goldman and J. Widom. Dataguides: Enabling query formulation and optimiza-

tion in semistructured databases. In VLDB, 1997.

