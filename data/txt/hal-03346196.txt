Benchmarking and challenges in security and privacy for
voice biometrics
Jean-Francois Bonastre, Hector Delgado, Nicholas Evans, Tomi Kinnunen,

Kong Aik Lee, Xuechen Liu, Andreas Nautsch, Paul-Gauthier Noe, Jose

Patino, Md Sahidullah, et al.

To cite this version:

Jean-Francois Bonastre, Hector Delgado, Nicholas Evans, Tomi Kinnunen, Kong Aik Lee, et al..
Benchmarking and challenges in security and privacy for voice biometrics. SPSC 2021, 1st ISCA Sym-
posium on Security and Privacy in Speech Communication, ISCA, Nov 2021, Magdeburg, Germany.
Ôøø10.21437/SPSC.2021-11Ôøø. Ôøøhal-03346196Ôøø

HAL Id: hal-03346196

https://hal.science/hal-03346196

Submitted on 10 Apr 2024

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L‚Äôarchive ouverte pluridisciplinaire HAL, est
destin√©e au d√©p√¥t et √† la diffusion de documents
scientifiques de niveau recherche, publi√©s ou non,
√©manant des √©tablissements d‚Äôenseignement et de
recherche fran√ßais ou √©trangers, des laboratoires
publics ou priv√©s.

Benchmarking and challenges in security and privacy for voice biometrics

Jean-Francois Bonastre, H¬¥ector Delgado, Nicholas Evans, Tomi Kinnunen,
Kong Aik Lee, Xuechen Liu, Andreas Nautsch, Paul-Gauthier No¬¥e, Jose Patino,
Md Sahidullah, Brij Mohan Lal Srivastava, Massimiliano Todisco,
Natalia Tomashenko, Emmanuel Vincent, Xin Wang, Junichi Yamagishi

ASVspoof and VoicePrivacy organising committees
organisers@asvspoof.org, organisers@voiceprivacychallenge.org

Abstract

For many decades, research in speech technologies has fo-
cused upon improving reliability. With this now meeting user
expectations for a range of diverse applications, speech tech-
nology is today omni-present. As result, a focus on security
and privacy has now come to the fore. Here, the research effort
is in its relative infancy and progress calls for greater, multi-
disciplinary collaboration with security, privacy, legal and ethi-
cal experts among others. Such collaboration is now underway.
To help catalyse the efforts, this paper provides a high-level
overview of some related research. It targets the non-speech au-
dience and describes the benchmarking methodology that has
spearheaded progress in traditional research and which now
drives recent security and privacy initiatives related to voice
biometrics. We describe: the ASVspoof challenge relating to
the development of spooÔ¨Ång countermeasures; the VoicePrivacy
initiative which promotes research in anonymisation for privacy
preservation.

1. Introduction

The voice is among the most natural and convenient means to
In
human-machine interaction and biometric authentication.
some scenarios, particularly telephony or teleconferencing ap-
plications, voice can be the only available biometric. While au-
tomatic speaker veriÔ¨Åcation (ASV) systems can provide a reli-
able means to authentication, like all biometric technologies, it
is not without security and privacy concerns. Arguably, these
concerns are potentially greater for voice biometrics than they
can be for authentication systems that use alternative biometric
characteristics.

Security concerns relate to the potential for ASV systems
to be manipulated by adversaries through spooÔ¨Ång attacks [1],
now referred to as presentation attacks [2]. Fraudsters can
launch spooÔ¨Ång attacks to gain illegitimate access to protected
services or resources by presenting to the ASV system a speech
recording which has been manipulated to sound1 like another
speaker. Without adequate protection, spooÔ¨Ång attacks can sub-
stantially degrade the reliability of almost any ASV system.

While not related speciÔ¨Åcally to voice biometrics, but to
speech technology more generally, privacy concerns relate to
the potential for speech data to be exploited for purposes other
than those to which an individual might have given consent [3].
Speech signals are a rich source of personal, private informa-
In providing recordings of speech to a particular voice
tion.

1We refer to machine perception rather than human perception; hu-

mans and machines do not hear in the same way.

service, the speaker usually furnishes the service provider with
much more information than is strictly necessary in order to per-
form the expected task, hence the need for privacy preservation.
In this article we describe two speciÔ¨Åc benchmarking chal-
lenges launched by the speech processing research community
to expedite solutions to security and privacy concerns. The Ô¨Årst
involves solutions to protect ASV systems from being manip-
ulated by spooÔ¨Ång in the form of spooÔ¨Ång countermeasures or
presentation attack detection systems [2] whose development
is spearheaded through the ASVspoof initiative launched in
2015 [4, 5, 6]. The second relates to the VoicePrivacy initiative,
launched in 2020 [7], which aims to promote the development
of privacy preservation solutions for speech technology. The
inaugural VoicePrivacy challenge focused upon anonymisation,
namely techniques to manipulate speech data in order that it
cannot be used with ASV systems to recognise the speaker.

The article is intended as a contribution to the efforts to
build bridges between the speech community and, e.g., the le-
gal and ethical communities and, in particular, to support the re-
cently formed Security and Privacy in Speech Communication
(SPSC) special interest group of the International Speech Com-
munication Association (ISCA). It targets the non-specialist and
is hence intentionally high-level, with a focus upon the essen-
tials and clarity, rather than upon scientiÔ¨Åc and technical rigour.
We describe the importance of benchmarking campaigns, clas-
siÔ¨Åer fundamentals and the early, classical approaches to per-
formance estimation. After presenting a high-level overview
of ASV systems, the remainder of the article introduces the
ASVspoof and VoicePrivacy initiatives.

2. Evaluation-driven research

In the early days, researchers collected their own datasets to de-
velop methods (algorithms and software). To make technologi-
cal progress in complex tasks such as speech or speaker recogni-
tion, and to be able to meaningfully compare different methods,
the need for commensurable performance benchmarking was
quickly recognised. From the mid-1990s, the National Institute
of Standards and Technology (NIST) ‚Äî a US-based standard-
isation body ‚Äî pioneered evaluation-driven research [8, 9].
The key ingredients are: (1) commonly agreed (often public)
data, evaluation rules, and performance metrics; (2) disentan-
gled roles for researchers (evaluees) and evaluators. Perfor-
mance claims should not be reported by evaluees but instead
by independent evaluators who provide common infrastructure
(data, rules, metrics) in the form of an evaluation campaign or
challenge. Only with such a level playing Ô¨Åeld can competing
methods can be meaningfully compared. There are many such

(a)

(b)

(c)

Figure 1:
Illustrative score distributions for (a) automatic
speaker veriÔ¨Åcation (ASV), (b) an ASV system subjected
to spooÔ¨Ång attacks and (c) an ASV system presented with
anonymised data.

challenges [10, 11] within the speech Ô¨Åeld. Typically, they re-
quire participants [12, 13] to design special-purpose software
to solve some speciÔ¨Åc tasks. For many, the software takes the
form of a binary classiÔ¨Åer, namely a system which must choose
between two mutually exclusive hypotheses.

Even if classiÔ¨Åers are based on well-established statistical
principles and are trained objectively via numerical optimisa-
tion techniques, they can (and do) make errors. Errors are the
result of over-simpliÔ¨Åed modelling assumptions, natural vari-
ability in the input data, sources of external nuisance variation,
or as a result of limited training data, etc. The design of ever-
more reliable classiÔ¨Åers is the traditional bread and butter of
machine learning and speech research.

A binary classiÔ¨Åer makes two types of errors: misses (false
rejections) and false alarms (false acceptances). Misses imply
that the classiÔ¨Åer rejects a positive input that should be accepted.
A false alarm results from the classiÔ¨Åer accepting a negative
input that should be rejected. ClassiÔ¨Åer performance can be
estimated by dividing the number of misses and false alarms by
the number of tested positive and negative cases respectively.
The resulting miss rate (Pmiss) and false alarm rate (PF A) can
be seen as proxies for user convenience and security.

Binary classiÔ¨Åer decisions are derived in two stages. The
Ô¨Årst involves computation of a ‚Äòsoft decision‚Äô (the score) ‚Äî
a real number that expresses the classiÔ¨Åer‚Äôs conÔ¨Ådence in the
positive case. Example raw score distributions for such a bi-
nary classiÔ¨Åer are illustrated in Fig. 1(a) were the positive case
is the target class and the negative case is the impostor class. In
practice, the score is often a logarithmic likelihood ratio (LLR)
and expresses the relative strength between the two compet-
ing hypotheses. Second, the score is compared with a pre-set
threshold Œ¥. Scores above the threshold imply ‚Äòaccept‚Äô, whereas
scores below the threshold imply ‚Äòreject‚Äô. By increasing the

threshold, the false alarm rate (PF A, red shaded area in Fig. 1)
can be reduced at the expense of a higher miss rate (Pmiss,
green shaded area) and vice versa. The trade-off can be readily
visualised in so-called detection error trade-off (DET) plots [14]
such as that illustrated in Fig. 2 (described below).

Since there are two error rates, which do we report? Or do
we report both? What threshold / how do we set it? The answer
to these questions is rather subtle and a detailed treatment is out-
side the scope of this paper. One particular metric, namely the
equal error rate (EER) is adopted for a broad range of tasks. It
is computed using the threshold that makes miss and false alarm
rates equal (see Fig. 2) ‚Äî thereby yielding a single number to
report. The lower the EER, the more reliable the classiÔ¨Åer.

The authors acknowledge that, even if the EER is depre-
cated in ISO standards [15], it provides a compact summary of
the discrimination capabilities of a classiÔ¨Åer ‚Äî how well it is
capable of observing (in speech, ‚Äòhearing‚Äô) differences between
positive and negative inputs. In practice, however, the EER does
not provide a full picture. More comprehensive approaches to
assessment have been developed and have been broadly adopted
by the community. These provide a view of classiÔ¨Åer perfor-
mance through the lens of a formal decision policy which rep-
resents the effective trade-off between the two decision out-
comes [16].

3. Automatic speaker veriÔ¨Åcation

ASV systems provide one of the most natural and convenient
means to biometric person authentication. Test recordings
(probes) are compared with enrolment recordings (references)
to verify (or not) a claimed identity. The reference is used to
create a model2 which is stored in a reference database. At test
time, the model corresponding to the claimed identity is com-
pared to the test utterance resulting in a soft score. A hard ac-
cept/reject decision can be fully automated (e.g., online bank-
ing) or semi-automated (with some human intervention, e.g.,
when forensic practitioners present voice evidence in court).
Scores should reÔ¨Çect reliably the extent to which the strength-
of-evidence supports the same/different identity propositions:
the higher the score, the greater the similarity and vice versa.

Detection error trade-off plots for two different ASV sys-
tems assessed using the ASVspoof 2019 logical access database
and the VoicePrivacy 2020 database are illustrated in Figs. 2
and 4 and show EERs of 2.5% and 1.1% respectively (blue pro-
Ô¨Åles). Each point on any one proÔ¨Åle corresponds to a different
decision threshold (operating point). The proÔ¨Åles show how
misses can be traded off against false alarms in order to meet
different application requirements.

4. Security vulnerabilities: ASVspoof

Without adequate protections, the reliability of ASV systems
can be compromised by the presentation of synthetic or con-
verted voice, replayed speech and impersonation [6]. Generated
automatically from a text input, today‚Äôs state-of-the-art synthe-
sis systems are capable of producing speech that the human can-
not distinguish from bona Ô¨Åde speech [17]. Voice conversion
systems operate directly upon an input speech signal and alter
the voice to that of another speaker [18]. Unlike synthetic and
converted voice spooÔ¨Ång attacks, which both demand a certain
technical expertise and suitable training and adaptation data,

2On account of their dynamic nature and the variability in speech

signals, we refer to models, not templates.

scoreImpostorTargetùû≠PmissPFAscoreImpostorTargetùû≠PmissPFASpoofPspoofscoreImpostorTargetùû≠PmissPFAFigure 2: Detection error trade-off plot for the ASVspoof 2019
logical access task. ProÔ¨Åles shown for the baseline ASV sys-
tem (blue proÔ¨Åle) and the same system subjected to the most
effective synthetic (text-to-speech, TTS) speech spooÔ¨Ång attack
(orange proÔ¨Åle) and the most effective voice conversion (VC)
spooÔ¨Ång attack (red proÔ¨Åle).

replay attacks can be launched by the layman, requiring only
consumer-grade recording and replaying devices. All can sub-
stantially degrade ASV reliability. Impersonation, while still a
threat [19, 20], is less effective.

The impact of spooÔ¨Ång attacks upon an ASV system is il-
lustrated in Fig. 1(b). SpooÔ¨Ång attacks introduce a third class
of input so that the ASV system must now cope with target
(matching), impostor (non-matching) and spoofed utterances.
A successful spooÔ¨Ång attack circumvents the ASV system by
provoking a score above the decision threshold. The score
distribution for spooÔ¨Ång attacks is illustrated in the middle of
Fig. 1(b). SpooÔ¨Ång attacks provoke a false alarm rate Pspoof
that is greater than the original false alarm rate PF A. One can
think of spooÔ¨Ång attacks as a special case of impostors where
there is a concerted effort to deceive the ASV system.

Without the capacity to distinguish between spoofed and
bona Ô¨Åde speech, ASV reliability will degrade as a result of
spooÔ¨Ång attacks, sometimes substantially. This degradation as-
sessed using the ASVspoof 2019 logical access database (syn-
thetic and converted voice spooÔ¨Ång attacks) is illustrated in
Fig. 2. The baseline EER (with no spooÔ¨Ång attacks) of 2.5%
increases to over 50% when impostor trials are replaced by the
most effective synthetic and converted voice spooÔ¨Ång attacks.
These results should be interpreted with caution, however. In
practice, one must consider the relative likelihood of the ASV
system being presented with target and impostor trials, versus
that of spooÔ¨Ång attacks. Without this consideration, the results
in Fig. 2 might convey an overly pessimistic view of the vulner-
abilities to spooÔ¨Ång. There is, in any case, potential to detect
attacks automatically using countermeasures.

The series of ASVspoof challenges held bi-annually since
2015 have spearheaded the development of spooÔ¨Ång counter-
measures or presentation attack detection (PAD) solutions for
ASV. SpooÔ¨Ång countermeasures can be applied prior to ASV
in order to detect attacks and prevent them from reaching the
ASV system. The most recently completed challenge was held
in 2019 and included separate logical access (synthetic and
converted voice attacks) and physical access (replay attacks)

Figure 3: As for Fig. 2 except for spooÔ¨Ång countermeasures.
ProÔ¨Åles shown for the top Ô¨Åve performing systems.

tasks [6]. DET plots for the top-Ô¨Åve performing spooÔ¨Ång coun-
termeasures for the ASVspoof 2019 logical access task are
shown in Fig. 3. They show EERs of as low as 0.2%.

Thus, while spooÔ¨Ång attacks can present a substantial threat
to reliability, and while human listeners may not be able to de-
tect spooÔ¨Ång attacks, countermeasures can be effective. Even
so, while ASV countermeasures have proven potential to detect
attacks, they can also degrade usability; they can erroneously
classify bona Ô¨Åde speech as spoofed speech. Assessment and
performance estimates should hence reÔ¨Çect the impact of both
spooÔ¨Ång and countermeasures upon the ASV system; counter-
measures should be assessed in tandem with ASV. Such more
elaborate approaches to assessment, e.g. [21], are outside the
scope of the current article.

5. Privacy implications: VoicePrivacy

Speech signals contain much more than just the spoken mes-
sage or the voice identity [22, 23]. The speaker‚Äôs sex/gender,
age, socio-economic and geographical background, emotion
and health condition etc. can all be estimated automatically us-
ing recordings of speech [24, 25]. With much of this infor-
mation being personal and private there is hence an interest to
develop privacy safeguards for speech technology. This is the
goal of the VoicePrivacy initiative, founded in 2020. While so-
lutions to privacy preservation can take many different forms,
and while VoicePrivacy may explore different approaches in the
future, the inaugural challenge focused upon the development
of anonymization solutions [7].

The idea is to protect privacy by distorting speech data such
that it cannot be used by ASV systems to recognise the speaker.
Anonymisation is achieved by suppressing the information in
speech signals that is typically used by machines to infer iden-
tity. On its own, this is relatively straightforward, e.g. by adding
sufÔ¨Åcient levels of background noise, or by replacing speech
with silence. The challenge comes from the requirement to
suppress personally identiÔ¨Åable attributes contained within the
speech signal while leaving all other attributes intact. These re-
quirements imply that anonymisation should not interfere with
the application of some down stream tasks such as automatic
speech recognition, nor should it introduce processing artefacts
that might degrade subjective intelligibility or naturalness. Last,
anonymised voices should remain distinctive, meaning the task

0.20.5 1  2  5 10 20 30 40 50 60 70 80 False Alarm probability (in %)0.20.5 1  2  5 10 20 30 40 50 60 70 80 Miss probability (in %)non-target (EER=2.46 %)worst VC spoofing attack (EER=65.25 %)worst TTS spoofing attack (EER=66.42 %)EER=2.46%0.20.5 1  2  5 10 20 30 40 50 False Alarm probability (in %)0.20.5 1  2  5 10 20 30 40 50 Miss probability (in %)Team 05 (EER=0.22 %)Team 45 (EER=1.86 %)Team 60 (EER=2.64 %)Team 24 (EER=3.45 %)Team 50 (EER=3.56 %)posed deÔ¨Ånitions for a number of distinct tasks and solutions;
established evaluation driven research initiatives as a vehicle to
successfully raise the proÔ¨Åle of security and privacy research
and to build new research communities; developed protocols,
criteria and metrics for the assessment of security and privacy
safeguards. Nonetheless, we are perhaps still far from having
a comprehensive appreciation of the implications as well as the
potential of safeguards.

Will spooÔ¨Ång ever be a solved problem? Possibly not.
Speech synthesis and voice conversion are long-established re-
search Ô¨Åelds with genuine applications, such as anonymisation,
yet the same technology poses a threat to the security of ASV.
The progress speech synthesis and voice conversion in recent
years has been impressive. Today‚Äôs technology produces syn-
thetic speech that humans cannot distinguish from bona Ô¨Åde
speech. Whether or not similar advances may one day result
in machines that produce synthetic speech that other machines
cannot detect is an intriguing question. For the time being it
is clear that, if we seek reliable, secure approaches to person
authentication using ASV, we must intensify our efforts in anti-
spooÔ¨Ång. Future directions include a focus on more adversarial
attacks, the development of countermeasures that function reli-
ably in the wild, e.g. in the face of background noise and other
sources of nuisance variation such as bandwidth and channel
variability which typify telephony ASV scenarios, as well as
the approach used to estimate performance.

The research effort in anonymisation is relatively embry-
onic. While there is an opportunity to provide some level of pro-
tection, current solutions fall short of delivering true anonymi-
sation. Furthermore, we have observed differences in the level
of privacy that a given anonymisation solution provides to dif-
ferent individuals. Whereas the protection for some can be
strong, others are left with relatively little protection at all.
Since personally identiÔ¨Åable information encapsulates far more
than that used by most ASV systems to infer identity, since
other alternative attributes can be used instead, and since cur-
rent anonymisation solutions do not necessarily suppress them,
is full anonymisation even technically possible?

Anonymisation solutions that focus only upon the attributes
used by typical ASV systems already degrade intelligibility
and naturalness. If all personally identiÔ¨Åable information can
be successfully suppressed, then what remains? Is personally
identiÔ¨Åable information so inextricably, indelibly embedded in
speech that it cannot be (fully) removed? Even if a speech sig-
nal can be fully anonymised, will anything resembling speech
remain? Or, for a given application, what level of intelligibil-
ity/naturalness can be sacriÔ¨Åced in order to achieve anonymisa-
tion? How should we even measure anonymisation performance
and privacy? Are the methodology and metrics adequate? How
does this research Ô¨Åt with broader solutions to privacy preserva-
tion, e.g. encryption and distributed learning?

That we have certainly raised more questions above than we
have answered serves to show that our community‚Äôs journey in
security and privacy research is only just beginning. Our efforts
must be redoubled looking to the future.

7. Acknowledgements

The work reported in this paper was supported by: the French
ANR (VoicePrivacy, VoicePersonae, DEEP-PRIVACY, Har-
pocrates); the European Commission (H2020 COMPRISE); the
Japan Science and Technology Agency (JST) with grant No.
JPMJCR18A6; JSPS KAKENHI No.21K17775; the Academy
of Finland (proj. 309629); Region Grand Est, France.

Figure 4: Detection error trade-off plot for the VoicePrivacy
2020 challenge (LibriSpeech test, male trials). ProÔ¨Åles shown
for the baseline ASV system (blue proÔ¨Åle) and the same system
presented with anonymised test utterances (orange proÔ¨Åle) and
anonymised test utterances when the ASV system is re-trained
using similarly anonymised training data (red proÔ¨Åle).

might better be referred to as pseudonymisation.

Rather than operating upon the speech signal so that it
reÔ¨Çects the voice of another, speciÔ¨Åc speaker, anonymisation
aims to prevent the speaker identity from being recognised.
Anonymisation acts to increase the confusion between utter-
ances produced by the same and different speakers. For a per-
fect anonymisation system, the score distributions correspond-
ing to impostor and target trials overlap. The desired effect of
anonymisation upon an ASV system is illustrated in Fig. 1(c).
In this case, the ASV system cannot produce simultaneously
both a low false alarm rate and a low miss rate, no matter what
the decision threshold, and the EER is 50%. Real anonymisa-
tion solutions are less effective.

The Ô¨Årst VoicePrivacy challenge was held in 2020 and at-
tracted submissions from 7 independent teams. A DET plot for
the primary challenge baseline [7] is illustrated in Fig. 4.
It
shows an increase in the EER from a baseline of 1% (blue pro-
Ô¨Åle) to 52% after anonymisation (orange proÔ¨Åle). The EER of
over 50% suggests that the anonymisation goal is met. How-
ever, if an anonymisation adversary were to adapt the ASV sys-
tem in light of anonymisation, then performance is less effec-
tive; anonymised utterances still contain some personally iden-
tiÔ¨Åable information (PII) and the potential to re-identify the
speaker remains. When the ASV system is re-trained or adapted
using similarly anonymised training data, the result is a lower
EER of 10.7% (red proÔ¨Åle); true anonymisation remains elu-
sive.

The above treatment does not reÔ¨Çect impacts upon intel-
In practice,
ligibility/naturalness, nor voice distinctiveness.
the multiple objectives and complex nature of anonymisation
means several different metrics are used in practice. The devel-
opment of more suitable approaches to assessment are the focus
of current research [26, 27, 28, 29].

6. ReÔ¨Çections and further considerations

The community has made substantial progress to address the se-
curity and privacy implications of speech technology and rapid
progress has been made in the last half-decade. We have: pro-

0.5 1  2  5 10 20 30 40 50 60 70 80 False Alarm probability (in %)0.5 1  2  5 10 20 30 40 50 60 70 80 Miss probability (in %)ASV baseline (EER=1.11%ASV trained on anonymised data,anonymised test (EER=10.69%)Anonymised test (EER=52.12%8. References

[1] M. Sahidullah, H. Delgado, M. Todisco, T. Kinnunen,
N. Evans, J. Yamagishi, and K.-A. Lee, ‚ÄúIntroduction to
voice presentation attack detection and recent advances,‚Äù
in Handbook of Biometric Anti-SpooÔ¨Ång, pp. 321‚Äì361.
Springer, 2019.

[2] ‚ÄúISO/IEC 30107. Information Technology ‚Äì Biometric
presentation attack detection,‚Äù
Standard, International
Organization for Standardization, Geneva, Switzerland,
2016.

[3] A. Nautsch, C. Jasserand, E. Kindt, M. Todisco, I. Tran-
coso, and N. Evans, ‚ÄúThe GDPR & Speech Data: ReÔ¨Çec-
tions of Legal and Technology Communities, First Steps
Towards a Common Understanding,‚Äù in Proc. Interspeech
2019, 2019, pp. 3695‚Äì3699.

[4] Z. Wu, T. Kinnunen, N. Evans, J. Yamagishi, C. Hanilc¬∏i,
M. Sahidullah, and A. Sizov, ‚ÄúASVspoof 2015: the Ô¨Årst
automatic speaker veriÔ¨Åcation spooÔ¨Ång and countermea-
sures challenge,‚Äù in Proc. Interspeech, 2015, pp. 2037‚Äì
2041.

[5] T. Kinnunen, M. Sahidullah, H. Delgado, M. Todisco,
N. Evans, J. Yamagishi, and K. A. Lee, ‚ÄúThe ASVspoof
2017 Challenge: Assessing the Limits of Replay SpooÔ¨Ång
Attack Detection,‚Äù in Proc. Interspeech, 2017, pp. 2‚Äì6.

[6] M. Todisco, X. Wang, V. Vestman, M. Sahidullah, H. Del-
gado, A. Nautsch, J. Yamagishi, N. Evans, T. H. Kin-
nunen, and K. A. Lee, ‚ÄúASVspoof 2019: future horizons
in spoofed and fake audio detection,‚Äù in Proc. Interspeech,
2019, pp. 1008‚Äì1012.

[7] N. Tomashenko, B. M. L. Srivastava, X. Wang, E. Vin-
cent, A. Nautsch, J. Yamagishi, N. Evans, J. Patino, J.-F.
Bonastre, P.-G. No¬¥e, and M. Todisco, ‚ÄúIntroducing the
VoicePrivacy Initiative,‚Äù in Proc. Interspeech, oct 2020,
pp. 1693‚Äì1697.

[8] K. A. Lee, O. Sadjadi, H. Li, and D. Reynolds, ‚ÄúTwo
decades into speaker recognition evaluation - are we there
yet?,‚Äù Computer Speech & Language, vol. 61, pp. 101058,
2020.

[9] C. S. Greenberg, L. P. Mason, et al., ‚ÄúTwo decades of
speaker recognition evaluation at the National Institute of
Standards and Technology,‚Äù Computer Speech & Lan-
guage, vol. 60, pp. 101032, 2020.

[10] A. Nagrani, J. S. Chung, W. Xie, and A. Zisserman, ‚ÄúVox-
celeb: Large-scale speaker veriÔ¨Åcation in the wild,‚Äù Com-
puter Speech & Language, vol. 60, pp. 101027, 2020.

[11] H. Zeinali, K. A. Lee,

J. Alam, and L. Burget,
‚ÄúSdSV Challenge 2020: Large-Scale Evaluation of Short-
Duration Speaker VeriÔ¨Åcation,‚Äù in Proc. Interspeech 2020,
2020, pp. 731‚Äì735.

[12] K. A. Lee, V. Hautamaki, T. Kinnunen, H. Yamamoto,
et al.,
‚ÄúI4U submission to NIST SRE 2018: Leverag-
ing from a decade of shared experiences,‚Äù in Proc. In-
terspeech, 2019, pp. 1497‚Äì1501.

[13] J. Villalba, N. Chen, D. Snyder, D. Garcia-Romero,
et al., ‚ÄúState-of-the-art speaker recognition for telephone
the JHU-MIT submission for NIST
and video speech:
SRE18,‚Äù in Proc. Interspeech, 2019, pp. 1488‚Äì1492.

[14] A. Martin, G. Doddington, T. Kamm, M. Ordowski, and
M. Przybocki, ‚ÄúThe DET Curve in Assessment of Detec-
tion Task Performance,‚Äù in Proc. Eurospeech, 1997, pp.
1895‚Äì1898.

[15] ‚ÄúISO/IEC IS 19795-1:2021. Information Technology ‚Äì
Biometric performance testing and reporting ‚Äì Part 1:
Principles and framework,‚Äù Standard, International Orga-
nization for Standardization, Geneva, Switzerland, 2021.
[16] A. Nautsch, Speaker Recognition in Unconstrained Envi-
ronments, Ph.D. thesis, Technische Universit¬®at Darmstadt,
2019.

[17] J. Shen, R. Pang, R. J. Weiss, et al., ‚ÄúNatural TTS synthe-
sis by conditioning WaveNet on Mel spectrogram predic-
tions,‚Äù in Proc. ICASSP, 2018, pp. 4779‚Äì4783.

[18] Z. Yi, W.-C. Huang, X. Tian, J. Yamagishi, et al., ‚ÄúVoice
Conversion Challenge 2020 ‚Äî Intra-lingual semi-parallel
in Proc. Joint
and cross-lingual voice conversion ‚Äî,‚Äù
Workshop for the Blizzard Challenge and Voice Conver-
sion Challenge 2020, 2020, pp. 80‚Äì98.

[19] R. G. Hautam¬®aki, T. Kinnunen, V. Hautam¬®aki, T. Leino,
and A.-M. Laukkanen, ‚ÄúI-vectors meet imitators: on vul-
nerability of speaker veriÔ¨Åcation systems against voice
mimicry.,‚Äù in Proc. Interspeech, 2013, pp. 930‚Äì934.
[20] Z. Wu, N. Evans, T. Kinnunen, J. Yamagishi, F. Alegre,
and H. Li, ‚ÄúSpooÔ¨Ång and countermeasures for speaker
veriÔ¨Åcation: A survey,‚Äù Speech Communication, vol. 66,
pp. 130‚Äì153, feb 2015.

[21] T. Kinnunen, H. Delgado, N. Evans, K. A. Lee,
et al.,
‚ÄúTandem Assessment of SpooÔ¨Ång Countermea-
sures and Automatic Speaker VeriÔ¨Åcation: Fundamen-
IEEE/ACM Transactions on Audio, Speech, and
tals,‚Äù
Language Processing, vol. 28, pp. 2195‚Äì2210, 2020.
[22] A. Nautsch, A. Jim¬¥enez, A. Treiber, J. Kolberg, et al.,
‚ÄúPreserving privacy in speaker and speech characterisa-
tion,‚Äù Computer Speech & Language, vol. 58, pp. 441‚Äì
480, 2019.

[23] COMPRISE,

‚ÄúDeliverable N¬∫5.1:

tion and GDPR requirements
https://www.compriseh2020.eu/Ô¨Åles/2019/06/d5.1.pdf,‚Äù
2019.

[online].

Data protec-
available:

[24] I. Shafran, M. Riley, and M. Mohri, ‚ÄúVoice signatures,‚Äù in

Proc. ASRU, 2003, pp. 31‚Äì36.

[25] T. Schultz, ‚ÄúSpeaker characteristics,‚Äù in Speaker classiÔ¨Å-

cation I, pp. 47‚Äì74. Springer, 2007.

[26] B. M. L. Srivastava, A. Bellet, M. Tommasi, and E. Vin-
‚ÄúPrivacy-Preserving Adversarial Representation
cent,
Learning in ASR: Reality or Illusion?,‚Äù in Interspeech
2019, sep 2019, pp. 3700‚Äì3704.

[27] A. Nautsch, J. Patino, N. Tomashenko, J. Yamagishi, P.-G.
No¬¥e, J.-F. Bonastre, M. Todisco, and N. Evans, ‚ÄúThe Pri-
vacy ZEBRA: Zero Evidence Biometric Recognition As-
sessment,‚Äù in Proc. Interspeech, 2020, pp. 1698‚Äì1702.

[28] M. Maouche, B. M. L. Srivastava, N. Vauquier, A. Bellet,
M. Tommasi, and E. Vincent, ‚ÄúA Comparative Study of
Speech Anonymization Metrics,‚Äù in Proc. Interspeech,
2020, pp. 1708‚Äì1712.

[29] P.-G. No¬¥e, J.-F. Bonastre, D. Matrouf, N. Tomashenko,
A. Nautsch, and N. Evans, ‚ÄúSpeech Pseudonymisation
Assessment Using Voice Similarity Matrices,‚Äù in Proc.
Interspeech 2020, 2020, pp. 1718‚Äì1722.

