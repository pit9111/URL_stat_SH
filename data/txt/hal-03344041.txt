Toward Generic Abstractions for Data of Any Model
Nelly Barret, Ioana Manolescu, Prajna Upadhyay

To cite this version:

Nelly Barret, Ioana Manolescu, Prajna Upadhyay. Toward Generic Abstractions for Data of Any
Model. BDA 2021 - Informal publication only, Oct 2021, Paris, France. ï¿¿hal-03344041v2ï¿¿

HAL Id: hal-03344041

https://inria.hal.science/hal-03344041v2

Submitted on 14 Sep 2021

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

Lâ€™archive ouverte pluridisciplinaire HAL, est
destinÃ©e au dÃ©pÃ´t et Ã  la diffusion de documents
scientifiques de niveau recherche, publiÃ©s ou non,
Ã©manant des Ã©tablissements dâ€™enseignement et de
recherche franÃ§ais ou Ã©trangers, des laboratoires
publics ou privÃ©s.

Toward Generic Abstractions for Data of Any Model

Nelly Barret, Ioana Manolescu, Prajna Upadhyay
Inria & Institut Polytechnique de Paris
nelly.barret@inria.fr,ioana.manolescu@inria.fr,prajna-devi.upadhyay@inria.fr

ABSTRACT
Digital data sharing leads to unprecedented opportunities to de-
velop data-driven systems for supporting economic activities, the
social and political life, and science. Many open-access datasets are
RDF graphs, but others are CSV files, Neo4J property graphs, JSON
or XML documents, etc.

Potential users need to understand a dataset in order to decide if
it is useful for their goal. While some datasets come with a schema
and/or documentation, this is not always the case. Data summariza-
tion or schema inference tools have been proposed, specializing in
XML, or JSON, or the RDF data models. In this work, we present a
dataset abstraction approach, which (ğ‘–) applies on relational, CSV,
XML, JSON, RDF or Property Graph data; (ğ‘–ğ‘–) computes an abstrac-
tion meant for humans (as opposed to a schema meant for a parser);
(ğ‘–ğ‘–ğ‘–) integrates Information Extraction data profiling, to also classify
dataset content among a set of categories of interest to the user.
Our abstractions are conceptually close to an Entity-Relationship
diagram, if one allows nested and possibly heterogeneous structure
within entities.

1 INTRODUCTION
Open-access data is multiplying over the Internet. This leads on
one hand, to the development of new businesses, economic op-
portunities and applications, and on the other hand, to circulating
knowledge on a variety of topics, from health to education, envi-
ronment, the arts, or leisure activities.

Many of the openly available datasets follow the RDF standard,
the W3Câ€™s recommendation for sharing data. The Linked Open Data
Cloud portal lists thousands of such datasets containing, e.g., na-
tional or worldwide statistics, music, scientific bibliographies, and
many other interesting, open RDF graphs are not listed there. How-
ever, Open Data sets are not (or not only) RDF, as demonstrated by
the following examples of very popular open datasets. (ğ‘–) CSV files
(each of which can be seen as a table) are shared on machine learn-
ing portals such as Kaggle or the French public portal data.gouv.fr;
The French national transparency database HATVP (Haute AutoritÃ©
pour la Transparence de la Vie Publique) also publishes CSV files;
(ğ‘–ğ‘–) relational databases, comprising several interrelated tables, are
used e.g. to disseminate the DBLP bibliographic data; (ğ‘–ğ‘–ğ‘–) XML is
the format used in hundreds of million of bibliographic notices, e.g.,
on PubMed; DBLP and HATVP data has also been shared as XML;
(ğ‘–ğ‘£) JSON has become more recently the format of choice, used e.g.
to describe the complete activity of the French parliament on the
websites NosDeputes.fr and NosSenateurs.fr; (ğ‘£) property graphs,
such as pioneered by Neo4J, are oriented graphs whose nodes and
edges may have labels and properties; this is the format used by
the International Consortium of Investigative Journalists to share
their investigative data.

Decades of data management research have shown that no new
data model completely replaces the previous ones. Some models

thought obsolete re-surface under new incarnations (think of nested
relations vs. document stores, or object databases vs. property
graphs). The model in which a dataset is produced or exported is
decided by the producers, depending on what they understand/are
familiar with, the system at their disposal for storing the data, and
the needs of foreseeable data users. The Open Data exchange sce-
narios, where data users often lack any institutional connection to
the producers, also forces users to cope with the data as it is, since
the producers have no incentive (and, often, lack the resources) to
restructure the data in a different format. Thus, we believe the data
model variety in Open Data is here to stay.

Users who must decide whether to use a dataset in an appli-
cation need to have a basic understanding of its content and
the suitability to their need. Towards this goal, schemas may be
available to describe the data structure, and/or documentation (text)
may describe its content in natural language. As help to the users,
schemas and documentations have some limitations: (ğ‘) schemas are
often unavailable, especially for semistructured data formats such
as JSON, RDF or XML and documentation is also often unavailable
or too terse to inform the users; (ğ‘) even when they are published,
or built by automated tools, e.g., [3â€“5, 7â€“9, 15], schemas are not
helpful (or too complex) for casual users, who ignore what an â€œXML
elementâ€, â€œJSON arrayâ€ or â€œRDF property nodeâ€ is; (ğ‘) schemas as
well as documentation describe the data according to the data pro-
ducerâ€™s terminology, not according to the consumerâ€™s. For instance,
in the XML dataset at the left in Figure 1, a public library labels
its data entries item, with the implicit knowledge that these are
documents (books, magazines, etc.), whereas from the usersâ€™ per-
spective, this dataset describes books; (ğ‘‘) by design, schemas do not
quantitatively reflect the data, whereas such information could be
very useful as a first insight on the data.
Towards a data model-independent dataset abstraction To fa-
cilitate the understanding of a dataset by a user, we compute a
compact description of the data, focusing on its most frequent content,
free of data model-specific syntactic details, and formulated in terms
that interest the user. To that end, we develop a single, integrated
method, applicable to any of the data models mentioned above. For
example, given any of the four bibliographic datasets in Figure 1
and a user interested in â€œbooksâ€, our approach would correctly
identify the dataset as pertinent for the userâ€™s question. As we will
explain, users can specify their categories of interest through a few
hints; with the help of popular knowledge bases, our system makes
suggestions to enlarge the set of hints and improve the chances of
users to have an accurate description of their dataset. We proceed
in several steps, which also organize the remainder of the paper.

(1.) We view each dataset as holding records, that is: objects
with some internal structure, simple or complex, representing a
concept or an object. For instance, the XML data in Figure 1 contains
two item records. Further, we identify collections grouping similar
records (some records may belong to no collection). For instance, the

Nelly Barret, Ioana Manolescu, Prajna Upadhyay

Figure 1: Motivating example: four bibliographic datasets, which we view as Collections of CreativeWork records.

bibliography XML element in Figure 1 can be seen as a collection
which holds the two records. We identify a set of requirements
for our record and collection detection method, and formalize the
problem of deriving them automatically from a dataset (Section 2).
(2.) We propose an algorithm for automatically identifying records
and collections in a dataset of any of the supported data models
(Section 3). For instance, given the XML dataset in Figure 1 (or the
relational or the RDF dataset from the same figure), our algorithm
understands it as a collection of records, each record corresponding
to a book, and having: a title, a collection of authors, and possibly
a collection of reviews.

(3.) We separate collections of Entities from collections of Rela-
tionships, in order to report to the user a structured, meaningful
dataset abstraction (Section 4).

(4.) To help users understand the data in their terms, we attempt
to assign each collection to a category from a predefined category
set, derived from a general-purpose ontology and/or based on the
concepts of interest to users, e.g., books in the above example. We
provide a method for our tool to enrich its knowledge of user-
specified categories by selectively gathering information from large
online knowledge bases. For instance, our algorithm classifies the
bibliography in Figure 1 as a Creative Work (a Schema.org standard
type including books, paintings, songs, etc.)
Deployment scenarios On one hand, data producers can use our
tool to automatically derive data abstractions, to be shared next to
the data; on the other hand, users can generate the abstractions
after downloading a candidate dataset, in order to assess its interest.

2 PROBLEM STATEMENT
We start by analyzing a set of requirements of our problem, then
formally state it.

2.1 Requirements
R1: data model-independent abstractions Our method needs to
go beyond the syntactic details to extract semantically meaningful
records, and understand if they are organized in collections.
R2: structurally rich abstractions The above problem can be
seen as reverse-engineering a dataset to identify its conceptual
model, which generalizes the classic Entity-Relationship model
behind relational databases [13] by allowing records to have multi-
valued attributes, and to contain other records and/or collections. For
instance, the second item record in Figure 1 contains a collection of
review records. A collection uniformly represents a list, set or bag
of records: data order and possible duplicates are not reflected in
our abstractions.
R3: see beyond the data structures In some cases, data syntax
features are insufficient to distinguish records from collections. On
one hand, in some data models, such as XML or RDF, the syntax does
not distinguish them, e.g., in XMark [14] benchmark documents,
an âŸ¨open_auctionsâŸ© element is clearly a collection, while a âŸ¨userâŸ©
element describes a record. On the other hand, even when the data
model distinguishes nodes that should naturally be records (e.g.,
JSON maps), from others that should be collections (e.g., JSON
arrays), we should not make this decision purely based on syntax.
Indeed, as also noted in [15], a short array could in fact designate an
object (e.g., three coordinates describe a geographical point), while
a map may be used to encode a list, e.g., with attributes named â€œ1â€,
â€œ2â€, â€œ3â€ etc., as in Le Mondeâ€™s Decodex dataset. It is important that
such variations in data design do not confuse our abstraction.
R4: implicit or explicit collections In our motivating example,
each of the four datasets holds a collection of books. In XML, the
collection is explicit (materialized by the âŸ¨bibliographyâŸ© node), and
the table node labeled Book plays the same role in the relational
dataset. In contrast, in the RDF dataset, there is no common parent

Toward Generic Abstractions for Data of Any Model

of the books; we say that the collection here is implicit. Note that
whether collections are explicit or implicit does not depend on the
original data model: in the relational dataset, the book collection is
explicit but the review collection is not; similarly, the RDF dataset
could have included a common parent to all the book nodes, which
would have made that collection explicit.
Availability and role of schemas and types What schema in-
formation can we expect to have, and how should we treat it?

Relational databases always have a schema, describing elemen-
tary data types, the attributes of each table, and possible integrity
constraints. In a CSV file, the number of attributes can be identi-
fied easily; their names may or may not be present, and data types
are not explicitly declared; data profiling [1] is needed to infer
their domains. XML documents may or may not have a schema,
expressed as a Document Type Description (DTD) or XML Schema
Description (XSD); these specify the allowed children for each type
of element. JSON documents usually come without a schema, but
recent methods [3, 4, 15] derive schemas from the documents. RDF
graphs are often schemaless, but they may be endowed with: (ğ‘–) an
ontology, expressed in RDF Schema or OWL, describing relation-
ships between the types and properties present in the graph, e.g.,
Any Student is a Person, or Anyone taking a Course is a Student;
(ğ‘–ğ‘–) a SHACL (Shapes Constraint Language) schema specification,
against which a graph may be validated or not. Schemas for property
graphs are being investigated actively [10], although no standard
has emerged yet.

Data types are basic components of schemas. When present,
types encapsulate valuable insights into the data organization and
semantics. Thus, we formulate the following requirement:
R5: explicit types When available, types should guide our identifi-
cation of records and collections, even though our approach should
not depend on them.

2.2 Abstraction approach
To satisfy requirement R1, we leverage the ConnectionLens sys-
tem [2, 6] which models the information from any relational data-
base, CSV, XML, JSON, RDF document, or property graph, as a
graph ğº = (ğ‘ , ğ¸) where ğ¸ âŠ† ğ‘ Ã— ğ‘ is a set of directed edges, and
ğœ†ğ‘ , ğœ†ğ¸ are two functions labeling each node (respectively, edge)
with a label (a string), that could in particular be ğœ– (the empty label).
Figure 2 illustrates this; for now, just focus on the nodes and edges
(their colors and the shaded areas will be explained later).

Relational data A relational table or a CSV dataset can be seen
as a set of tuples, each with the same attributes. This can be turned
into a graph by modeling the dataset as a table node, having one
child tuple node for each tuple (or line in the CSV file); this child
has one child attribute node for each attribute. Figure 2 illustrates
this for the sample relational dataset in Figure 1. Following R5, we
leverage foreign key constraints expressed in a relational database
schema as follows. Whenever relation ğ‘† includes a foreign key to
relation ğ‘…, an edge is created leading from each tuple in ğ‘†, to the
respective ğ‘… tuple node.

XML and JSON data are naturally converted into trees.
RDF data Each triple (ğ‘ , ğ‘, ğ‘œ) from an RDF graph is converted
into an edge between the (single) node labeled ğ‘  to the (single) node
labeled ğ‘œ; the edge is labeled ğ‘. We denote by ğœ the special RDF type

property used to explicitly connect an URI to its type (which is also
a node in the graph).

Property graphs (PG) In this rich, directed graph data model:
(ğ‘–) each node may have a set of attributes with a name and a value;
(ğ‘–ğ‘–) each edge can similarly have attributes; (ğ‘–ğ‘–ğ‘–) zero or more labels
may be attached to each node and/or edge, playing roughly the
role of a type. In our graphs, each node/edge has at most one label.
Therefore, we transform a property graph as follows. Each PG node
becomes a node ğ‘› âˆˆ ğ‘ , labeled ğœ–. Each label ğ‘™ of a PG node ğ‘›
ğœ
becomes an edge ğ‘›
âˆ’â†’ ğ‘›ğ‘™ âˆˆ ğ¸, where ğ‘›ğ‘™ is a leaf node labeled ğ‘™ and
ğœ is the RDF type property mentioned above. Each attribute of ğ‘›,
ğ‘
named ğ‘ and whose value is ğ‘, becomes an edge ğ‘›
âˆ’â†’ ğ‘›ğ‘ âˆˆ ğ¸ where
ğ‘›ğ‘ is a leaf node labeled ğ‘. Each PG edge ğ‘’ is turned into a node
ğ‘›ğ‘’ , labeled ğœ–, plus two edges, connecting it to its source and target
nodes in the original PG; ğ‘›ğ‘’ also has outgoing edges modeling the
attributes of ğ‘’, similarly to PG nodes.

Extracted entities The graph built by ConnectionLens out of
any dataset is enriched through entity extraction, applied on each
value (string, leaf) node present in the dataset [2]. In our example,
Alice, Bob, Carole and David are recognized as Person entities.
The set ğ‘‡ğ¸ of entity types also includes: Location, Organization,
Date, URIs, emails, hashtags, etc. Entities will be used to categorize
collections (Section 5).
Problem statement Given the graph ğº = (ğ‘ , ğ¸) obtained as
above, our goal is to:

(1) Identify records and collections, that is: find (ğ‘–) a set R âŠ† ğ‘ of
nodes which we call records; (ğ‘–ğ‘–) a set of sets C = {ğ¶1, ğ¶2, . . .},
where each ğ¶ğ‘– is a set of elements from R, the ğ¶ğ‘– â€™s are pair-
wise disjoint, and for each ğ¶ğ‘– , there may exist a node ğ‘›ğ¶ğ‘–
such that (ğ‘›ğ¶ğ‘– , ğ‘Ÿ ğ‘—
ğ‘– âˆˆ ğ¶ğ‘– , in other words:
ğ‘›ğ¶ğ‘– is a parent (in ğº) of all the nodes from ğ¶ğ‘– ; (ğ‘–ğ‘–ğ‘–) for each
ğ‘Ÿ âˆˆ R, a set of nodes and edges of ğº which we view as part
of the record ğ‘Ÿ .

ğ‘– ) âˆˆ ğ¸ for every ğ‘Ÿ ğ‘—

(2) Separate the collections in C into Cğ¸ and Cğ‘…, respectively
collections of entities and collections of relationships.
(3) Classify the collections of entities: given a set of categories,
and a set of hints (see Section 5), assign to each collection
the category it is closest to (or none if does not fit the given
categories).

The nodes ğ‘ \ R \ {ğ‘›ğ¶ğ‘–

| ğ¶ğ‘– âˆˆ C} which are neither records nor
collection nodes are called sub-records and their set is denoted SR.
Together with edges connecting them to each other and to a record
ğ‘Ÿ , sub-records form the record content sought in (3b) below.

As shown above, for now, only collections of entities are classified
(not those of relationships). The reason not to classify relationships
is that entities (â€œthingsâ€) seem even easier to understand for non-IT
users, and it is easier for them to provide hints (see later) about
entities, than about relationships.

3 IDENTIFYING RECORDS AND

COLLECTIONS

We start by a convenient transformation on the graph ğº. Some of
its edges have labels (e.g., RDF triples, edges in JSON maps) while
others carry the empty label ğœ–. For uniformity, we transform ğº into
ğ‘™
an unlabeled graph ğº â€², replacing each labeled edge ğ‘›1
âˆ’â†’ ğ‘›2 with

Nelly Barret, Ioana Manolescu, Prajna Upadhyay

Figure 2: Graph representations of the four bibliographic datasets shown in Figure 1.

an intermediary node labeled ğ‘™, and connected to ğ‘›1, ğ‘›2 as follows:
ğœ–
âˆ’â†’ ğ‘›2. From now on, we will work on ğº â€², whose nodes
ğ‘›1
and edges will be simply denoted as (ğ‘ â€², ğ¸ â€²). Then:

ğœ–
âˆ’â†’ ğ‘›ğ‘™

(1) First, we compute a quotient summary of ğº â€², that is: we
identify a partition P = {ğ‘ğ‘– }ğ‘– of its nodes ğ‘ â€², such that
(cid:208)ğ‘– ğ‘ğ‘– = ğ‘ â€² and the ğ‘ğ‘– are pairwise disjont. We say the
nodes from a given set ğ‘ğ‘– are equivalent, and call ğ‘ğ‘– an
equivalence class. Then, the quotient summary of ğº â€² is a
graph whose nodes are the equivalence classes, and such
that whenever ğº â€² contains an edge ğ‘›1 â†’ ğ‘›2, its summary
contains the edge ğ¸ğ¶ (ğ‘›1) â†’ ğ¸ğ¶ (ğ‘›2), where ğ¸ğ¶ (ğ‘›ğ‘– ) denotes
the equivalence class of ğ‘›ğ‘– for ğ‘– âˆˆ {1, 2}. Many quotient sum-
marization techniques have been proposed [5]; we discuss
some of them below. Note that while the quotient summary
guides the abstraction, it may still differ from it quite substan-
tially: a set ğ‘ğ‘– may contain records from multiple collections;
it may contain an explicit collection node; finally, it may
contain nodes which turn out to be in SR.

(2) Next, we compute the signature of each equivalence class, i.e.
an object reflecting the entities extracted out of the nodes
of that equivalence class. This signature holds such statis-
tics for every entity type in ğ‘‡ğ¸ (Section 2.2). For instance,
given the XML document presented in Figure 2, the signa-
ture of the equivalence class representing the nodes âŸ¨authorâŸ©
is: {"total_length":14, "PERSON": {"occurrences":3,
"extracted_length":14}} because the three authors have
been recognised as Person entities.

(3) We consider that each ğ‘ğ‘– is a union of one or more collections,
plus possibly (more rarely) a few records. To separate these,
we proceed as follows:
(a) We cluster the nodes in each ğ‘ğ‘– according to their structure.
(i) We transform ğ‘ğ‘– into a set of transactions D, by turning
each node into a transaction, whose item set is the set
of labels of the nodeâ€™s non-leaf children. Thus, the first
book from the XML bibliography in Figure 2 has the
items title, id, reviews and authors while the second
has title, id and authors.

(ii) Given an item set ğ‘‹ âˆˆ D, we denote by ğ‘† (ğ‘‹ ) the sup-
port of ğ‘‹ , defined by ğ‘† (ğ‘‹ ) = |{ğ‘Œ âˆˆ D | ğ‘‹ âŠ† ğ‘Œ }|. We
also define the transferred support of an item set ğ‘‹ , de-
noted ğ‘‡ (ğ‘‹ ), based on the set of itemsets Y obtained as
follows. Y is initialized with all the proper subsets of ğ‘‹
(not ğ‘‹ and not the empty set) that appear in D. Then,
we traverse Y in the decreasing order of the itemset
size, and remove all subsets of ğ‘Œ from Y. For a given
ğ‘‹ and Y, the transferred support ğ‘‡ (ğ‘‹ ) is defined as:
ğ‘‡ (ğ‘‹ ) = ğ‘† (ğ‘‹ ) + (cid:205)ğ‘Œ |ğ‘Œ âˆˆY ğ‘‡ (ğ‘Œ ).
Our clustering algorithm starts by computing ğ‘† (ğ‘‹ ) and
ğ‘‡ (ğ‘‹ ) for each ğ‘‹ in D. Then, it proceeds in a greedy
manner, choosing the ğ‘‹ âˆˆ D with the highest value
of ğ‘‡ (ğ‘‹ ), and creating a collection ğ‘ containing ğ‘‹ and
all the D transactions whose items are all included in
those of ğ‘‹ . We then remove ğ‘ and the previously se-
lected transactions from D and repeat the procedure.
Each ğ‘ of more than ğ‘¡ elements (where ğ‘¡ is a threshold,

Toward Generic Abstractions for Data of Any Model

e.g., ğ‘¡ = 2) is considered a collection; if the ğ‘ elements
have a common parent, that becomes the collection
node, otherwise, the collection is implicit. For each ğ‘,
every child ğ‘Ÿ âˆˆ ğ‘ is considered a record, part of ğ‘. The
elements of ğ‘ that are not considered collections nor
records are considered sub-records. For instance, in
Figure 2, the XML sample describes 4 collections (blue
nodes): a âŸ¨bibliographyâŸ© containing âŸ¨itemâŸ© records (or-
ange nodes), two sets of âŸ¨authorsâŸ© containing âŸ¨authorâŸ©
records and a set of âŸ¨reviewsâŸ© containing âŸ¨reviewâŸ© records.
The sub-records are the green nodes.

(b) For each record ğ‘Ÿ âˆˆ R, we build its content, i.e. a directed
acyclic graph (DAG) ğ‘‘ğ‘Ÿ by following edges outgoing from
ğ‘Ÿ , until we reach leaf nodes, or another record node ğ‘Ÿ â€²,
or a collection node. The content of each record is repre-
sented by a light yellow box in Figure 2. For instance, in
the XML bibliography, the second âŸ¨bookâŸ© record includes
the âŸ¨titleâŸ©, the two âŸ¨authorâŸ©, and the two âŸ¨reviewâŸ©. Similarly,
for the RDF example, the second âŸ¨bookâŸ© record consists
of its âŸ¨titleâŸ©, plus few âŸ¨authorsâŸ© and âŸ¨reviewsâŸ©. For the re-
lational database, the âŸ¨bookâŸ© record consists of the âŸ¨titleâŸ©
and the âŸ¨idâŸ© of the book. Note that the content of a record
is extensive using transitivity (e.g. the âŸ¨authorâŸ© collection
is part of the âŸ¨bookâŸ©).

In (1), knowledge about possible node types should be injected
in P, thus satisfying R5. R2 is met by including in a record many
nodes reachable from it, in step (3b). Finally, step (3a) satisfies R3
by operating on the graph content (not on the original syntax), and
R4 by detecting both implicit and explicit collections.

We now discuss possible choices for the quotient summary tech-
nique. The most general method, applicable to arbitrary graphs,
consists of building a quotient graph summary [5], such as those
described in [8] which can be built in linear time in the input size. In
particular, a type-first quotient summary [8] uses type information
when available to group nodes by their set of most general types
(an RDF node may have several types, and a PG node may have
several labels), while partitioning untyped nodes according to their
incoming and outgoing nodes. Alternatively, in the particular case
of tree data models, such as XML or JSON, P may be a Dataguide [9],
which can also be constructed in linear time in the size of the input.

4 DISTINGUISHING ENTITIES FROM

RELATIONSHIPS

We now analyse the collections to separate C into Cğ¸ , the set of
entity collections, from Cğ‘…, the set of relationships collection. For
instance, the âŸ¨authorsâŸ© node in the XML data describes entities while
the âŸ¨wroteâŸ© nodes in the property graph describe relationships.

We say a collection ğ‘ is in Cğ‘… iff there exist two collections ğ‘
and ğ‘ such that: âˆ€ğ‘Ÿğ‘ âˆˆ ğ‘, âˆƒ!ğ‘Ÿğ‘ âˆˆ ğ‘, âˆƒ!ğ‘Ÿğ‘ âˆˆ ğ‘ such that ğ‘Ÿğ‘, ğ‘Ÿğ‘ are
connected by an edge in ğ¸ â€² and similarly ğ‘Ÿğ‘, ğ‘Ÿğ‘ are connected by an
ğ¸ â€² edge. If this is not the case, we consider that ğ‘ is a collection of
entities. For instance, in the property graph in Figure 2, each record
in the implicit collection âŸ¨wroteâŸ© links one âŸ¨authorâŸ© and one âŸ¨bookâŸ©,
therefore the collection âŸ¨wroteâŸ© contains relationships; the âŸ¨authorâŸ©
and âŸ¨bookâŸ© collections are collections of entities. This check can be
sped up by exploiting connection statistics that can be gathered
while computing the quotient summary of ğº â€².

5 CLASSIFYING COLLECTIONS
Our next step is to classify each collection in Cğ¸ into a given set
K of categories of interest to the user, or Other if no category
is pertinent. In our example, we consider the categories Person,
Organization, Location, Event and Creative Work. As input to the
classification process, we are also given a set of hints H . A hint â„ âˆˆ
H is a tuple (ğ´, ğ‘™, ğµ) where ğ´ âŠ† K, ğ‘™ is a label and ğµ is a signature
pattern, which is matched (satisfied) by an individual signature, or
not. Such a hint states that a node which has a child labeled ğ‘™ and
its signature matches ğµ, should be classified as one of the types in ğ´.
For instance, the hint ({ğ‘‚ğ‘Ÿğ‘”ğ‘ğ‘›ğ‘–ğ‘§ğ‘ğ‘¡ğ‘–ğ‘œğ‘›}, â„ğ‘ğ‘ ğ¶ğ¸ğ‘‚, {ğ‘ƒğ‘’ğ‘Ÿğ‘ ğ‘œğ‘›}) states
that a record having a property hasCEO, whose signature matches
Person, should be assigned to the category Organization.

5.1 Classification algorithm
Algorithm 1 details our classification.

(1) For each record ğ‘Ÿ âˆˆ ğ‘, we initialize Kğ‘Ÿ , a multiset of candidate
categories for ğ‘Ÿ , and a vector of scores of ğ‘Ÿ for each hint. Kğ‘Ÿ is
a multiset to accomodate the possibility that a given category
may be suggested by several hints.

(2) If the record ğ‘Ÿ has a label semantically close to one of the cat-
egories in K, this category is stored as a candidate category
in Kğ‘Ÿ , and the similarity score recorded in scores.

(3) For each child ğ‘›ğ‘ of the record ğ‘Ÿ , we create a pair ğœ‹ containing

the label of ğ‘›ğ‘ and the signature of ğ‘›ğ‘.

(4) Next, we compute the similarity of ğœ‹ with each hint â„ in
H according to Equation 1. This equation gives the similar-
ity between a node and a hint, based on the label and the
signature of both elements:

ğ‘ ğ‘–ğ‘š(ğœ‹, â„) = ğ‘ ğ‘–ğ‘”_ğ‘ ğ‘–ğ‘š(ğœ‹ .ğ‘†, â„.ğµ)

(cid:213)

+

ğ‘˜ğ‘– âˆˆğ¾1,ğ‘˜ ğ‘— âˆˆğ¾2

ğ‘ğ‘œğ‘ ğ‘–ğ‘›ğ‘’_ğ‘ ğ‘–ğ‘š(ğ‘‰ (ğ‘˜ğ‘– ), ğ‘‰ (ğ‘˜ ğ‘— ))

(1)

where ğ¾1 is the set of keywords present in ğœ‹ .ğ‘™ğ‘ğ‘ğ‘’ğ‘™, ğ¾2 is
the set of keywords present in â„.ğ‘™, and ğ‘ ğ‘–ğ‘”_ğ‘ ğ‘–ğ‘š(ğœ‹ .ğ‘†, â„.ğµ)
is the similarity between the individual signature ğœ‹ .ğ‘† and
the signature pattern â„.ğµ, ğ‘‰ (ğ‘˜ğ‘– ) is a vector representation
(embedding) of keyword ğ‘˜ğ‘– in a multidimensional space. Such
embeddings enable detecting that a node labeled writer is
close to a hint labeled author, even if they are different words.
We currently use the Word2Vec model [11] for this task.
(5) For each ğœ‹, we choose the hint â„ leading to the highest
similarity score for ğœ‹. Each category indicated by the domain
of â„ is added to Kğ‘Ÿ .

(6) We classify the record ğ‘Ÿ in the category that is the most
frequent in Kğ‘Ÿ , if one category is more frequent than 50%;
otherwise, we classify it as Other.

(7) Finally, we classify the collection ğ‘ in the most popular cate-

gory among its records.

5.2 Constructing hints
The classification of the collections depends on the quality of hints
provided as input. Users looking for a concept, e.g., creative work,
may have in mind a few properties that creative works have, such
as title or author, and may provide them as hints. Our approach
works better if there are many hints, in order to obtain a decisive
category vote. To overcome burdening human experts, we use

Algorithm 1: Classifying a collection ğ‘
Input: a collection ğ‘, hints H , categories K

1 foreach ğ‘Ÿ âˆˆ C do
Kğ‘Ÿ â† âˆ…
2
scores â† âˆ…
foreach ğ‘˜ âˆˆ K do

4

3

5

6

7

8

9

10

11

12

if the similarity between ğ‘˜ and the label of ğ‘Ÿ is higher
than a threshold then
Kğ‘Ÿ â† Kğ‘Ÿ âˆª {ğ‘˜ }

foreach ğ‘›ğ‘ âˆˆ ğ‘Ÿ .children do

ğœ‹ â† (ğ‘›ğ‘.ğ‘™ğ‘ğ‘ğ‘’ğ‘™, ğ‘›ğ‘.ğ‘ ğ‘–ğ‘”ğ‘›ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’)
foreach â„ âˆˆ H do

scores â† scores (cid:208)(â„, ğ‘ ğ‘–ğ‘š(ğœ‹, â„))

bestHint â† argmax(scores)
Kğ‘Ÿ â† Kğ‘Ÿ âˆª bestHint.domain

Classify ğ‘Ÿ in the most frequent ğ‘˜ âˆˆ Kğ‘Ÿ , or Other
13
14 Classify ğ‘ with the most frequent category of its records

knowledge bases like Wikidata [16] and Yago [12] to enhance an
existing set of hints, as follows.

A knowledge base ğ¾ğµ consists of triples of the form âŸ¨a,r,bâŸ©,
where r is the relationship between the entities a and b. For example,
the triple âŸ¨Albert Wessels, spouse, Elisabeth EybersâŸ© states that
Albert Wesselsâ€™ spouse is Elisabeth Eybers. It also makes statements
about the entity type, such as âŸ¨Albert Wessels, type, PersonâŸ©,
from which we can obtain the category an entity belongs to.

Let ğ‘˜ âˆˆ K be the category we want to enhance hints for and ğ¾ğµ
be the knowledge base. The set of properties ğ‘ƒğ‘˜ that are likely to
be associated with ğ‘˜ can be acquired using the following equation:

ğ‘ƒğ‘˜ = {ğ‘Ÿ | âŸ¨a,r,bâŸ© âˆˆ ğ¾ğµ âˆ§ âŸ¨a,type,kâŸ© âˆˆ ğ¾ğµ}
(2)
For example, the triple âŸ¨Albert Wessels, type, PersonâŸ© exists in
YAGO, and Albert Wessels participates in triples such as âŸ¨Albert
Wessels, nationality, South AfricaâŸ© and âŸ¨Albert Wessels, spouse,
Elisabeth EybersâŸ© among many others. So, nationality and spouse
would be added to the set ğ‘ƒğ‘ƒğ‘’ğ‘Ÿğ‘ ğ‘œğ‘›.

The acquired set of properties may contain some inaccurate
information, e.g., for the category Organization, some properties
such as date of birth were retrieved. This happens because knowl-
edge bases such as Wikidata are collaboratively created, which can
lead to errors, as evident from the triples âŸ¨Steven Shankman, type,
OrganizationâŸ© and âŸ¨Steven Shankman, date of birth, 1947âŸ©. We
avoid such errors by scoring each property in the set ğ‘ƒğ‘˜ , i.e. errors
such as the one reported above will lead to a low score. Formally:
Category score. This score is set proportional to the number
of instances of the category the property was participating with.
ğ‘_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ (ğ‘, ğ‘˜) for each ğ‘ âˆˆ ğ‘ƒğ‘˜ is computed as follows:

ğ‘_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ (ğ‘, ğ‘˜) = |{ğ‘ | âŸ¨a,p,bâŸ© âˆˆ ğ¾ğµ âˆ§ âŸ¨a,type,kâŸ© âˆˆ ğ¾ğµ}|

(3)

While this prunes away errors, we might still get some properties
which are common for all the categories in K, thus are not very
useful in distinguishing between these categories. For example, the
property Google Knowledge Graph ID appears for all the different
categories. We introduce another score to penalize such properties:

Nelly Barret, Ioana Manolescu, Prajna Upadhyay

Inverse category score. This score quantifies how unique a prop-
erty is for distinguishing between categories. Let ğ‘ƒğ‘ğ‘™ğ‘™ = {ğ‘ƒğ‘˜1
, . . . ,
ğ‘ƒğ‘˜|K | } be the set of sets of properties retrieved for each category
ğ‘˜ğ‘– âˆˆ K according to Equation 2. The inverse category score of a
property ğ‘ is computed as follows:
ğ‘–ğ‘£ğ‘_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ (ğ‘) = ğ‘™ğ‘œğ‘”

1 + |K |
1 + |{ğ‘ƒ | ğ‘ âˆˆ ğ‘ƒ, ğ‘ƒ âˆˆ ğ‘ƒğ‘ğ‘™ğ‘™ }|

, ğ‘ƒğ‘˜2

(4)

(cid:18)

(cid:19)

, ğ‘ƒğ‘˜2

A property ğ‘ that appears in all sets ğ‘ƒğ‘˜ğ‘–
, âˆ€ğ‘– âˆˆ 1, 2, ..., |K | will get
ğ‘–ğ‘£ğ‘_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ (ğ‘) = 0 since ğ‘™ğ‘œğ‘”(1) = 0. A property ğ‘ which appears in
only one of the sets ğ‘ƒğ‘˜1

, ..., ğ‘ƒğ‘˜|K | will get the highest score.

Total score. The total score of a property is given by a product of
category score and inverse category score, as shown by Equation 5.
ğ‘¡ğ‘œğ‘¡_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ (ğ‘) = ğ‘_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ (ğ‘) âˆ— ğ‘–ğ‘£ğ‘_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ (ğ‘) + 1
(5)
We score the properties in decreasing order of ğ‘¡ğ‘œğ‘¡_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ and
retain the top-ğ‘§ ğ‘ƒğ‘˜ğ‘§ properties for each category ğ‘˜. For each ğ‘˜ âˆˆ K
and ğ‘ âˆˆ ğ‘ƒğ‘˜ğ‘§ , we create a hint ({ğ´}, ğ‘, ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ (ğ‘)), where ğ´ = {ğ‘˜ :
ğ‘ âˆˆ ğ‘ƒğ‘˜ğ‘§ } and ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ (ğ‘) is the range of the property ğ‘ if available
from ğ¾ğµ, and âˆ… if not present. We are currently working to make
the signature patterns ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ (ğ‘) more specific, by exploiting the
knowledge the ğ¾ğµ may have about the domain of the property ğ‘.

6 CONCLUSION AND PERSPECTIVES
The approach described above aims at producing expressive abstrac-
tion of datasets organized in a variety of data models. This goes
through their transformation in graphs, identifying records and col-
lections, analyzing and classifying collections. The implementation
and fine-tuning of our system is ongoing.
Acknowledgments. This work is funded by DIM RFSI PHD 2020-
01 and AI Chair SourcesSay project (ANR-20-CHIA-0015-01) grants.

REFERENCES
[1] Z. Abedjan, L. Golab, F. Naumann, and T. Papenbrock. Data Profiling. Synthesis

Lectures on Data Management. Morgan & Claypool Publishers, 2018.

[2] A. C. Anadiotis, O. Balalau, C. Conceicao, H. Galhardas, M. Y. Haddad,
I. Manolescu, T. Merabti, and J. You. Graph integration of structured, semistruc-
tured and unstructured data for data journalism. Information Systems, July 2021.
[3] M. A. Baazizi, C. Berti, D. Colazzo, G. Ghelli, and C. Sartiani. Human-in-the-loop

schema inference for massive JSON datasets. In EDBT, 2020.

[4] M. A. Baazizi, D. Colazzo, G. Ghelli, and C. Sartiani. Parametric schema inference

for massive JSON datasets. VLDB J., 28(4), 2019.

[5] S. Cebiric, F. GoasdouÃ©, H. Kondylakis, D. Kotzinos, I. Manolescu, G. Troullinou,
and M. Zneika. Summarizing Semantic Graphs: A Survey. The VLDB Journal,
28(3), June 2019.

[6] C. Chanial, R. Dziri, H. Galhardas, J. Leblay, M. L. Nguyen, and I. Manolescu.
ConnectionLens: Finding connections across heterogeneous data sources (demon-
stration). PVLDB, 11(12), 2018.

[7] D. Colazzo, G. Ghelli, and C. Sartiani. Schemas for safe and efficient XML

processing. In ICDE. IEEE Computer Society, 2011.

[8] F. GoasdouÃ©, P. Guzewicz, and I. Manolescu. RDF graph summarization for

first-sight structure discovery. The VLDB Journal, 29(5), Apr. 2020.

[9] R. Goldman and J. Widom. Dataguides: Enabling query formulation and opti-

mization in semistructured databases. In VLDB, 1997.

[10] H. Lbath, A. Bonifati, and R. Harmer. Schema inference for property graphs. In

EDBT. OpenProceedings.org, 2021.

[11] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed represen-

tations of words and phrases and their compositionality. In NIPS, 2013.

[12] T. Pellissier Tanon, G. Weikum, and F. Suchanek. Yago 4: A reason-able knowledge

base. In ESWC, 2020.

[13] R. Ramakhrishnan and J. Gehrke. Database Management Systems (3rd edition).

McGraw-Hill, 2003.

[14] A. Schmidt, F. Waas, M. L. Kersten, M. J. Carey, I. Manolescu, and R. Busse. Xmark:

A benchmark for XML data management. In PVLDB, 2002.

[15] W. Spoth, O. A. Kennedy, Y. Lu, B. Hammerschmidt, and Z. H. Liu. Reducing

ambiguity in JSON schema discovery. In SIGMOD, 2021.

[16] D. VrandeÄiÄ‡ and M. KrÃ¶tzsch. Wikidata: A free collaborative knowledgebase.

Commun. ACM, 57(10), Sept. 2014.

