On speaker verification from the neural network
footprint of personalized acoustic models
Natalia Tomashenko, Salima Mdhaffar, Marc Tommasi, Yannick EstÃ¨ve,

Jean-FranÃ§ois Bonastre

To cite this version:

Natalia Tomashenko, Salima Mdhaffar, Marc Tommasi, Yannick EstÃ¨ve, Jean-FranÃ§ois Bonastre. On
speaker verification from the neural network footprint of personalized acoustic models. JournÃ©es
dâ€™Ã‰tudes sur la Parole - JEP2022, Jun 2022, Ãle de Noirmoutier, France. ï¿¿hal-03626964ï¿¿

HAL Id: hal-03626964

https://inria.hal.science/hal-03626964

Submitted on 4 Jul 2022

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

Lâ€™archive ouverte pluridisciplinaire HAL, est
destinÃ©e au dÃ©pÃ´t et Ã  la diffusion de documents
scientifiques de niveau recherche, publiÃ©s ou non,
Ã©manant des Ã©tablissements dâ€™enseignement et de
recherche franÃ§ais ou Ã©trangers, des laboratoires
publics ou privÃ©s.

Sur la vÃ©rification du locuteur Ã  partir de traces dâ€™exÃ©cution de
modÃ¨les acoustiques personnalisÃ©s

Natalia Tomashenko1, Salima Mdhaffar1, Marc Tommasi2, Yannick EstÃ¨ve1,
Jean-FranÃ§ois Bonastre1
(1) LIA, Avignon UniversitÃ©, France; (2) UniversitÃ© de Lille, Inria, France
prÃ©nom.nom@univ-avignon.fr, prÃ©nom.nom@inria.fr

RÃ‰SUMÃ‰
Les modÃ¨les acoustiques personnalisÃ©s sont construits par entraÃ®nement Ã  partir de donnÃ©es provenant
dâ€™un locuteur unique en raffinant un modÃ¨le gÃ©nÃ©rique. Une question importante est de savoir si lâ€™accÃ¨s
Ã  ces modÃ¨les personnalisÃ©s permet facilement de construire une attaque permettant dâ€™identifier le
locuteur associÃ©. Ce problÃ¨me est important dans le contexte de lâ€™apprentissage fÃ©dÃ©rÃ© de modÃ¨les
pour la reconnaissance de la parole oÃ¹ un modÃ¨le global est appris sur un serveur Ã  partir des
modifications des paramÃ¨tres des modÃ¨les reÃ§ues de plusieurs clients. Nous proposons une mÃ©thode
qui consiste Ã  construire des empreintes de ces modÃ¨les Ã  partir des traces de leur application sur
un jeu de donnÃ©es fixe et indÃ©pendant que nous appelons indicateur. GrÃ¢ce Ã  ces empreintes, nous
dÃ©veloppons deux modÃ¨les dâ€™attaques trÃ¨s efficaces qui visent Ã  infÃ©rer lâ€™identitÃ© du locuteur.

ABSTRACT
On speaker verification from the neural network footprint of personalized acoustic models

Speaker personalized acoustic models are obtained from a global model by updating its parameters
using speakerâ€™s data. An important question is whether access to these personalized models allows to
easily build an attack to identify the associated speaker. This problem is especially important in the
context of federated learning of speech recognition acoustic models where a global model is learnt
on the server using the updates received from multiple clients. We propose an approach to analyze
information in neural network acoustic models based on a neural network footprint on the so-called
indicator dataset. Using this method, we develop two very effective attack models that allow to infer
speaker identity from the updated personalized models without access to the usersâ€™ speech data.
MOTS-CLÃ‰S : Vie privÃ©e, apprentissage fÃ©dÃ©rÃ©, modÃ¨les acoustiques, modÃ¨les dâ€™attaques, recon-
naissance vocale, vÃ©rification du locuteur.

KEYWORDS: Privacy, federated learning, acoustic models, attack models, speech recognition,
speaker verification.

1

Introduction

Lâ€™apprentissage fÃ©dÃ©rÃ© (AF) pour la reconnaissance automatique de la parole (RAP) connaÃ®t une
grande popularitÃ© dans plusieurs tÃ¢ches et travaux (Cui et al., 2021; Dimitriadis et al., 2020; Guliani,
2021; Yu et al., 2021; Tomashenko et al., 2022a) 1. Le respect de la vie privÃ©e est lâ€™un des principaux

1. Cet article repose sur le travail prÃ©sentÃ© dans (Tomashenko et al., 2022a).

dÃ©fis de lâ€™AF (Li et al., 2020; Mothukuri et al., 2021). Contrairement aux algorithmes dâ€™apprentissage
classiques qui utilisent un serveur contenant les donnÃ©es dâ€™apprentissage, lâ€™AF apprend sur des
donnÃ©es stockÃ©es localement et communique uniquement les modifications (mises Ã  jour). Ceci permet
de protÃ©ger les donnÃ©es personnelles puisquâ€™elles ne sont ni stockÃ©es dans un serveur, ni partagÃ©es
avec dâ€™autres utilisateurs. Cependant, ces mises Ã  jour peuvent encore contenir certaines informations
sensibles (Geiping et al., 2020; Carlini et al., 2019). Des travaux rÃ©cents ont montrÃ© que les modÃ¨les
appris par lâ€™AF sont vulnÃ©rables Ã  diffÃ©rents types dâ€™attaques (Truex et al., 2019; Wang et al., 2019).
Les techniques pour amÃ©liorer la confidentialitÃ© dans un cadre de lâ€™AF sâ€™appuient principalement sur
deux approches (Mothukuri et al., 2021) : le calcul multipartite sÃ©curisÃ© (Bonawitz et al., 2016) et la
confidentialitÃ© diffÃ©rentielle (Dwork, 2006). Les mÃ©thodes de chiffrement (Smaragdis & Shashanka,
2007) comme le chiffrement entiÃ¨rement homomorphe (Smaragdis & Shashanka, 2007) et le calcul
multipartite sÃ©curisÃ© effectuent le calcul dans le domaine cryptÃ©. Ces mÃ©thodes sont trop coÃ»teuses
en termes de calcul. Les mÃ©thodes de confidentialitÃ© diffÃ©rentielle prÃ©servent la confidentialitÃ© en
ajoutant du bruit aux paramÃ¨tres des utilisateurs (Dwork, 2006). Cependant, ces solutions peuvent
dÃ©grader les performances dâ€™apprentissage Ã  cause de lâ€™incertitude quâ€™elles introduisent dans les
paramÃ¨tres. Les mÃ©thodes alternatives Ã  la protection de la confidentialitÃ© pour la parole comprennent
les mÃ©thodes de suppression qui sont destinÃ©es Ã  lâ€™analyse des sons ambiants, et lâ€™anonymisation
(Tomashenko et al., 2022b) qui vise Ã  supprimer les informations personnelles identifiables dans
le signal vocal en gardant tous les autres attributs. Ces mÃ©thodes de protection de la confidentialitÃ©
peuvent Ãªtre combinÃ©es et intÃ©grÃ©es de maniÃ¨re hybride dans un cadre dâ€™AF.

MalgrÃ© lâ€™intÃ©rÃªt rÃ©cent portÃ© Ã  lâ€™AF pour la RAP et Ã  dâ€™autres tÃ¢ches telles que le repÃ©rage de mots-
clÃ©s (Leroy et al., 2019), la reconnaissance des Ã©motions (Latif et al., 2020), et la vÃ©rification du
locuteur (Granqvist et al., 2020), il existe trÃ¨s peu dâ€™Ã©tudes sur les attaques de confidentialitÃ©, dans
un contexte dâ€™AF, des modÃ¨les acoustiques (MA) pour la reconnaissance de la parole. Il a tout de
mÃªme Ã©tÃ© montrÃ© rÃ©cemment quâ€™il est possible dâ€™extraire des informations sur le locuteur Ã  partir
des modifications portÃ©es sur les poids dâ€™un modÃ¨le acoustique neuronal lors de sa personnalisation
(Mdhaffar et al., 2022).

Nos travaux sâ€™inscrivent dans le cadre de ces attaques : nous Ã©tudions les informations propres au
locuteur qui peuvent Ãªtre extraites Ã  partir de modÃ¨les acoustiques personnalisÃ©s mis Ã  jour localement.
Nous explorons diffÃ©rentes modÃ¨les dâ€™attaques qui opÃ¨rent directement sur les paramÃ¨tres du modÃ¨le
mis Ã  jour sans avoir accÃ¨s aux donnÃ©es rÃ©elles de lâ€™utilisateur. Lâ€™idÃ©e principale des mÃ©thodes
proposÃ©es est dâ€™utiliser un jeu de donnÃ©es externe (indicateur) pour analyser lâ€™empreinte des modÃ¨les
acoustiques sur ces donnÃ©es. Une autre contribution importante de ce travail concerne lâ€™analyse des
informations sur le locuteur reprÃ©sentÃ©es dans les modÃ¨les acoustiques neuronaux adaptÃ©s.

2 Apprentissage fÃ©dÃ©rÃ© pour les modÃ¨les acoustiques de RAP

Nous considÃ©rons un scÃ©nario classique dâ€™apprentissage fÃ©dÃ©rÃ© oÃ¹ un modÃ¨le acoustique neuronal
global est entraÃ®nÃ© sur un serveur Ã  lâ€™aide des donnÃ©es stockÃ©es localement sur plusieurs dispositifs
distants (Li et al., 2020). Lâ€™apprentissage du modÃ¨le global est effectuÃ© sous la contrainte que les
donnÃ©es vocales dâ€™apprentissage sont stockÃ©es et traitÃ©es localement sur les dispositifs des utilisateurs
(clients). Seules les mises Ã  jour du modÃ¨le sont transmises au serveur Ã  partir de chaque client. Le
modÃ¨le global est appris sur le serveur en fonction des mises Ã  jour reÃ§ues de plusieurs clients. La
Figure 1 illustre lâ€™AF dans un rÃ©seau distribuÃ© de clients. Tout dâ€™abord, le modÃ¨le acoustique initial de

reconnaissance de la parole Wg est distribuÃ© Ã  lâ€™ensemble des systÃ¨mes des N utilisateurs (locuteurs).
Ensuite, le modÃ¨le global initial est exÃ©cutÃ© sur chaque dispositif utilisateur si (i âˆˆ 1..N ) et mis Ã  jour
localement sur les donnÃ©es privÃ©es de lâ€™utilisateur. Les modÃ¨les mis Ã  jour Wsi sont ensuite transmis
au serveur oÃ¹ ils sont agrÃ©gÃ©s pour obtenir un nouveau modÃ¨le global W âˆ—
g . En gÃ©nÃ©ral, les modÃ¨les
personnalisÃ©s mis Ã  jour sont agrÃ©gÃ©s en utilisant la moyenne fÃ©dÃ©rÃ©e et ses variations (McMahan
et al., 2017). Ensuite, le modÃ¨le global mis Ã  jour W âˆ—
g est partagÃ© avec les clients. Ce processus est
rÃ©pÃ©tÃ© plusieurs fois jusquâ€™Ã  la convergence du modÃ¨le ou en fixant un nombre dâ€™itÃ©rations. Lâ€™utilitÃ© et
lâ€™efficacitÃ© de lâ€™apprentissage des modÃ¨les entraÃ®nÃ©s dans le cadre dâ€™AF ont Ã©tÃ© Ã©tudiÃ©es avec succÃ¨s
dans des travaux rÃ©cents (Cui et al., 2021; Dimitriadis et al., 2020; Guliani, 2021; Yu et al., 2021).
Dans cette Ã©tude, nous nous concentrons sur lâ€™aspect de la protection de la vie privÃ©e.

FIGURE 1 â€“ Apprentissage fÃ©dÃ©rÃ© dans un rÃ©seau distribuÃ© de clients : 1) TÃ©lÃ©chargement du modÃ¨le
global Wg par les clients. 2) Adaptation au locuteur de Wg sur les appareils locaux en utilisant
les donnÃ©es privÃ©es de lâ€™utilisateur. 3) Collecte et agrÃ©gation de plusieurs modÃ¨les personnalisÃ©s
Ws1,...,WsN sur le serveur. 4) Partage du modÃ¨le rÃ©sultant W âˆ—

g avec les diffÃ©rents clients.

3 ModÃ¨les dâ€™attaques

Dans cette section, nous dÃ©crivons le scÃ©nario de protection de la confidentialitÃ© et nous prÃ©sentons
deux modÃ¨les dâ€™attaques.

3.1 ScÃ©nario de prÃ©servation de la confidentialitÃ©

La prÃ©servation de la confidentialitÃ© est formulÃ©e comme un jeu entre des utilisateurs qui partagent
certaines donnÃ©es et des attaquants qui accÃ¨dent Ã  ces donnÃ©es ou Ã  des donnÃ©es dÃ©rivÃ©es de celles-ci
et visent Ã  dÃ©duire des informations sur les utilisateurs (Tomashenko et al., 2020). Les attaquants
visent Ã  attaquer les utilisateurs en utilisant les informations dÃ©tenues par le serveur. Ils peuvent avoir
accÃ¨s Ã  des modÃ¨les personnalisÃ©s. Dans ce travail, nous supposons quâ€™un attaquant a accÃ¨s aux
donnÃ©es suivantes :

â€” Un modÃ¨le global initial Wg.

â€” Un modÃ¨le personnalisÃ© Ws du locuteur cible (target) s qui est inscrit dans le systÃ¨me dâ€™ap-
prentissage fÃ©dÃ©rÃ©. Le modÃ¨le personnalisÃ© correspondant a Ã©tÃ© obtenu Ã  partir du modÃ¨le

ServerClient 1Client 2Client Nğ‘Šğ‘”ğ‘Šğ‘”âˆ—ğ‘Šğ‘ 1ğ‘Šğ‘ 2ğ‘Šğ‘ ğ‘ğ‘Šğ‘”ğ‘Šğ‘”ğ‘Šğ‘”111222444333global Wg en ajustant par fine-tuning les poids de Wg Ã  lâ€™aide des donnÃ©es du locuteur. Nous
considÃ©rons ce modÃ¨le comme enrollment pour un attaquant.

â€” Des modÃ¨les personnalisÃ©s des locuteurs non-cibles (non-target) et cibles (target) : Ws1 ,...,WsN .

Nous appellerons ces modÃ¨les test trial.

Lâ€™objectif de lâ€™attaquant est de rÃ©aliser une tÃ¢che de vÃ©rification automatique du locuteur (VAL)
en utilisant le modÃ¨le de donnÃ©es dâ€™inscription (enrollment) sous la forme de Ws et les donnÃ©es
dâ€™essai (test trial) de test sous la forme de modÃ¨les Ws1,...,WsN . Autrement dit, Ã©tant donnÃ© un
modÃ¨le Ws correspondant au locuteur cible, la tÃ¢che consiste Ã  identifier quels modÃ¨les parmi les
Wsi correspondent ou non au locuteur cible (Bonastre et al., 2021).

3.2 ModÃ¨les dâ€™attaques

Les deux approches proposÃ©es reposent sur lâ€™hypothÃ¨se que nous pouvons capturer des informations
sur lâ€™identitÃ© du locuteur s Ã  partir du modÃ¨le correspondant adaptÃ© au locuteur Ws et du modÃ¨le
global Wg en comparant les sorties de ces deux modÃ¨les acoustiques neuronaux provenant des
couches cachÃ©es h sur certaines donnÃ©es vocales. Nous appellerons ces donnÃ©es vocales indicateur.
Ces donnÃ©es ne sont liÃ©es ni aux donnÃ©es de test ni aux donnÃ©es dâ€™apprentissage des modÃ¨les.

ModÃ¨le dâ€™attaque A1. La Figure 2 illustre la VAL pour le modÃ¨le dâ€™attaque A1 proposÃ© dans
ce papier. Ce modÃ¨le dâ€™attaque comporte plusieurs Ã©tapes. On considÃ¨re un ensemble dâ€™Ã©noncÃ©s
(utterances) dans lâ€™ensemble de donnÃ©es indicateur I = {u1, . . . , uJ } ; une sÃ©quence de vecteurs
j } ; un ensemble de modÃ¨les personnalisÃ©s W = {Ws1 , . . . , WsN } ;
dans lâ€™Ã©noncÃ© uj={u1
et un identifiant dâ€™une couche cachÃ©e dans le MA global ou personnalisÃ© reprÃ©sentÃ© par h. Nous
prÃ©sentons ci-dessous les Ã©tapes pour le modÃ¨le dâ€™attaque A1.

j , . . ., uTj

1. âˆ€ Wsi âˆˆ W, âˆ€ uj âˆˆ I nous calculons les valeurs dâ€™activation de la couche h pour les paires
t=1, et les diffÃ©rences par vecteur

g (uj) = {wh,t

t=1 et W h

g,j }Tj

si,j}Tj
de modÃ¨les : W h
si
entre les sorties correspondantes :

(uj) = {wh,t

âˆ†h

si (uj) = {âˆ†h,t

si,j}Tj

t=1,

oÃ¹ âˆ†h,t

si,j = wh,t

si,j âˆ’ wh,t
g,j ,

t âˆˆ 1..Tj.

(1)

2. Pour chaque modÃ¨le personnalisÃ©, nous calculons les vecteurs de moyenne et dâ€™Ã©cart type pour

âˆ†h,t

si,j sur tous les vecteurs de parole dans les donnÃ©es indicateur I :

Âµh
si

=

(cid:80)I

j=1

(cid:80)Tj

t=1 âˆ†h,t
si,j

(cid:80)I

j=1 Tj

(cid:32) (cid:80)I

j=1

et Ïƒh

si =

si,j âˆ’ Âµh
si

(cid:80)Tj

t=1(âˆ†h,t
(cid:80)I
j=1 Tj

(cid:33) 1

2

)2

.

(2)

3. Pour une paire de modÃ¨les personnalisÃ©s Wsi and Wsk , nous calculons un score de similaritÃ©
Ï pour la couche cachÃ©e h sur lâ€™ensemble de donnÃ©es indicateur sur la base de la distance
euclidienne normalisÃ©e L2 entre les paires de vecteurs correspondants pour les moyennes et les
Ã©carts types :

Ï(W h

si , W h

sk ) = Î±Âµ

âˆ¥Âµh
si
âˆ¥Âµh
si

âˆ’ Âµh
sk
âˆ¥2âˆ¥Âµh
sk

âˆ¥2
âˆ¥2

+ Î±Ïƒ

âˆ¥Ïƒh
âˆ¥Ïƒh

si âˆ’ Ïƒh
si âˆ¥2âˆ¥Ïƒh

sk âˆ¥2
sk âˆ¥2

,

(3)

oÃ¹ Î±Âµ, Î±Ïƒ sont des paramÃ¨tres fixes dans toutes les expÃ©riences.

4. En utilisant les scores de similaritÃ© obtenues pour toutes les paires de matrices, nous pouvons

effectuer une tÃ¢che de vÃ©rification du locuteur.

FIGURE 2 â€“ Calcul des statistiques pour le modÃ¨le dâ€™attaque A1.

ModÃ¨le dâ€™attaque A2. Pour le deuxiÃ¨me modÃ¨le dâ€™attaque, nous entraÃ®nons un modÃ¨le rÃ©seau
neuronal (RN) comme indiquÃ© dans la Figure 3. Ce modÃ¨le RN utilise des modÃ¨les personnalisÃ©s
et le modÃ¨le globale ainsi que le jeu de donnÃ©es indicateur pour lâ€™apprentissage. Il est entraÃ®nÃ© Ã 
prÃ©dire lâ€™identitÃ© dâ€™un locuteur Ã  partir du modÃ¨le personnalisÃ© correspondant. Lorsque le modÃ¨le est
entraÃ®nÃ©, nous lâ€™utilisons au moment de lâ€™Ã©valuation pour extraire les reprÃ©sentations (embeddings) du
locuteur de maniÃ¨re similaire aux x-vecteurs (Snyder et al., 2018). Le modÃ¨le est composÃ© de deux
parties (une partie fixe et une partie Ã  entraÃ®ner). Les sorties de la partie fixe sont les sÃ©quences âˆ†h
si de
vecteurs calculÃ©s sur les donnÃ©es indicateur comme dÃ©fini dans la Formule (1). Pour chaque modÃ¨le
personnalisÃ© Wsi, nous calculons âˆ†h
(u) sera
utilisÃ© comme entrÃ©e de la deuxiÃ¨me partie (entraÃ®nÃ©e) du RN, qui comprend plusieurs couches de RN
de type time delay neural network (TDNN) (Peddinti et al., 2015) et une couche de type statistical
pooling.

si pour tous les Ã©noncÃ©s u du corpus indicateur. âˆ†h
si

FIGURE 3 â€“ Apprentissage dâ€™un extracteur dâ€™embeddings du locuteur pour le modÃ¨le dâ€™attaque A2.

4 RÃ©sultats expÃ©rimentaux

4.1 DonnÃ©es

Les expÃ©riences ont Ã©tÃ© menÃ©es sur la partition dâ€™adaptation au locuteur du corpus TED-LIUM 3
(Hernandez et al., 2018). Ce corpus disponible publiquement contient les confÃ©rences TED qui

Pool de modÃ¨lespersonnalisÃ©sIndicateurdonnÃ©esvocalesğ‘Šğ‘ 1â„(ğ‘¢)ğ‘Šğ‘”â„(ğ‘¢)ğ‘Šğ‘ ğ‘ğ‘Šğ‘ 1ğ‘Šğ‘ 2ğ‘Šğ‘”ğ‘¢ğ‘¢ğ‘Šğ‘ ğ‘â„(ğ‘¢)Î”ğ‘ 1â„(ğ‘¢)Î”ğ‘ ğ‘â„(ğ‘¢)[Î¼ğ‘ 1â„, ğœğ‘ 1â„][Î¼ğ‘ ğ‘â„, ğœğ‘ ğ‘â„]â€¦IndicateurdonnÃ©esvocalesEmbedding du locuteurğ‘ Couche TDNN Stat poolingCouches fixesğ‘Šğ‘ â„ğ‘Šğ‘”â„âˆ†ğ‘ Softmax: identifiantsdes locuteursPool de modÃ¨lespersonnalisÃ©sğ‘Šğ‘ Couche TDNN reprÃ©sentent 452 heures de donnÃ©es vocales en anglais provenant dâ€™environ 2K locuteurs, 16kHz.
Nous avons sÃ©lectionnÃ© trois ensembles de donnÃ©es Ã  partir du corpus dâ€™entraÃ®nement TED-LIUM 3 :
Train-G, Part-1, Part-2 avec des sous-ensembles de locuteurs disjoints comme indiquÃ© dans le
tableau 1. Le jeu de donnÃ©es indicateur a Ã©tÃ© utilisÃ© pour entraÃ®ner les modÃ¨les dâ€™attaques. Il est
composÃ© de 320 Ã©noncÃ©s (utterances) sÃ©lectionnÃ©s parmi les 32 locuteurs des ensembles de donnÃ©es
de test et de dÃ©veloppement du corpus TED-LIUM 3. Les locuteurs du jeu de donnÃ©es indicateur
sont disjoints des locuteurs de Train-G, Part-1, et Part-2. Pour chaque locuteur de lâ€™ensemble de
donnÃ©es indicateur, nous sÃ©lectionnons uniquement 10 Ã©noncÃ©s. La taille totale du jeu de donnÃ©es
indicateur est de 32 minutes. Lâ€™ensemble de donnÃ©es Train-G a Ã©tÃ© utilisÃ© pour entraÃ®ner un modÃ¨le
acoustique global initial Wg. Part-1 et Part-2 ont Ã©tÃ© utilisÃ©s pour obtenir deux ensembles de modÃ¨les
personnalisÃ©s.

Train-G Part-1 Part-2 Indicateur

DurÃ©e, heures
200
880
Nombre de locuteurs
Nombre de modÃ¨les personnalisÃ©s â€”

86
736
1300

73
0.5
32
634
1079 â€”

TABLE 1 â€“ Statistiques de donnÃ©es

4.2 ModÃ¨les acoustiques pour la reconnaissance automatique de la parole

Les modÃ¨les acoustiques pour la RAP suivent une architecture neuronale de type TDNN (Peddinti
et al., 2015) et ont Ã©tÃ© entraÃ®nÃ©s en utilisant la boÃ®te Ã  outils de reconnaissance vocale Kaldi (Povey
et al., 2011). Les coefficients cepstraux de frÃ©quence Mel (MFCC) Ã  40 dimensions, concatÃ©nÃ©s Ã 
des i-vecteurs Ã  100 dimensions, ont Ã©tÃ© utilisÃ©s comme entrÃ©e dans les rÃ©seaux de neurones. Chaque
modÃ¨le comporte treize couches cachÃ©es de 512 dimensions, suivies dâ€™une couche softmax dans
laquelle 3664 Ã©tats de triphonie ont Ã©tÃ© utilisÃ©s comme target 2. Le modÃ¨le global initial Wg a Ã©tÃ©
entraÃ®nÃ© en utilisant le critÃ¨re lattice-free maximum mutual information (LF-MMI) (Povey et al.,
2016). Les deux types de stratÃ©gies dâ€™augmentation des donnÃ©es vocales ont Ã©tÃ© appliquÃ©es pour les
donnÃ©es dâ€™entraÃ®nement et dâ€™adaptation : perturbation de la vitesse (avec des facteurs de 0,9, 1,0, 1,1)
et perturbation du volume, comme dans (Peddinti et al., 2015). Chaque modÃ¨le est composÃ© dâ€™environ
13.8 millions de paramÃ¨tres. Le modÃ¨le global initial Wg a Ã©tÃ© entraÃ®nÃ© sur le Train-G. Des modÃ¨les
personnalisÃ©s Wsi ont Ã©tÃ© obtenus en ajustant finement tous les paramÃ¨tres de Wg sur les donnÃ©es des
locuteurs de Part-1 et Part-2 comme dÃ©crit dans (Tomashenko et al., 2022a). Pour tous les modÃ¨les
personnalisÃ©s de locuteurs, nous utilisons approximativement la mÃªme quantitÃ© de donnÃ©es vocales
pour effectuer le rÃ©glage fin (adaptation du locuteur ou fine-tuning) â€“ environ 4 minutes par modÃ¨le.
Pour la plupart des locuteurs (564 dans Part-1, 463 dans Part-2), nous avons obtenu deux modÃ¨les
personnalisÃ©s diffÃ©rents (par locuteur) sur des sous-ensembles dâ€™adaptation disjoints, pour les autres
locuteurs, nous ne disposons de donnÃ©es dâ€™adaptation que pour un seul modÃ¨le.

4.3 ModÃ¨les dâ€™attaques

Nous Ã©tudions deux approches pour les modÃ¨les dâ€™attaques : A1 â€“ une approche simple basÃ©e sur
lâ€™analyse statistique comparative des sorties du modÃ¨le de RN et le score de similaritÃ© associÃ© entre

2. En suivant la notation de (Peddinti et al., 2015), la configuration du modÃ¨le peut Ãªtre dÃ©crite comme suit : {-1,0,1} Ã— 6

couches ; {-3,0,3} Ã— 7 couches.

les modÃ¨les personnalisÃ©s, et A2 â€“ une approche basÃ©e sur les RN. Pour les essais sur les cibles (test
target trials), nous utilisons des comparaisons entre diffÃ©rents modÃ¨les personnalisÃ©s des mÃªmes
locuteurs (564 dans le Part-2), et pour les essais sur les non-cibles (test non-target trials), nous avons
sÃ©lectionnÃ© alÃ©atoirement 10K paires de modÃ¨les de diffÃ©rents locuteurs (choisis au hasard parmi
toutes les 1079 Ã— 1078/2 comparaisons possibles) dans un ensemble de donnÃ©es correspondant.

ModÃ¨le dâ€™attaque A1. Le premier modÃ¨le dâ€™attaque a Ã©tÃ© appliquÃ© comme dÃ©crit dans la sec-
tion (3.2). Les paramÃ¨tres Î±Âµ, Î±Ïƒ dans la formule (3) sont respectivement Ã©gaux Ã  1 et 10. Ce modÃ¨le
a Ã©tÃ© Ã©valuÃ© sur un jeu de donnÃ©es de modÃ¨les personnalisÃ©s. Le jeu de donnÃ©es indicateur est le
mÃªme dans toutes les expÃ©riences.

ModÃ¨le dâ€™attaque A2. Pour entraÃ®ner le modÃ¨le dâ€™attaque A2, nous utilisons 1300 modÃ¨les de
locuteurs personnalisÃ©s correspondant Ã  736 locuteurs uniques de Part-1. Lorsque nous avons appliquÃ©
la partie fixe de lâ€™architecture prÃ©sentÃ©e dans la Figure 3 au jeu de donnÃ©es indicateur de 32 minutes
pour chaque modÃ¨le de locuteur dans Part-1, nous avons obtenu les donnÃ©es dâ€™entraÃ®nement avec la
quantitÃ© correspondant Ã  environ 693h (32Ã—1300). La partie entraÃ®nÃ©e du modÃ¨le neuronal, illustrÃ©e
dans la Figure 3, a une topologie similaire Ã  celle dâ€™un extracteur de x-vecteur conventionnel (Snyder
et al., 2018). Cependant, lâ€™extracteur de x-vecteur permet de prÃ©dire lâ€™identitÃ© du locuteur pour le
segment de discours donnÃ© alors que notre modÃ¨le proposÃ© apprend Ã  prÃ©dire lâ€™identitÃ© du locuteur
Ã  partir de la partie W h
s dâ€™un modÃ¨le personnalisÃ© du locuteur. Nous avons entraÃ®nÃ© deux modÃ¨les
dâ€™attaques correspondant aux deux valeurs du paramÃ¨tre h âˆˆ {1, 5} â€“ une couche cachÃ©e dans les MA
neuronaux RAP Ã  laquelle nous calculons les activations. Les valeurs h ont Ã©tÃ© choisies en fonction
des rÃ©sultats obtenus pour le modÃ¨le dâ€™attaque A1. La dimension de sortie de la partie fixe est 512. La
partie fixe est suivie par la partie entraÃ®nÃ©e qui consiste en sept couches TDNN cachÃ©es et une couche
de regroupement statistique introduite aprÃ¨s la cinquiÃ¨me couche TDNN. La sortie est une couche
softmax avec les cibles (sorties) correspondant aux locuteurs dans le pool de modÃ¨les personnalisÃ©s
de locuteurs (nombre de locuteurs uniques dans Part-1).

4.4 RÃ©sultats

Les modÃ¨les dâ€™attaques ont Ã©tÃ© Ã©valuÃ©s en termes de equal error rate (EER) 3. Les rÃ©sultats du modÃ¨le
dâ€™attaque A1 sont prÃ©sentÃ©s dans la Figure 4. Les informations de locuteur peuvent Ãªtre capturÃ©es
pour toutes les valeurs de h avec un succÃ¨s variable : EER varie de 0,86% (pour la premiÃ¨re couche
cachÃ©e) Ã  20,51% (pour la couche cachÃ©e supÃ©rieure). Pour analyser lâ€™impact de chaque partie de
la somme de la formule (3) sur les performances de la VAL, nous calculons sÃ©parÃ©ment le score de
similaritÃ© Ï en utilisant uniquement les moyennes (Î±Ïƒ = 0) ou uniquement les Ã©carts types (Î±Âµ = 0).
Lâ€™impact de chaque terme de la somme change pour les diffÃ©rentes couches cachÃ©es. Lorsque nous
utilisons uniquement les Ã©carts-types, nous observons le plus faible EER sur la premiÃ¨re couche.
Dans le cas de lâ€™utilisation des moyennes uniquement, la premiÃ¨re couche est, au contraire, lâ€™une des
moins informatives pour la vÃ©rification du locuteur. Pour toutes les autres couches, la combinaison
des moyennes et des Ã©carts-types fournit des rÃ©sultats supÃ©rieurs Ã  ceux obtenus dans les cas oÃ¹ une
seule de ces composantes est utilisÃ©e.

Nous choisissons deux valeurs h âˆˆ {1, 5} qui montrent des rÃ©sultats prometteurs pour le modÃ¨le
A1, et nous utilisons les sorties correspondantes pour entraÃ®ner deux modÃ¨les dâ€™attaques avec la

3. En dÃ©signant par Pfa(Î¸) et Pmiss(Î¸) les taux de faux positifs (false alarm) et de faux nÃ©gatifs (miss rates) au seuil Î¸,
lâ€™EER correspond au seuil Î¸EER pour lequel les deux taux dâ€™erreur de dÃ©tection sont Ã©gaux : EER = Pfa(Î¸EER) = Pmiss(Î¸EER).

FIGURE 4 â€“ EER, % pour le modÃ¨le dâ€™attaque A1 en fonction de la couche cachÃ©e h, Ã©valuÃ© sur Part-2. Âµ + Ïƒ
â€“ les moyennes et les Ã©carts types ont Ã©tÃ© utilisÃ©s pour calculer le score de similaritÃ©. Ï ; Âµ â€“ uniquement les
moyennes ; et Ïƒ â€“ uniquement les Ã©carts types ont Ã©tÃ© utilisÃ©s.

configuration A2. Les rÃ©sultats comparatifs des deux modÃ¨les dâ€™attaques sont prÃ©sentÃ©s dans le
tableau 2. Pour h = 5, le deuxiÃ¨me modÃ¨le dâ€™attaque fournit une amÃ©lioration significative des
performances par rapport au premier et rÃ©duit lâ€™EER de 7% Ã  2%. Pour h = 1, nous nâ€™avons pu
obtenir aucune amÃ©lioration en entraÃ®nant un modÃ¨le dâ€™attaque basÃ© sur un rÃ©seau de neurones : les
rÃ©sultats pour A2 dans ce cas sont moins bons par rapport Ã  lâ€™approche simple A1.

ModÃ¨le dâ€™attaque

h=1

h=5

A1
A2

0.86
12.31

7.11
1.94

TABLE 2 â€“ EER, % Ã©valuÃ© sur Part-2, h - indicateur dâ€™une couche cachÃ©e

5 Conclusions

Dans cette Ã©tude, nous nous sommes concentrÃ©s sur le problÃ¨me de la protection de la vie privÃ©e pour
les modÃ¨les acoustique de RAP construits dans un cadre dâ€™apprentissage fÃ©dÃ©rÃ©. Nous avons explorÃ©
dans quelle mesure ces modÃ¨les de RAP sont vulnÃ©rables aux attaques contre la confidentialitÃ©. Nous
avons dÃ©veloppÃ© deux modÃ¨les dâ€™attaques qui visent Ã  dÃ©duire lâ€™identitÃ© du locuteur Ã  partir des
modÃ¨les personnalisÃ©s mis Ã  jour localement sans avoir accÃ¨s aux donnÃ©es vocales des locuteurs
cibles. Un modÃ¨le dâ€™attaque est basÃ© sur le score de similaritÃ© proposÃ© entre les modÃ¨les acoustiques
personnalisÃ©s, calculÃ© sur un ensemble de donnÃ©es indicateur externe, et un autre est un modÃ¨le
neuronal. Nous avons dÃ©montrÃ© sur le corpus TED-LIUM 3 que les deux modÃ¨les dâ€™attaque sont trÃ¨s
efficaces et peuvent fournir un EER dâ€™environ 1% pour le modÃ¨le dâ€™attaque simple A1 et 2% pour le
modÃ¨le dâ€™attaque neuronal A2. Une autre contribution importante de ce travail est la dÃ©couverte que la
premiÃ¨re couche des modÃ¨les acoustiques personnalisÃ©s contient une grande quantitÃ© dâ€™informations
sur le locuteur qui sont principalement contenues dans les valeurs de dÃ©viation standard calculÃ©es sur
les donnÃ©es indicateur. Cette propriÃ©tÃ© intÃ©ressante des modÃ¨les acoustiques neuronaux personnalisÃ©s
ouvre de nouvelles perspectives Ã©galement pour la VAL. Dans des travaux futurs, nous prÃ©voyons de
lâ€™utiliser pour dÃ©velopper un systÃ¨me VAL efficace.

Remerciements
Ces travaux ont Ã©tÃ© financÃ© par les projets VoicePersonae (ANR-18-JSTS-0001), DEEP-PRIVACY (ANR18-CE23-0018) et
programme de Recherche et dâ€™Innovation Horizon 2020 (Marie SkÅ‚odowska-Curie grant, No 101007666). Ces travaux ont
bÃ©nÃ©ficiÃ© dâ€™un accÃ¨s aux moyens de calcul de lâ€™IDRIS au travers de lâ€™allocation de ressources 2021-AD011013331 attribuÃ©e
par GENCI.

123456789101112130510152025300.868.4112.977.947.117.988.8611.9813.1714.1516.0818.9620.51+Couche cachÃ©eEER,%RÃ©fÃ©rences

BONASTRE J.-F. et al. (2021). Benchmarking and challenges in security and privacy for voice
biometrics. In Proc. 2021 ISCA Symposium on Security and Privacy in Speech Communication, p.
52â€“56.

BONAWITZ K., IVANOV V., KREUTER B. et al. (2016). Practical secure aggregation for federated
learning on user-held data. arXiv preprint arXiv :1611.04482.

CARLINI N., LIU C., ERLINGSSON Ãš., KOS J. & SONG D. (2019). The secret sharer : Evaluating
and testing unintended memorization in neural networks. p. 267â€“284.

CUI X. et al. (2021). Federated acoustic modeling for automatic speech recognition. In ICASSP.

DIMITRIADIS D. et al. (2020). A federated approach in training acoustic models. p. 981â€“985.

DWORK C. (2006). Differential privacy.

GEIPING J. et al. (2020). Inverting gradientsâ€“how easy is it to break privacy in federated learning ?

GRANQVIST F. et al. (2020). Improving on-device speaker verification using federated learning
with privacy.

GULIANI D. A. (2021). Training speech recognition models with federated learning : A quality/cost
framework.

HERNANDEZ F., NGUYEN V., GHANNAY S., TOMASHENKO N. et al. (2018). TED-LIUM 3 :
twice as much data and corpus repartition for experiments on speaker adaptation. p. 198â€“208.

LATIF S. et al. (2020). Federated learning for speech emotion recognition applications. p. 341â€“342.

LEROY D. et al. (2019). Federated learning for keyword spotting. p. 6341â€“6345.

LI T., SAHU A. K., TALWALKAR A. & SMITH V. (2020). Federated learning : Challenges, methods,
and future directions. IEEE Signal Processing Magazine, 37(3), 50â€“60.

MCMAHAN B., MOORE E., RAMAGE D. et al. (2017). Communication-efficient learning of deep
networks from decentralized data. p. 1273â€“1282.

MDHAFFAR S. et al. (2022). Retrieving speaker information from personalized acoustic models for
speech recognition. In ICASSP.

MOTHUKURI V., PARIZI R. M., POURIYEH S., HUANG Y. et al. (2021). A survey on security and
privacy of federated learning. Future Generation Computer Systems, 115, 619â€“640.

PEDDINTI V., POVEY D. & KHUDANPUR S. (2015). A time delay neural network architecture for
efficient modeling of long temporal contexts.

POVEY D., GHOSHAL A., BOULIANNE G. et al. (2011). The Kaldi speech recognition toolkit.

POVEY D., PEDDINTI V., GALVEZ D., GHAHREMANI P., MANOHAR V., NA X. et al. (2016).
Purely sequence-trained neural networks for ASR based on lattice-free MMI. p. 2751â€“2755.

SMARAGDIS P. & SHASHANKA M. (2007). A framework for secure speech recognition. IEEE
Transactions on Audio, Speech, and Language Processing, 15(4), 1404â€“1413.

SNYDER D., GARCIA-ROMERO D., SELL G., POVEY D. & KHUDANPUR S. (2018). X-vectors :
Robust DNN embeddings for speaker recognition. p. 5329â€“5333.

TOMASHENKO N., MDHAFFAR S., TOMMASI M., ESTÃˆVE Y. & BONASTRE J.-F. (2022a).
Privacy attacks for automatic speech recognition acoustic models in a federated learning framework.
ICASSP.

TOMASHENKO N., SRIVASTAVA B. M. L., WANG X., VINCENT E., NAUTSCH A., YAMAGISHI
J., EVANS N. et al. (2020). Introducing the VoicePrivacy initiative. p. 1693â€“1697.

TOMASHENKO N., WANG X., VINCENT E. et al. (2022b). The VoicePrivacy 2020 Challenge :
Results and findings. Computer Speech & Language, 74, 101362.

TRUEX S., LIU L., GURSOY M. E., YU L. & WEI W. (2019). Demystifying membership inference
attacks in machine learning as a service. IEEE Transactions on Services Computing.

WANG Z., SONG M., ZHANG Z., SONG Y., WANG Q. & QI H. (2019). Beyond inferring class
representatives : User-level privacy leakage from federated learning. p. 2512â€“2520.

YU W., FREIWALD J., TEWES S., HUENNEMEYER F. & KOLOSSA D. (2021). Federated learning
in ASR : Not as easy as you think. arXiv preprint arXiv :2109.15108.

